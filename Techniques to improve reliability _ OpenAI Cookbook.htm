<!DOCTYPE html>
<!-- saved from url=(0070)https://cookbook.openai.com/articles/techniques_to_improve_reliability -->
<html lang="en" class="scroll-smooth scroll-pt-20 light" style="color-scheme: light;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link rel="stylesheet" href="./Techniques to improve reliability _ OpenAI Cookbook_files/1e3edbbf5fd038f3.css" crossorigin="" data-precedence="next"><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/fd9d1056-94c97424aeb854c5.js" async="" crossorigin=""></script><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/864-0197db867122dae1.js" async="" crossorigin=""></script><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/main-app-8f13c08947745519.js" async="" crossorigin=""></script><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/polyfills-c67a75d1b6f99dc8.js" crossorigin="" nomodule=""></script><title>Techniques to improve reliability | OpenAI Cookbook</title><meta name="description" content="Open-source examples and guides for building with the OpenAI API. Browse a collection of snippets, advanced techniques and walkthroughs. Share your own examples and guides."><meta name="keywords" content="openai, cookbook, api, examples, guides, gpt, chatgpt, gpt-4, embeddings"><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:title" content="Techniques to improve reliability | OpenAI Cookbook"><meta property="og:image" content="https://cookbook-99ou907x6-openai.vercel.app/og?title=Techniques%20to%20improve%20reliability&amp;tags=completions"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="660"><meta property="og:image:alt" content="OpenAI Cookbook | Techniques to improve reliability"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Techniques to improve reliability | OpenAI Cookbook"><meta name="twitter:image" content="https://cookbook-99ou907x6-openai.vercel.app/og?title=Techniques%20to%20improve%20reliability&amp;tags=completions"><meta name="twitter:image:width" content="1200"><meta name="twitter:image:height" content="660"><meta name="twitter:image:alt" content="OpenAI Cookbook | Techniques to improve reliability"><link rel="icon" href="https://cookbook.openai.com/favicon.svg"><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/script.js" defer="" data-sdkn="@vercel/analytics" data-sdkv="1.1.0"></script></head><body><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><script src="./Techniques to improve reliability _ OpenAI Cookbook_files/webpack-64f9b25b529b9a1d.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/1e3edbbf5fd038f3.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:I{\"id\":6054,\"chunks\":[\"272:static/chunks/webpack-64f9b25b529b9a1d.js\",\"971:static/chunks/fd9d1056-94c97424aeb854c5.js\",\"864:static/chunks/864-0197db867122dae1.js\"],\"name\":\"\",\"async\":false}\n5:I{\"id\":1729,\"chunks\":[\"272:static/chunks/webpack-64f9b25b529b9a1d.js\",\"971:static/chunks/fd9d1056-94c97424aeb854c5.js\",\"864:static/chunks/864-0197db867122dae1.js\"],\"name\":\"\",\"async\":false}\n6:I{\"id\":9588,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/c"])</script><script>self.__next_f.push([1,"hunks/558-b7a9a2c01701916a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/chunks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"ThemeProvider\",\"async\":false}\n8:I{\"id\":1443,\"chunks\":[\"272:static/chunks/webpack-64f9b25b529b9a1d.js\",\"971:static/chunks/fd9d1056-94c97424aeb854c5.js\",\"864:static/chunks/864-0197db867122dae1.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":8639,\"chunks\":[\"272:static/chunks/webpack-64f9b25b529b9a1d.js\",\"971:static/chunks/fd9d1056-94c97424aeb854c5.js\",\"864:static/chunks/864-0197db867122"])</script><script>self.__next_f.push([1,"dae1.js\"],\"name\":\"\",\"async\":false}\nc:I{\"id\":4441,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/chunks/558-b7a9a2c01701916a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/chunks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"Analytics\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1e3edbbf5fd038f3.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"_uIp6RffhzFr74JVVIASU\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/articles/techniques_to_improve_reliability\",\"initialTree\":[\"\",{\"children\":[[\"path\",\"articles/techniques_to_improve_reliability\",\"c\"],{\"children\":[\"__PAGE__?{\\\"path\\\":[\\\"articles\\\",\\\"techniques_to_improve_reliability\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L4\"],\"globalErrorComponent\":\"$5\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth scroll-pt-20\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L6\",null,{\"attribute\":\"class\",\"defaultTheme\":\"system\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[null,[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center pt-8 pb-32 px-4 sm:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-3xl w-full\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",[\"path\",\"articles/techniques_to_improve_reliability\",\"c\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$La\",\"$Lb\",null],\"segment\":\"__PAGE__?{\\\"path\\\":[\\\"articles\\\",\\\"techniques_to_improve_reliability\\\"]}\"},\"styles\":[]}]}]}],null],\"segment\":[\"path\",\"articles/techniques_to_improve_reliability\",\"c\"]},\"styles\":[]}]]}],[\"$\",\"$Lc\",null,{}]]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"b:\"$Ld\"\n"])</script><script>self.__next_f.push([1,"4:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Techniques to improve reliability | OpenAI Cookbook\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Open-source examples and guides for building with the OpenAI API. Browse a collection of snippets, advanced techniques and walkthroughs. Share your own examples and guides.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"openai, cookbook, api, examples, guides, gpt, chatgpt, gpt-4, embeddings\"}],[\"$\",\"meta\",\"4\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Techniques to improve reliability | OpenAI Cookbook\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:image\",\"content\":\"https://cookbook-99ou907x6-openai.vercel.app/og?title=Techniques%20to%20improve%20reliability\u0026tags=completions\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image:height\",\"content\":\"660\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image:alt\",\"content\":\"OpenAI Cookbook | Techniques to improve reliability\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Techniques to improve reliability | OpenAI Cookbook\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://cookbook-99ou907x6-openai.vercel.app/og?title=Techniques%20to%20improve%20reliability\u0026tags=completions\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:image:height\",\"content\":\"660\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image:alt\",\"content\":\"OpenAI Cookbook | Techniques to improve reliability\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]]\n"])</script><script>self.__next_f.push([1,"a:null\n"])</script><script>self.__next_f.push([1,"e:I{\"id\":8665,\"chunks\":[\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"408:static/chunks/408-69aa55354c6889e0.js\",\"884:static/chunks/app/[...path]/page-67f505025e0f5475.js\"],\"name\":\"AuthorsAndDate\",\"async\":false}\nf:T49a,"])</script><script>self.__next_f.push([1,"M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z"])</script><script>self.__next_f.push([1,"d:[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"sm:text-3xl text-2xl font-bold my-2\",\"children\":[\"$\",\"span\",null,{\"children\":\"Techniques to improve reliability\"}]}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-end\",\"children\":[[\"$\",\"$Le\",null,{\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"date\":\"2022-09-12\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors border-border border flex gap-2 items-center\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-4 h-4\",\"children\":[\"$\",\"path\",null,{\"d\":\"$f\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}],[\"$\",\"span\",null,{\"className\":\"max-sm:hidden\",\"children\":\"Open in Github\"}]]}]]}],[\"$\",\"hr\",null,{\"className\":\"mt-4 mb-8 border-border\"}]]}],false,false,\"$L10\"]}]\n"])</script><script>self.__next_f.push([1,"14:I{\"id\":4060,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/chunks/558-b7a9a2c01701916a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/chunks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"MobileNav\",\"async\":false}\n15:I{\"id\":5964,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/chunks/558-b7a9a2c01701916a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/ch"])</script><script>self.__next_f.push([1,"unks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"\",\"async\":false}\n18:I{\"id\":4757,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/chunks/558-b7a9a2c01701916a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/chunks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"ThemeToggle\",\"async\":false}\n19:I{\"id\":2490,\"chunks\":[\"310:static/chunks/0e5ce63c-35e019a325775ceb.js\",\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"558:static/chunks/558-b7a9a2c0170191"])</script><script>self.__next_f.push([1,"6a.js\",\"941:static/chunks/941-89de3ae4773d1fbc.js\",\"185:static/chunks/app/layout-36d3679dad28d2f9.js\"],\"name\":\"Search\",\"async\":false}\n10:\"$L11\"\n12:T677,"])</script><script>self.__next_f.push([1,"m297.06 130.97c7.26-21.79 4.76-45.66-6.85-65.48-17.46-30.4-52.56-46.04-86.84-38.68-15.25-17.18-37.16-26.95-60.13-26.81-35.04-.08-66.13 22.48-76.91 55.82-22.51 4.61-41.94 18.7-53.31 38.67-17.59 30.32-13.58 68.54 9.92 94.54-7.26 21.79-4.76 45.66 6.85 65.48 17.46 30.4 52.56 46.04 86.84 38.68 15.24 17.18 37.16 26.95 60.13 26.8 35.06.09 66.16-22.49 76.94-55.86 22.51-4.61 41.94-18.7 53.31-38.67 17.57-30.32 13.55-68.51-9.94-94.51zm-120.28 168.11c-14.03.02-27.62-4.89-38.39-13.88.49-.26 1.34-.73 1.89-1.07l63.72-36.8c3.26-1.85 5.26-5.32 5.24-9.07v-89.83l26.93 15.55c.29.14.48.42.52.74v74.39c-.04 33.08-26.83 59.9-59.91 59.97zm-128.84-55.03c-7.03-12.14-9.56-26.37-7.15-40.18.47.28 1.3.79 1.89 1.13l63.72 36.8c3.23 1.89 7.23 1.89 10.47 0l77.79-44.92v31.1c.02.32-.13.63-.38.83l-64.41 37.19c-28.69 16.52-65.33 6.7-81.92-21.95zm-16.77-139.09c7-12.16 18.05-21.46 31.21-26.29 0 .55-.03 1.52-.03 2.2v73.61c-.02 3.74 1.98 7.21 5.23 9.06l77.79 44.91-26.93 15.55c-.27.18-.61.21-.91.08l-64.42-37.22c-28.63-16.58-38.45-53.21-21.95-81.89zm221.26 51.49-77.79-44.92 26.93-15.54c.27-.18.61-.21.91-.08l64.42 37.19c28.68 16.57 38.51 53.26 21.94 81.94-7.01 12.14-18.05 21.44-31.2 26.28v-75.81c.03-3.74-1.96-7.2-5.2-9.06zm26.8-40.34c-.47-.29-1.3-.79-1.89-1.13l-63.72-36.8c-3.23-1.89-7.23-1.89-10.47 0l-77.79 44.92v-31.1c-.02-.32.13-.63.38-.83l64.41-37.16c28.69-16.55 65.37-6.7 81.91 22 6.99 12.12 9.52 26.31 7.15 40.1zm-168.51 55.43-26.94-15.55c-.29-.14-.48-.42-.52-.74v-74.39c.02-33.12 26.89-59.96 60.01-59.94 14.01 0 27.57 4.92 38.34 13.88-.49.26-1.33.73-1.89 1.07l-63.72 36.8c-3.26 1.85-5.26 5.31-5.24 9.06l-.04 89.79zm14.63-31.54 34.65-20.01 34.65 20v40.01l-34.65 20-34.65-20z"])</script><script>self.__next_f.push([1,"13:T677,"])</script><script>self.__next_f.push([1,"m297.06 130.97c7.26-21.79 4.76-45.66-6.85-65.48-17.46-30.4-52.56-46.04-86.84-38.68-15.25-17.18-37.16-26.95-60.13-26.81-35.04-.08-66.13 22.48-76.91 55.82-22.51 4.61-41.94 18.7-53.31 38.67-17.59 30.32-13.58 68.54 9.92 94.54-7.26 21.79-4.76 45.66 6.85 65.48 17.46 30.4 52.56 46.04 86.84 38.68 15.24 17.18 37.16 26.95 60.13 26.8 35.06.09 66.16-22.49 76.94-55.86 22.51-4.61 41.94-18.7 53.31-38.67 17.57-30.32 13.55-68.51-9.94-94.51zm-120.28 168.11c-14.03.02-27.62-4.89-38.39-13.88.49-.26 1.34-.73 1.89-1.07l63.72-36.8c3.26-1.85 5.26-5.32 5.24-9.07v-89.83l26.93 15.55c.29.14.48.42.52.74v74.39c-.04 33.08-26.83 59.9-59.91 59.97zm-128.84-55.03c-7.03-12.14-9.56-26.37-7.15-40.18.47.28 1.3.79 1.89 1.13l63.72 36.8c3.23 1.89 7.23 1.89 10.47 0l77.79-44.92v31.1c.02.32-.13.63-.38.83l-64.41 37.19c-28.69 16.52-65.33 6.7-81.92-21.95zm-16.77-139.09c7-12.16 18.05-21.46 31.21-26.29 0 .55-.03 1.52-.03 2.2v73.61c-.02 3.74 1.98 7.21 5.23 9.06l77.79 44.91-26.93 15.55c-.27.18-.61.21-.91.08l-64.42-37.22c-28.63-16.58-38.45-53.21-21.95-81.89zm221.26 51.49-77.79-44.92 26.93-15.54c.27-.18.61-.21.91-.08l64.42 37.19c28.68 16.57 38.51 53.26 21.94 81.94-7.01 12.14-18.05 21.44-31.2 26.28v-75.81c.03-3.74-1.96-7.2-5.2-9.06zm26.8-40.34c-.47-.29-1.3-.79-1.89-1.13l-63.72-36.8c-3.23-1.89-7.23-1.89-10.47 0l-77.79 44.92v-31.1c-.02-.32.13-.63.38-.83l64.41-37.16c28.69-16.55 65.37-6.7 81.91 22 6.99 12.12 9.52 26.31 7.15 40.1zm-168.51 55.43-26.94-15.55c-.29-.14-.48-.42-.52-.74v-74.39c.02-33.12 26.89-59.96 60.01-59.94 14.01 0 27.57 4.92 38.34 13.88-.49.26-1.33.73-1.89 1.07l-63.72 36.8c-3.26 1.85-5.26 5.31-5.24 9.06l-.04 89.79zm14.63-31.54 34.65-20.01 34.65 20v40.01l-34.65 20-34.65-20z"])</script><script>self.__next_f.push([1,"16:T565,"])</script><script>self.__next_f.push([1,"M0.877075 7.49972C0.877075 3.84204 3.84222 0.876892 7.49991 0.876892C11.1576 0.876892 14.1227 3.84204 14.1227 7.49972C14.1227 11.1574 11.1576 14.1226 7.49991 14.1226C3.84222 14.1226 0.877075 11.1574 0.877075 7.49972ZM7.49991 1.82689C4.36689 1.82689 1.82708 4.36671 1.82708 7.49972C1.82708 10.6327 4.36689 13.1726 7.49991 13.1726C10.6329 13.1726 13.1727 10.6327 13.1727 7.49972C13.1727 4.36671 10.6329 1.82689 7.49991 1.82689ZM8.24993 10.5C8.24993 10.9142 7.91414 11.25 7.49993 11.25C7.08571 11.25 6.74993 10.9142 6.74993 10.5C6.74993 10.0858 7.08571 9.75 7.49993 9.75C7.91414 9.75 8.24993 10.0858 8.24993 10.5ZM6.05003 6.25C6.05003 5.57211 6.63511 4.925 7.50003 4.925C8.36496 4.925 8.95003 5.57211 8.95003 6.25C8.95003 6.74118 8.68002 6.99212 8.21447 7.27494C8.16251 7.30651 8.10258 7.34131 8.03847 7.37854L8.03841 7.37858C7.85521 7.48497 7.63788 7.61119 7.47449 7.73849C7.23214 7.92732 6.95003 8.23198 6.95003 8.7C6.95004 9.00376 7.19628 9.25 7.50004 9.25C7.8024 9.25 8.04778 9.00601 8.05002 8.70417L8.05056 8.7033C8.05924 8.6896 8.08493 8.65735 8.15058 8.6062C8.25207 8.52712 8.36508 8.46163 8.51567 8.37436L8.51571 8.37433C8.59422 8.32883 8.68296 8.27741 8.78559 8.21506C9.32004 7.89038 10.05 7.35382 10.05 6.25C10.05 4.92789 8.93511 3.825 7.50003 3.825C6.06496 3.825 4.95003 4.92789 4.95003 6.25C4.95003 6.55376 5.19628 6.8 5.50003 6.8C5.80379 6.8 6.05003 6.55376 6.05003 6.25Z"])</script><script>self.__next_f.push([1,"17:T49a,"])</script><script>self.__next_f.push([1,"M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z"])</script><script>self.__next_f.push([1,"1a:T2d1e,"])</script><script>self.__next_f.push([1,"\nThis notebook demonstrates the use of the logprobs parameter in the Chat Completions API. When logprobs is enabled, the API returns the log probabilities of each output token, along with a limited number of the most likely tokens at each token position and their log probabilities. The relevant request parameters are:\nlogprobs: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model.\ntop_logprobs: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\nLog probabilities of output tokens indicate the likelihood of each token occurring in the sequence given the context. To simplify, a logprob is log(p), where p = probability of a token occurring at a specific position based on the previous tokens in the context. Some key points about logprobs:\nHigher log probabilities suggest a higher likelihood of the token in that context. This allows users to gauge the model's confidence in its output or explore alternative responses the model considered.\nLogprob can be any negative number or 0.0. 0.0 corresponds to 100% probability.\nLogprobs allow us to compute the joint probability of a sequence as the sum of the logprobs of the individual tokens. This is useful for scoring and ranking model outputs. Another common approach is to take the average per-token logprob of a sentence to choose the best generation.\nWe can examine the logprobs assigned to different candidate tokens to understand what options the model considered plausible or implausible.\n\nWhile there are a wide array of use cases for logprobs, this notebook will focus on its use for:\n\nClassification tasks\n\nLarge Language Models excel at many classification tasks, but accurately measuring the model's confidence in its outputs can be challenging. logprobs provide a probability associated with each class prediction, enabling users to set their own classification or confidence thresholds.\n\nRetrieval (Q\u0026A) evaluation\n\nlogprobs can assist with self-evaluation in retrieval applications. In the Q\u0026A example, the model outputs a contrived has_sufficient_context_for_answer boolean, which can serve as a confidence score of whether the answer is contained in the retrieved content. Evaluations of this type can reduce retrieval-based hallucinations and enhance accuracy.\n\nAutocomplete\nlogprobs could help us decide how to suggest words as a user is typing.\n\nToken highlighting and outputting bytes\nUsers can easily create a token highlighter using the built in tokenization that comes with enabling logprobs. Additionally, the bytes parameter includes the ASCII encoding of each output character, which is particularly useful for reproducing emojis and special characters.\n\nCalculating perplexity\nlogprobs can be used to help us assess the model's overall confidence in a result and help us compare the confidence of results from different prompts.\n0. Imports and utils\n1. Using logprobs to assess confidence for classification tasks\nLet's say we want to create a system to classify news articles into a set of pre-defined categories. Without logprobs, we can use Chat Completions to do this, but it is much more difficult to assess the certainty with which the model made its classifications.\n\nNow, with logprobs enabled, we can see exactly how confident the model is in its predictions, which is crucial for creating an accurate and trustworthy classifier. For example, if the log probability for the chosen category is high, this suggests the model is quite confident in its classification. If it's low, this suggests the model is less confident. This can be particularly useful in cases where the model's classification is not what you expected, or when the model's output needs to be reviewed or validated by a human.\nWe'll begin with a prompt that presents the model with four categories: Technology, Politics, Sports, and Arts. The model is then tasked with classifying articles into these categories based solely on their headlines.\nLet's look at three sample headlines, and first begin with a standard Chat Completions output, without logprobs\nHere we can see the selected category for each headline. However, we have no visibility into the confidence of the model in its predictions. Let's rerun the same prompt but with logprobs enabled, and top_logprobs set to 2 (this will show us the 2 most likely output tokens for each token). Additionally we can also output the linear probability of each output token, in order to convert the log probability to the more easily interprable scale of 0-100%.\nAs expected from the first two headlines, gpt-4 is nearly 100% confident in its classifications, as the content is clearly technology and politics focused respectively. However, the third headline combines both sports and art-related themes, so we see the model is less confident in its selection.\n\nThis shows how important using logprobs can be, as if we are using LLMs for classification tasks we can set confidence theshholds, or output several potential output tokens if the log probability of the selected output is not sufficiently high. For instance, if we are creating a recommendation engine to tag articles, we can automatically classify headlines crossing a certain threshold, and send the less certain headlines for manual review.\n2. Retrieval confidence scoring to reduce hallucinations\nTo reduce hallucinations, and the performance of our RAG-based Q\u0026A system, we can use logprobs to evaluate how confident the model is in its retrieval.\nLet's say we have built a retrieval system using RAG for Q\u0026A, but are struggling with hallucinated answers to our questions. Note: we will use a hardcoded article for this example, but see other entries in the cookbook for tutorials on using RAG for Q\u0026A.\nNow, what we can do is ask the model to respond to the question, but then also evaluate its response. Specifically, we will ask the model to output a boolean has_sufficient_context_for_answer. We can then evaluate the logprobs to see just how confident the model is that its answer was contained in the provided context\nFor the first two questions, our model asserts with (near) 100% confidence that the article has sufficient context to answer the posed questions.\nOn the other hand, for the more tricky questions which are less clearly answered in the article, the model is less confident that it has sufficient context. This is a great guardrail to help ensure our retrieved content is sufficient.\nThis self-evaluation can help reduce hallucinations, as you can restrict answers or re-prompt the user when your sufficient_context_for_answer log probability is below a certain threshold. Methods like this have been shown to significantly reduce RAG for Q\u0026A hallucinations and errors (Example)\n3. Autocomplete\nAnother use case for logprobs are autocomplete systems. Without creating the entire autocomplete system end-to-end, let's demonstrate how logprobs could help us decide how to suggest words as a user is typing.\nFirst, let's come up with a sample sentence: \"My least favorite TV show is Breaking Bad.\" Let's say we want it to dynamically recommend the next word or token as we are typing the sentence, but only if the model is quite sure of what the next word will be. To demonstrate this, let's break up the sentence into sequential components.\nNow, we can ask gpt-3.5-turbo to act as an autocomplete engine with whatever context the model is given. We can enable logprobs and can see how confident the model is in its prediction.\nLet's look at the high confidence autocompletions:\nThese look reasonable! We can feel confident in those suggestions. It's pretty likely you want to write 'show' after writing 'My least favorite TV'! Now let's look at the autocompletion suggestions the model was less confident about:\nThese are logical as well. It's pretty unclear what the user is going to say with just the prefix 'my least favorite', and it's really anyone's guess what the author's favorite TV show is.\nSo, using gpt-3.5-turbo, we can create the root of a dynamic autocompletion engine with logprobs!\n4. Highlighter and bytes parameter\nLet's quickly touch on creating a simple token highlighter with logprobs, and using the bytes parameter. First, we can create a function that counts and highlights each token. While this doesn't use the log probabilities, it uses the built in tokenization that comes with enabling logprobs.\nNext, let's reconstruct a sentence using the bytes parameter. With logprobs enabled, we are given both each token and the ASCII (decimal utf-8) values of the token string. These ASCII values can be helpful when handling tokens of or containing emojis or special characters.\nHere, we see that while the first token was \\xf0\\x9f\\x92', we can get its ASCII value and append it to a bytes array. Then, we can easily decode this array into a full sentence, and validate with our assert statement that the decoded bytes is the same as our completion message!\n\nAdditionally, we can get the joint probability of the entire completion, which is the exponentiated product of each token's log probability. This gives us how likely this given completion is given the prompt. Since, our prompt is quite directive (asking for a certain emoji and its name), the joint probability of this output is high! If we ask for a random output however, we'll see a much lower joint probability. This can also be a good tactic for developers during prompt engineering.\n5. Calculating perplexity\n\nWhen looking to assess the model's confidence in a result, it can be useful to calculate perplexity, which is a measure of the uncertainty. Perplexity can be calculated by exponentiating the negative of the average of the logprobs. Generally, a higher perplexity indicates a more uncertain result, and a lower perplexity indicates a more confident result. As such, perplexity can be used to both assess the result of an individual model run and also to compare the relative confidence of results between model runs. While a high confidence doesn't guarantee result accuracy, it can be a helpful signal that can be paired with other evaluation metrics to build a better understanding of your prompt's behavior.\n\nFor example, let's say that I want to use gpt-3.5-turbo to learn more about artificial intelligence. I could ask a question about recent history and a question about the future:\nIn this example, gpt-3.5-turbo returned a lower perplexity score for a more deterministic question about recent history, and a higher perplexity score for a more speculative assessment about the near future. Again, while these differences don't guarantee accuracy, they help point the way for our interpretation of the model's results and our future use of them.\n6. Conclusion\nNice! We were able to use the logprobs parameter to build a more robust classifier, evaluate our retrieval for Q\u0026A system, and encode and decode each 'byte' of our tokens! logprobs adds useful information and signal to our completions output, and we are excited to see how developers incorporate it to improve applications.\n7. Possible extensions\nThere are many other use cases for logprobs that are not covered in this cookbook. We can use logprobs for:\n  Moderation\n  Keyword selection\n  Improve prompts and interpretability of outputs\n  Token healing\n  and more!"])</script><script>self.__next_f.push([1,"1b:T1a82,"])</script><script>self.__next_f.push([1,"\nThis notebook illustrates the use of the new Assistants API (GPT-4), and DALL·E-3 in crafting informative and visually appealing slides.\nCreating slides is a pivotal aspect of many jobs, but can be laborious and time-consuming. Additionally, extracting insights from data and articulating them effectively on slides can be challenging.  This cookbook recipe will demonstrate how you can utilize the new Assistants API to facilitate the end to end slide creation process for you without you having to touch Microsoft PowerPoint or Google Slides, saving you valuable time and effort!\n0. Setup\n1. Creating the content\nIn this recipe, we will be creating a brief fictional presentation for the quarterly financial review of our company, NotReal Corporation. We want to highlight some key trends we are seeing that are affecting the profitability of our company. Let's say we have the some financial data at our disposal. Let's load in the data, and take a look...\nAs you can see, this data has quarterly revenue, costs and customer data across different distribution channels. Let's create an Assistant\nthat can act as a personal analyst and make a nice visualization for our PowerPoint!\nFirst, we need to upload our file so our Assistant can access it.\nNow, we're ready to create our Assistant. We can instruct our assistant to act as a data scientist, and take any queries we give it and run the necessary code to output the proper data visualization. The instructions parameter here is akin to system instructions in the ChatCompletions endpoint, and can help guide the assistant. We can also turn on the tool of Code Interpreter, so our Assistant will be able to code. Finally, we can specifiy any files we want to use, which in this case is just the financial_data file we created above.\nLet's create a thread now, and as our first request ask the Assistant to calculate quarterly profits, and then plot the profits by distribution channel over time. The assistant will automatically calculate the profit for each quarter, and also create a new column combining quarter and year, without us having to ask for that directly. We can also specify the colors of each line.\nNo we can execute the run of our thread\nWe can now start a loop that will check if the image has been created. Note: This may take a few minutes\nLet's see the messages the Assistant added.\nWe can see that the last message (latest message is shown first) from the assistant contains the image file we are looking for. An interesting note here is that the Assistant was able to attempt several times to parse the JSON data, as the first parsing was unsuccessful, demonstrating the assistant's adaptability.\nLet's load in the plot!\nThe Image\nNice! So, with just one sentence, we were able to have our assistant use code interpreter to\ncalculate the profitability, and graph the three lineplots of the various distribution channels.\nNow we have a nice visual for our slide, but we want some insights to go along with it.\n2. Generating insights\nTo get insights from our image, we simply need to add a new message to our thread. Our Assistant will know to use the message history to give us some concise takeaways from the visual provided.\nNow, once the run has completed, we can see the latest message\nCool! So our assistant was able to identify the noteworthy growth in Online Sales profit, and infer that this shows the importance of a large digital presence. Now let's get a compelling title for the slide.\nAnd the title is:\n3. DALL·E-3 title image\nNice, now we have a title, a plot and two bullet points. We're almost ready to put this all on a slide, but as a final step, let's have DALL·E-3 come up with an image to use as the title slide of the presentation.\nNote: DALL·E-3 is not yet available within the assistants API but is coming soon!\nWe'll feed in a brief description of our company (NotRealCorp) and have DALL·E-3 do the rest!\nCool, now we can add this image to our thread. First, we can save the image locally, then upload it to the assistants API using the File upload endpoint. Let's also take a look at our image\n\nImage\n4. Creating the slides\nWe now have all the content we need to create the slides. While we could simply add a message asking for slides, but let's instead give the assistant a slide template, using the python-pptx library, to use. This will ensure we get a deck in the style we want. See the Extensions section at the end of the notebook for notes on creating the template.\nLet's set a few quick variables for our slides. We want the company name, NotRealCorp, to be on the title slide, and the title of the presentation should 'Quartlerly financial planning metting, Q3, 2023'.\nAnd for the data slide, we have:\nHere we have a template to create a Title Slide. The template below was created by uploading the image of a desirable title slide to GPT-V, and asking for the python-pptx code to create that template. The inputs to the template are the image_path, title_text, and subtitle_text.\nNow, we have a PPTX file saved with all of our created content!.\nLet's look at the screenshots of the .pptx we just created using JUST the assistants API and DALL·E-3. We don't have a seed parameter yet in the Assistants API, so the DALL·E-3 image and wordings will be slightly different from what you see when you run this notebook, due to the non-determinism of LLMs, but the outputs should be directionally the same.\nThe title slide:\nTitle Slide\nAnd the data slide:\nData Slide\n5. Conclusion\nWoo! While these slides could use some formatting tweaks, we have made some great content using the Assistants API, GPT-4 and DALL·E-3. We were able to take a .csv file with financial data, and use our assisant to calculate profit by quarter across distribution channels, plot the results, identify insights and key takeaways from the visualization, and create a summarative title. And, given just a description of our company, NotRealCorp, we used DALL·E-3 to make an awesome title image.\nWhile we are still a ways away from entirely automating this process without a human in the loop, hopefully this notebook can make the slide creation process a bit easier for you. More importantly, this notebook can ideally give you a glimpse into the potential of the assistants API! We're excited to see what you build.\n6. Extensions\nWhen  DALL·E-3 is incorporated in the Assistants API, we will have the ability to request the generated title image within the thread.\nGPT-4-Vision is not yet supported in the Assistants API, but could have been used to gather insights from the line plot image.\nGPT-4-Vision was used to generate the python-pptx template included in this recipe, so a potential extension project could be demonstrating best practices around converting images to slide templates."])</script><script>self.__next_f.push([1,"1c:Tcb3,"])</script><script>self.__next_f.push([1,"\nThis notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model.\nIt checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs.\nThe method shown here corresponds to the current fine-tuning method for gpt-3.5-turbo.\nSee legacy fine-tuning for models like babbage-002 and davinci-002.\nData loading\n\nWe first load the chat dataset from an example JSONL file.\nFormat validation\n\nWe can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n\nData Type Check: Checks whether each entry in the dataset is a dictionary (dict). Error type: data_type.\nPresence of Message List: Checks if a messages list is present in each entry. Error type: missing_messages_list.\nMessage Keys Check: Validates that each message in the messages list contains the keys role and content. Error type: message_missing_key.\nUnrecognized Keys in Messages: Logs if a message has keys other than role, content, weight, function_call, and name. Error type: message_unrecognized_key.\nRole Validation: Ensures the role is one of \"system\", \"user\", or \"assistant\". Error type: unrecognized_role.\nContent Validation: Verifies that content has textual data and is a string. Error type: missing_content.\nAssistant Message Presence: Checks that each conversation has at least one message from the assistant. Error type: example_missing_assistant_message.\n\nThe code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n\nToken Counting Utilities\n\nLets define a few helpful utilities to be used in the rest of the notebook.\nData Warnings and Token Counts\n\nWith some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n\nMissing System/User Messages: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\nNumber of Messages Per Example: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\nTotal Tokens Per Example: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\nTokens in Assistant's Messages: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\nToken Limit Warnings: Checks if any examples exceed the maximum token limit (16,385 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n\nCost Estimation\n\nIn this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count.\nSee https://openai.com/pricing to estimate total costs."])</script><script>self.__next_f.push([1,"1d:T4a2,"])</script><script>self.__next_f.push([1,"\nThere are many ways to classify text. This notebook shares an example of text classification using embeddings. For many text classification tasks, we've seen fine-tuned models do better than embeddings. See an example of fine-tuned models for classification in Fine-tuned_classification.ipynb. We also recommend having more examples than embedding dimensions, which we don't quite achieve here.\n\nIn this text classification task, we predict the score of a food review (1 to 5) based on the embedding of the review's text. We split the dataset into a training and a testing set for all the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the Get_embeddings_from_dataset Notebook.\n\nWe can see that the model has learnt to distinguish between the categories decently. 5-star reviews show the best performance overall, and this is not too surprising, since they are the most common in the dataset.\nUnsurprisingly 5-star and 1-star reviews seem to be easier to predict. Perhaps with more data, the nuances between 2-4 stars could be better predicted, but there's also probably more subjectivity in how people use the inbetween scores."])</script><script>self.__next_f.push([1,"1e:T5ef,"])</script><script>self.__next_f.push([1,"\nThis notebook covers use cases where your data is unlabelled but has features that can be used to cluster them into meaningful categories. The challenge with clustering is making the features that make those clusters stand out human-readable, and that is where we'll look to use GPT-3 to generate meaningful cluster descriptions for us. We can then use these to apply labels to a previously unlabelled dataset.\n\nTo feed the model we use embeddings created using the approach displayed in the notebook Multiclass classification for transactions Notebook, applied to the full 359 transactions in the dataset to give us a bigger pool for learning\nSetup\nClustering\n\nWe'll reuse the approach from the Clustering Notebook, using K-Means to cluster our dataset using the feature embeddings we created previously. We'll then use the Completions endpoint to generate cluster descriptions for us and judge their effectiveness\nConclusion\n\nWe now have five new clusters that we can use to describe our data. Looking at the visualisation some of our clusters have some overlap and we'll need some tuning to get to the right place, but already we can see that GPT-3 has made some effective inferences. In particular, it picked up that items including legal deposits were related to literature archival, which is true but the model was given no clues on. Very cool, and with some tuning we can create a base set of clusters that we can then use with a multiclass classifier to generalise to other transactional datasets we might use."])</script><script>self.__next_f.push([1,"1f:T67e,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how Ada embeddings can be used to implement semantic code search. For this demonstration, we use our own openai-python code repository. We implement a simple version of file parsing and extracting of functions from python files, which can be embedded, indexed, and queried.\nHelper Functions\n\nWe first setup some simple parsing functions that allow us to extract important information from our codebase.\nData Loading\n\nWe'll first load the openai-python folder and extract the needed information using the functions we defined above.\nNow that we have our content, we can pass the data to the text-embedding-3-small model and get back our vector embeddings.\nTesting\n\nLet's test our endpoint with some simple queries. If you're familiar with the openai-python repository, you'll see that we're able to easily find functions we're looking for only a simple English description.\n\nWe define a search_functions method that takes our data that contains our embeddings, a query string, and some other configuration options. The process of searching our database works like such:\n\nWe first embed our query string (code_query) with text-embedding-3-small. The reasoning here is that a query string like 'a function that reverses a string' and a function like 'def reverse(string): return string[::-1]' will be very similar when embedded.\nWe then calculate the cosine similarity between our query string embedding and all data points in our database. This gives a distance between each point and our query.\nWe finally sort all of our data points by their distance to our query string and return the number of results requested in the function parameters."])</script><script>self.__next_f.push([1,"20:Te31,"])</script><script>self.__next_f.push([1,"\nThis notebook demonstrates one way to customize OpenAI embeddings to a particular task.\n\nThe input is training data in the form of [text_1, text_2, label] where label is +1 if the pairs are similar and -1 if the pairs are dissimilar.\n\nThe output is a matrix that you can use to multiply your embeddings. The product of this multiplication is a 'custom embedding' that will better emphasize aspects of the text relevant to your use case. In binary classification use cases, we've seen error rates drop by as much as 50%.\n\nIn the following example, I use 1,000 sentence pairs picked from the SNLI corpus. Each pair of sentences are logically entailed (i.e., one implies the other). These pairs are our positives (label = 1). We generate synthetic negatives by combining sentences from different pairs, which are presumed to not be logically entailed (label = -1).\n\nFor a clustering use case, you can generate positives by creating pairs from texts in the same clusters and generate negatives by creating pairs from sentences in different clusters.\n\nWith other data sets, we have seen decent improvement with as little as ~100 training examples. Of course, performance will be better with  more examples.\n0. Imports\n1. Inputs\n\nMost inputs are here. The key things to change are where to load your datset from, where to save a cache of embeddings to, and which embedding engine you want to use.\n\nDepending on how your data is formatted, you'll want to rewrite the process_input_data function.\n2. Load and process input data\n3. Split data into training test sets\n\nNote that it's important to split data into training and test sets before generating synethetic negatives or positives. You don't want any text strings in the training data to show up in the test data. If there's contamination, the test metrics will look better than they'll actually be in production.\n4. Generate synthetic negatives\n\nThis is another piece of the code that you will need to modify to match your use case.\n\nIf you have data with positives and negatives, you can skip this section.\n\nIf you have data with only positives, you can mostly keep it as is, where it generates negatives only.\n\nIf you have multiclass data, you will want to generate both positives and negatives. The positives can be pairs of text that share labels, and the negatives can be pairs of text that do not share labels.\n\nThe final output should be a dataframe with text pairs, where each pair is labeled -1 or 1.\n5. Calculate embeddings and cosine similarities\n\nHere, I create a cache to save the embeddings. This is handy so that you don't have to pay again if you want to run the code again.\n6. Plot distribution of cosine similarity\n\nHere we measure similarity of text using cosine similarity. In our experience, most distance functions (L1, L2, cosine similarity) all work about the same. Note that our embeddings are already normalized to length 1, so cosine similarity is equivalent to dot product.\n\nThe graphs show how much the overlap there is between the distribution of cosine similarities for similar and dissimilar pairs. If there is a high amount of overlap, that means there are some dissimilar pairs with greater cosine similarity than some similar pairs.\n\nThe accuracy I compute is the accuracy of a simple rule that predicts 'similar (1)' if the cosine similarity is above some threshold X and otherwise predicts 'dissimilar (0)'.\n7. Optimize the matrix using the training data provided\n8. Plot the before \u0026 after, showing the results of the best matrix found during training\n\nThe better the matrix is, the more cleanly it will separate the similar and dissimilar pairs."])</script><script>self.__next_f.push([1,"21:Tc2e,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how we prepared a dataset of Wikipedia articles for search, used in Question_answering_using_embeddings.ipynb.\n\nProcedure:\n\nPrerequisites: Import libraries, set API key (if needed)\nCollect: We download a few hundred Wikipedia articles about the 2022 Olympics\nChunk: Documents are split into short, semi-self-contained sections to be embedded\nEmbed: Each section is embedded with the OpenAI API\nStore: Embeddings are saved in a CSV file (for large datasets, use a vector database)\n0. Prerequisites\n\nImport libraries\nInstall any missing libraries with pip install in your terminal. E.g.,\n\npip install openai\n\n(You can also do this in a notebook cell with !pip install openai.)\n\nIf you install any libraries, be sure to restart the notebook kernel.\nSet API key (if needed)\n\nNote that the OpenAI library will try to read your API key from the OPENAI_API_KEY environment variable. If you haven't already, set this environment variable by following these instructions.\n1. Collect documents\n\nIn this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics.\n2. Chunk documents\n\nNow that we have our reference documents, we need to prepare them for search.\n\nBecause GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n\nFor this specific example on Wikipedia articles, we'll:\nDiscard less relevant-looking sections like External Links and Footnotes\nClean up the text by removing reference tags (e.g., ), whitespace, and super short sections\nSplit each article into sections\nPrepend titles and subtitles to each section's text, to help GPT understand the context\nIf a section is long (say, \u003e 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs\nNext, we'll recursively split long sections into smaller sections.\n\nThere's no perfect recipe for splitting text into sections.\n\nSome tradeoffs include:\nLonger sections may be better for questions that require more context\nLonger sections may be worse for retrieval, as they may have more topics muddled together\nShorter sections are better for reducing costs (which are proportional to the number of tokens)\nShorter sections allow more sections to be retrieved, which may help with recall\nOverlapping sections may help prevent answers from being cut by section boundaries\n\nHere, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible.\n3. Embed document chunks\n\nNow that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n\n(For large embedding jobs, use a script like api_request_parallel_processor.py to parallelize requests while throttling to stay under rate limits.)\n4. Store document chunks and embeddings\n\nBecause this example only uses a few thousand strings, we'll store them in a CSV file.\n\n(For larger datasets, use a vector database, which will be more performant.)"])</script><script>self.__next_f.push([1,"22:Tb12,"])</script><script>self.__next_f.push([1,"\nOpenAI's embedding models cannot embed text that exceeds a maximum length. The maximum length varies by model, and is measured by tokens, not string length. If you are unfamiliar with tokenization, check out How to count tokens with tiktoken.\n\nThis notebook shows how to handle texts that are longer than a model's maximum context length. We'll demonstrate using embeddings from text-embedding-3-small, but the same ideas can be applied to other models and tasks. To learn more about embeddings, check out the OpenAI Embeddings Guide.\n\n1. Model context length\n\nFirst, we select the model and define a function to get embeddings from the API.\nThe text-embedding-3-small model has a context length of 8191 tokens with the cl100k_base encoding, and we can see that going over that limit causes an error.\nClearly we want to avoid these errors, particularly when handling programmatically with a large number of embeddings. Yet, we still might be faced with texts that are longer than the maximum context length. Below we describe and provide recipes for the main approaches to handling these longer texts: (1) simply truncating the text to the maximum allowed length, and (2) chunking the text and embedding each chunk individually.\n1. Truncating the input text\n\nThe simplest solution is to truncate the input text to the maximum allowed length. Because the context length is measured in tokens, we have to first tokenize the text before truncating it. The API accepts inputs both in the form of text or tokens, so as long as you are careful that you are using the appropriate encoding, there is no need to convert the tokens back into string form. Below is an example of such a truncation function.\nOur example from before now works without error.\n2. Chunking the input text\n\nThough truncation works, discarding potentially relevant text is a clear drawback. Another approach is to divide the input text into chunks and then embed each chunk individually. Then, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).\n\nWe will take a function from Python's own cookbook that breaks up a sequence into chunks.\nNow we define a function that encodes a string into tokens and then breaks it up into chunks.\nFinally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The average flag can be set to True to return the weighted average of the chunk embeddings, or False to simply return the unmodified list of chunk embeddings.\nOnce again, we can now handle long input texts.\nIn some cases, it may make sense to split chunks on paragraph boundaries or sentence boundaries to help preserve the meaning of the text."])</script><script>self.__next_f.push([1,"23:T885,"])</script><script>self.__next_f.push([1,"\nGPT-3 can help us extract key figures, dates or other bits of important content from documents that are too big to fit into the context window. One approach for solving this is to chunk the document up and process each chunk separately, before combining into one list of answers.\n\nIn this notebook we'll run through this approach:\nLoad in a long PDF and pull the text out\nCreate a prompt to be used to extract key bits of information\nChunk up our document and process each chunk to pull any answers out\nCombine them at the end\nThis simple approach will then be extended to three more difficult questions\n\nApproach\n\nSetup**: Take a PDF, a Formula 1 Financial Regulation document on Power Units, and extract the text from it for entity extraction. We'll use this to try to extract answers that are buried in the content.\nSimple Entity Extraction**: Extract key bits of information from chunks of a document by:\n    Creating a template prompt with our questions and an example of the format it expects\n    Create a function to take a chunk of text as input, combine with the prompt and get a response\n    Run a script to chunk the text, extract answers and output them for parsing\nComplex Entity Extraction**: Ask some more difficult questions which require tougher reasoning to work out\nSetup\nSimple Entity Extraction\nComplex Entity Extraction\nConsolidation\n\nWe've been able to extract the first two answers safely, while the third was confounded by the date that appeared on every page, though the correct answer is in there as well.\n\nTo tune this further you can consider experimenting with:\nA more descriptive or specific prompt\nIf you have sufficient training data, fine-tuning a model to find a set of outputs very well\nThe way you chunk your data - we have gone for 1000 tokens with no overlap, but more intelligent chunking that breaks info into sections, cuts by tokens or similar may get better results\n\nHowever, with minimal tuning we have now answered 6 questions of varying difficulty using the contents of a long document, and have a reusable approach that we can apply to any long document requiring entity extraction. Look forward to seeing what you can do with this!"])</script><script>self.__next_f.push([1,"24:Te82,"])</script><script>self.__next_f.push([1,"\nWe will fine-tune a babbage-002 classifier (replacement for the ada models) to distinguish between the two sports: Baseball and Hockey.\nData exploration\n The newsgroup dataset can be loaded using sklearn. First we will look at the data itself:\nOne sample from the baseball category can be seen above. It is an email to a mailing list. We can observe that we have 1197 examples in total, which are evenly split between the two sports.\nData Preparation\nWe transform the dataset into a pandas dataframe, with a column for prompt and completion. The prompt contains the email from the mailing list, and the completion is a name of the sport, either hockey or baseball. For demonstration purposes only and speed of fine-tuning we take only 300 examples. In a real use case the more examples the better the performance.\nBoth baseball and hockey are single tokens. We save the dataset as a jsonl file.\nData Preparation tool\nWe can now use a data preparation tool which will suggest a few improvements to our dataset before fine-tuning. Before launching the tool we update the openai library to ensure we're using the latest data preparation tool. We additionally specify -q which auto-accepts all suggestions.\nThe tool helpfully suggests a few improvements to the dataset and splits the dataset into training and validation set.\n\nA suffix between a prompt and a completion is necessary to tell the model that the input text has stopped, and that it now needs to predict the class. Since we use the same separator in each example, the model is able to learn that it is meant to predict either baseball or hockey following the separator.\nA whitespace prefix in completions is useful, as most word tokens are tokenized with a space prefix.\nThe tool also recognized that this is likely a classification task, so it suggested to split the dataset into training and validation datasets. This will allow us to easily measure expected performance on new data.\nFine-tuning\nThe tool suggests we run the following command to train the dataset. Since this is a classification task, we would like to know what the generalization performance on the provided validation set is for our classification use case.\n\nWe can simply copy the suggested command from the CLI tool. We specifically add -m ada to fine-tune a cheaper and faster ada model, which is usually comperable in performance to slower and more expensive models on classification use cases.\nThe model is successfully trained in about ten minutes. You can watch the finetune happen on https://platform.openai.com/finetune/\n\nYou can also check on its status programatically:\n[Advanced] Results and expected model performance\nWe can now download the results file to observe the expected performance on a held out validation set.\nThe accuracy reaches 99.6%. On the plot below we can see how accuracy on the validation set increases during the training run.\nUsing the model\nWe can now call the model to get the predictions.\nWe need to use the same separator following the prompt which we used during fine-tuning. In this case it is \\n\\n###\\n\\n. Since we're concerned with classification, we want the temperature to be as low as possible, and we only require one token completion to determine the prediction of the model.\nTo get the log probabilities, we can specify logprobs parameter on the completion request\nWe can see that the model predicts hockey as a lot more likely than baseball, which is the correct prediction. By requesting log_probs, we can see the prediction (log) probability for each class.\nGeneralization\nInterestingly, our fine-tuned classifier is quite versatile. Despite being trained on emails to different mailing lists, it also successfully predicts tweets."])</script><script>self.__next_f.push([1,"25:T16d8,"])</script><script>self.__next_f.push([1,"\nThis notebook is centered around the integration of the Google Places API and custom user profiles to enhance location-based searches. Our approach involves using the Google Places API in combination with user preferences, aiming to make location discovery more personal and relevant. Please note that while we focus on the Google Places API in this instance, there are numerous other APIs you could explore and apply in a similar fashion.\n\nWe'll explore the application of three main components:\n\nCustomer profile: This mock profile captures individual preferences for types of places (e.g., restaurants, parks, museums), budget, preferred ratings, and other specific requirements.\n\nGoogle Places API: This API provides real-time data about nearby places. It factors in various data points such as ratings, types of venues, costs, and more from the locations around you.\n\nFunction calling: A single command such as \"I'm hungry\" or \"I want to visit a museum\" activates the function which combines the user profile data and Google Places API to identify suitable venues.\n\nThis notebook introduces two primary use cases:\n\nProfile-based recommendations: Learn how to create a user profile and make place recommendations based on individual preferences.\n\nAPI integration with function calling: Understand how to integrate and call Google Places API effectively to source real-time data of various places using function calling.\n\nPlease note that while this system is highly versatile, its effectiveness may vary based on user preferences and available place data. For the purposes of this notebook, the customer data is fake and the location is hardcoded.\nSetup\n\nGoogle Places API\n\nTo use the Google Places API, you'll need two things:\n\nGoogle Account: If you don't already have one, you will need to create a Google account.\n\nGoogle Places API Key: The API key is a unique identifier that is used to authenticate requests associated with your project for usage and billing purposes. You can get your API key from the Google Cloud Console.\n\nPlease note that Google Places API is a paid service, and the cost is associated with the number of API calls made. Keep track of your usage to avoid any unexpected charges.\n\n\n\nThe requests library is also needed, you can download it by using the following command:\n\npip install requests\nIn this code snippet, we are defining a function fetch_customer_profile that accepts a user_id and returns a mock user profile.\n\nThis function simulates an API call that fetches user data from a database. For this demo, we're using hard-coded data. The user profile contains various details such as the user's location (set to the coordinates of the Golden Gate Bridge for this example), preferences in food and activities, app usage metrics, recent interactions, and user rank.\n\nIn a production environment, you would replace this hard-coded data with a real API call to your user database.\n\nRequesting and processing data from Google Places API\n\nThe function call_google_places_api serves to request information from the Google Places API and provide a list of the top two places based on a given place_type and optional food_preference. We've limited this function to the top two results to manage usage since this is a paid service. However, you can modify this to retrieve any number of results as per your requirement.\n\nThe function is configured with a hardcoded location (set to the coordinates of the Transamerica Pyramid), your Google API key, and specific request parameters. Depending on the place_type, it formulates the appropriate API request URL. If the place_type is a restaurant and a food_preference is specified, it is included in the API request.\n\nAfter sending the GET request, the function checks the response status. If it's successful, it processes the JSON response, extracts the relevant details using the get_place_details function, and returns them in a human-readable format. If the request fails, it prints out the error for debugging.\n\nThe get_place_details function is used to retrieve more detailed information about a place, given its place_id. It sends a GET request to the Google Place Details API and returns the result if the request is successful. If the request fails, it prints out the error for debugging.\n\nBoth functions handle exceptions and return an error message if something goes wrong.\nGenerating user-specific recommendations with GPT-3.5-Turbo and Google Places API\n\nThe function provide_user_specific_recommendations interacts with GPT-3.5-Turbo and the Google Places API to provide responses tailored to a user's preferences and location.\n\nFirst, it fetches the customer's profile using their user_id. If no profile is found, it returns an error message.\n\nWith a valid profile, it extracts the customer's food preferences and then interacts with the OpenAI model. It provides an initial system message, giving context to the AI model about its role, user preferences, and the usage of the Google Places API function.\n\nThe user input is also sent to the model as a message, and the function call_google_places_api is defined in the functions parameter for the AI model to call as needed.\n\nFinally, it processes the model's response. If the model makes a function call to the Google Places API, the function is executed with the appropriate arguments, and the names of nearby places are returned. If there are no such places or the request isn't understood, appropriate error messages are returned.\n\nExecuting user-specific recommendations\n\nUpon execution, the function fetches the user's profile, interacts with the AI model, processes the model's response, calls the Google Places API if necessary, and ultimately returns a list of recommendations tailored to the user's preferences and location. The printed output would consist of these personalized recommendations."])</script><script>self.__next_f.push([1,"26:T159a,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through how to use LangChain to augment an OpenAI model with access to external tools. In particular, you'll be able to create LLM agents that use custom tools to answer user queries.\n\nWhat is Langchain?\nLangChain is a framework for developing applications powered by language models. Their framework enables you to build layered LLM-powered applications that are context-aware and able to interact dynamically with their environment as agents, leading to simplified code for you and a more dynamic user experience for your customers.\n\nWhy do LLMs need to use Tools?\nOne of the most common challenges with LLMs is overcoming the lack of recency and specificity in their training data - answers can be out of date, and they are prone to hallucinations given the huge variety in their knowledge base. Tools are a great method of allowing an LLM to answer within a controlled context that draws on your existing knowledge bases and internal APIs - instead of trying to prompt engineer the LLM all the way to your intended answer, you allow it access to tools that it calls on dynamically for info, parses, and serves to customer.\n\nProviding LLMs access to tools can enable them to answer questions with context directly from search engines, APIs or your own databases. Instead of answering directly, an LLM with access to tools can perform intermediate steps to gather relevant information. Tools can also be used in combination. For example, a language model can be made to use a search tool to lookup quantitative information and a calculator to execute calculations.\n\nNotebook Sections\n\nSetup:** Import packages and connect to a Pinecone vector database.\nLLM Agent:** Build an agent that leverages a modified version of the ReAct framework to do chain-of-thought reasoning.\nLLM Agent with History:** Provide the LLM with access to previous steps in the conversation.\nKnowledge Base:** Create a knowledge base of \"Stuff You Should Know\" podcast episodes, to be accessed through a tool.\nLLM Agent with Tools:** Extend the agent with access to multiple tools and test that it uses them to answer questions.\nSetup\n\nImport libraries and set up a connection to a Pinecone vector database.\n\nYou can substitute Pinecone for any other vectorstore or database - there are a selection that are supported by Langchain natively, while other connectors will need to be developed yourself.\nFor acquiring an API key to connect with Pinecone, you can set up a free account and store it in the api_key variable below or in your environment variables under PINECONE_API_KEY\nRun this code block if you want to clear the index, or if the index doesn't exist yet\n\nCheck whether the index with the same name already exists - if so, delete it\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n\nCreates new index\npinecone.create_index(name=index_name, dimension=1536)\nindex = pinecone.Index(index_name=index_name)\n\nConfirm our index was created\npinecone.list_indexes()\nLLM Agent\n\nAn LLM agent in Langchain has many configurable components, which are detailed in the Langchain documentation.\n\nWe'll employ a few of the core concepts to make an agent that talks in the way we want, can use tools to answer questions, and uses the appropriate language model to power the conversation.\nPrompt Template:** The input template to control the LLM's behaviour and how it accepts inputs and produces outputs - this is the brain that drives your application (docs).\nOutput Parser:** A method of parsing the output from the prompt. If the LLM produces output using certain headers, you can enable complex interactions where variables are generated by the LLM in their response and passed into the next step of the chain (docs).\nLLM Chain:* A Chain brings together a prompt template with an LLM that will execute it - in this case we'll be using Tool:* An external service that the LLM can use to retrieve information or execute commands should the user require it (docs).\nAgent:** The glue that brings all of this together, an agent can call multiple LLM Chains, each with their own tools. Agents can be extended with your own logic to allow retries, error handling and any other methods you choose to add reliability to your application (docs).\n\nNB: Before using this cookbook with the Search tool you'll need to sign up on https://serpapi.com/ and generate an API key. Once you have it, store it in an environment variable named ## LLM Agent with History\n\nExtend the LLM Agent with the ability to retain a memory and use it as context as it continues the conversation.\n\nWe use a simple ## Knowledge base\n\nCreate a custom vectorstore for the Agent to use as a tool to answer questions with. We'll store the results in Pinecone, which is supported by LangChain (Docs, API reference). For help getting started with Pinecone or other vector databases, we have a cookbook to help you get started.\n\nYou can check the LangChain documentation to see what other vectorstores and databases are available.\n\nFor this example we'll use the transcripts of the Stuff You Should Know podcast, which was provided thanks to OSF DOI 10.17605/OSF.IO/VM9NT\nLLM Agent with Tools\n\nExtend our list of tools by creating a RetrievalQA chain leveraging our Pinecone knowledge base.\nYou now have a template to deploy conversational agents with tools. If you want to extend this with a Custom Agent to add your own retry behaviour or treatment of input/output variables, then follow this article.\n\nWe look forward to seeing what you build!"])</script><script>self.__next_f.push([1,"27:T5e6,"])</script><script>self.__next_f.push([1,"\nThis notebook builds on the concepts in the argument generation notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n\nWe'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\nget_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\nread_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n\nThis will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n\nWalkthrough\n\nThis cookbook takes you through the following workflow:\n\nSearch utilities:** Creating the two functions that access arXiv for answers.\nConfigure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\narXiv conversation:** Put all of this together in live conversation.\n\nSearch utilities\n\nWe'll first set up some utilities that will underpin our two functions.\n\nDownloaded papers will be stored in a directory (we use ## Configure Agent\n\nWe'll create our agent in this step, including a ## arXiv conversation\n\nLet's put this all together by testing our functions out in conversation."])</script><script>self.__next_f.push([1,"28:T1269,"])</script><script>self.__next_f.push([1,"\nThis notebook covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n\ntools is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n\nWithin the tools parameter, if the functions parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can be forced to use a specific function by setting the tool_choice parameter to {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}. The API can also be forced to not use any function by setting the tool_choice parameter to \"none\". If a function is used, the output will contain \"finish_reason\": \"tool_calls\" in the response, as well as a tool_calls object that has the name of the function and the generated function arguments.\n\nOverview\n\nThis notebook contains the following 2 sections:\n\nHow to generate function arguments:** Specify a set of functions and use the API to generate function arguments.\nHow to call functions with model generated arguments:** Close the loop by actually executing functions with model generated arguments.\nHow to generate function arguments\nUtilities\n\nFirst let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state.\nBasic concepts\n\nLet's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification.\nIf we prompt the model about the current weather, it will respond with some clarifying questions.\nOnce we provide the missing information, it will generate the appropriate function arguments for us.\nBy prompting it differently, we can get it to target the other function we've told it about.\nOnce again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast.\nForcing the use of specific functions or no function\nWe can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it.\nWe can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call.\nParallel Function Calling\n\nNewer models such as gpt-4o or gpt-3.5-turbo can call multiple functions in one turn.\nHow to call functions with model generated arguments\n\nIn our next example, we'll demonstrate how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database. For simplicity we'll use the Chinook sample database.\n\nNote: SQL generation can be high-risk in a production environment since models are not perfectly reliable at generating correct SQL.\nSpecifying a function to execute SQL queries\n\nFirst let's define some helpful utility functions to extract data from a SQLite database.\nNow can use these utility functions to extract a representation of the database schema.\nAs before, we'll define a function specification for the function we'd like the API to generate arguments for. Notice that we are inserting the database schema into the function specification. This will be important for the model to know about.\nExecuting SQL queries\n\nNow let's implement the function that will actually excute queries against the database.\nSteps to invoke a function call using Chat Completions API:\n\nStep 1: Prompt the model with content that may result in model selecting a tool to use. The description of the tools such as a function names and signature is defined in the 'Tools' list and passed to the model in API call. If selected, the function name and parameters are included in the response.\n\nStep 2: Check programmatically if model wanted to call a function. If true, proceed to step 3.\nStep 3: Extract the function name and parameters from response, call the function with parameters. Append the result to messages.\nStep 4: Invoke the chat completions API with the message list to get the response.\nNext Steps\n\nSee our other notebook that demonstrates how to use the Chat Completions API and functions for knowledge retrieval to interact conversationally with a knowledge base."])</script><script>self.__next_f.push([1,"29:T10bc,"])</script><script>self.__next_f.push([1,"\ntiktoken is a fast open-source tokenizer by OpenAI.\n\nGiven a text string (e.g., \"tiktoken is great!\") and an encoding (e.g., \"cl100k_base\"), a tokenizer can split the text string into a list of tokens (e.g., [\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]).\n\nSplitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\n\nEncodings\n\nEncodings specify how text is converted into tokens. Different models use different encodings.\n\ntiktoken supports three encodings used by OpenAI models:\n\n| Encoding name           | OpenAI models                                       |\n|-------------------------|-----------------------------------------------------|\n| cl100k_base           | gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large  |\n| p50k_base             | Codex models, text-davinci-002, text-davinci-003|\n| r50k_base (or gpt2) | GPT-3 models like davinci                         |\n\nYou can retrieve the encoding for a model using tiktoken.encoding_for_model() as follows:\nencoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n\nNote that p50k_base overlaps substantially with r50k_base, and for non-code applications, they will usually give the same tokens.\n\nTokenizer libraries by language\n\nFor cl100k_base and p50k_base encodings:\nPython: tiktoken\n.NET / C#: SharpToken, TiktokenSharp\nJava: jtokkit\nGolang: tiktoken-go\nRust: tiktoken-rs\n\nFor r50k_base (gpt2) encodings, tokenizers are available in many languages.\nPython: tiktoken (or alternatively GPT2TokenizerFast)\nJavaScript: gpt-3-encoder\n.NET / C#: GPT Tokenizer\nJava: gpt2-tokenizer-java\nPHP: GPT-3-Encoder-PHP\nGolang: tiktoken-go\nRust: tiktoken-rs\n\n(OpenAI makes no endorsements or guarantees of third-party libraries.)\n\nHow strings are typically tokenized\n\nIn English, tokens commonly range in length from one character to one word (e.g., \"t\" or \" great\"), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., \" is\" instead of \"is \" or \" \"+\"is\"). You can quickly check how a string is tokenized at the OpenAI Tokenizer, or the third-party Tiktokenizer webapp.\n0. Install tiktoken\n\nIf needed, install tiktoken with pip:\n1. Import tiktoken\n2. Load an encoding\n\nUse tiktoken.get_encoding() to load an encoding by name.\n\nThe first time this runs, it will require an internet connection to download. Later runs won't need an internet connection.\nUse tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.\n3. Turn text into tokens with encoding.encode()\n\n\nThe .encode() method converts a text string into a list of token integers.\nCount tokens by counting the length of the list returned by .encode().\n4. Turn tokens into text with encoding.decode()\n.decode() converts a list of token integers to a string.\nWarning: although .decode() can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.\nFor single tokens, .decode_single_token_bytes() safely converts a single integer token to the bytes it represents.\n(The b in front of the strings indicates that the strings are byte strings.)\n5. Comparing encodings\n\nDifferent encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings.\n6. Counting tokens for chat completions API calls\n\nChatGPT models like gpt-3.5-turbo and gpt-4 use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n\nBelow is an example function for counting tokens for messages passed to gpt-3.5-turbo or gpt-4.\n\nNote that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n\nIn particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below."])</script><script>self.__next_f.push([1,"2a:T139c,"])</script><script>self.__next_f.push([1,"\nFine-tuning improves the model by training on many more examples than can fit in a prompt, letting you achieve better results on a wide number of tasks. This notebook provides a step-by-step guide for our new GPT-4o mini fine-tuning. We'll perform entity extraction using the RecipeNLG dataset, which provides various recipes and a list of extracted generic ingredients for each. This is a common dataset for named entity recognition (NER) tasks.\n\nNote: GPT-4o mini fine-tuning is available to developers in our Tier 4 and 5 usage tiers. You can start fine-tuning GPT-4o mini by visiting your fine-tuning dashboard, clicking \"create\", and selecting “gpt-4o-mini-2024-07-18” from the base model drop-down.\n\nWe will go through the following steps:\n\nSetup: Loading our dataset and filtering down to one domain to fine-tune on.\nData preparation: Preparing your data for fine-tuning by creating training and validation examples, and uploading them to the Files endpoint.\nFine-tuning: Creating your fine-tuned model.\nInference: Using your fine-tuned model for inference on new inputs.\n\nBy the end of this you should be able to train, evaluate and deploy a fine-tuned gpt-4o-mini-2024-07-18 model.\n\nFor more information on fine-tuning, you can refer to our documentation guide or API reference.\n\nSetup\n\nFine-tuning works best when focused on a particular domain. It's important to make sure your dataset is both focused enough for the model to learn, but general enough that unseen examples won't be missed. Having this in mind, we have extracted a subset from the RecipesNLG dataset to only contain documents from cookbooks.com.\n\nData preparation\n\nWe'll begin by preparing our data. When fine-tuning with the ChatCompletion format, each training example is a simple list of messages. For example, an entry could look like:\n\n[{'role': 'system',\n  'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'},\n\n {'role': 'user',\n  'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '},\n\n {'role': 'assistant',\n  'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}]\n\nDuring the training process this conversation will be split, with the final entry being the completion that the model will produce, and the remainder of the messages acting as the prompt. Consider this when building your training examples - if your model will act on multi-turn conversations, then please provide representative examples so it doesn't perform poorly when the conversation starts to expand.\n\nPlease note that currently there is a 4096 token limit for each training example. Anything longer than this will be truncated at 4096 tokens.\n\nLet's now do this for a subset of the dataset to use as our training data. You can begin with even 30-50 well-pruned examples. You should see performance continue to scale linearly as you increase the size of the training set, but your jobs will also take longer.\n\nIn addition to training data, we can also optionally provide validation data, which will be used to make sure that the model does not overfit your training set.\n\nWe then need to save our data as .jsonl files, with each line being one training example conversation.\n\nThis is what the first 5 lines of our training .jsonl file look like:\n\nUpload files\n\nYou can now upload the files to our Files endpoint to be used by the fine-tuned model.\n\nFine-tuning\n\nNow we can create our fine-tuning job with the generated files and an optional suffix to identify the model. The response will contain an id which you can use to retrieve updates on the job.\n\nNote: The files have to first be processed by our system, so you might get a File not ready error. In that case, simply retry a few minutes later.\n\nCheck job status\n\nYou can make a GET request to the https://api.openai.com/v1/alpha/fine-tunes endpoint to list your alpha fine-tune jobs. In this instance you'll want to check that the ID you got from the previous step ends up as status: succeeded.\n\nOnce it is completed, you can use the result_files to sample the results from the validation set (if you uploaded one), and use the ID from the fine_tuned_model parameter to invoke your trained model.\n\nWe can track the progress of the fine-tune with the events endpoint. You can rerun the cell below a few times until the fine-tune is ready.\n\nNow that it's done, we can get a fine-tuned model ID from the job:\n\nInference\n\nThe last step is to use your fine-tuned model for inference. Similar to the classic FineTuning, you simply call ChatCompletions with your new fine-tuned model name filling the model parameter.\n\nConclusion\n\nCongratulations, you are now ready to fine-tune your own models using the ChatCompletion format! We look forward to seeing what you build"])</script><script>self.__next_f.push([1,"2b:T19a4,"])</script><script>self.__next_f.push([1,"\nChatGPT is powered by gpt-3.5-turbo and gpt-4, OpenAI's most advanced models.\n\nYou can build your own applications with gpt-3.5-turbo or gpt-4 using the OpenAI API.\n\nChat models take a series of messages as input, and return an AI-written message as output.\n\nThis guide illustrates the chat format with a few example API calls.\n1. Import the openai library\n2. An example chat completion API call\n\nA chat completion API call parameters,\nRequired\nmodel: the name of the model you want to use (e.g., gpt-3.5-turbo, gpt-4, gpt-3.5-turbo-16k-1106)\nmessages: a list of message objects, where each object has two required fields:\n    role: the role of the messenger (either system, user, assistant or tool)\n    content: the content of the message (e.g., Write me a beautiful poem)\n\nMessages can also contain an optional name field, which give the messenger a name. E.g., example-user, Alice, BlackbeardBot. Names may not contain spaces.\n\nOptional\nfrequency_penalty: Penalizes tokens based on their frequency, reducing repetition.\nlogit_bias: Modifies likelihood of specified tokens with bias values.\nlogprobs: Returns log probabilities of output tokens if true.\ntop_logprobs: Specifies the number of most likely tokens to return at each position.\nmax_tokens: Sets the maximum number of generated tokens in chat completion.\nn: Generates a specified number of chat completion choices for each input.\npresence_penalty: Penalizes new tokens based on their presence in the text.\nresponse_format: Specifies the output format, e.g., JSON mode.\nseed: Ensures deterministic sampling with a specified seed.\nstop: Specifies up to 4 sequences where the API should stop generating tokens.\nstream: Sends partial message deltas as tokens become available.\ntemperature: Sets the sampling temperature between 0 and 2.\ntop_p: Uses nucleus sampling; considers tokens with top_p probability mass.\ntools: Lists functions the model may call.\ntool_choice: Controls the model's function calls (none/auto/function).\nuser: Unique identifier for end-user monitoring and abuse detection.\n\n\nAs of January 2024, you can also optionally submit a list of functions that tell GPT whether it can generate JSON to feed into a function. For details, see the documentation, API reference, or the Cookbook guide How to call functions with chat models.\n\nTypically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n\nLet's look at an example chat API calls to see how the chat format works in practice.\nAs you can see, the response object has a few fields:\nid: the ID of the request\nchoices: a list of completion objects (only one, unless you set n greater than 1)\n    finish_reason: the reason the model stopped generating text (either stop, or length if max_tokens limit was reached)\n    index: The index of the choice in the list of choices.\n    logprobs: Log probability information for the choice.\n    message: the message object generated by the model\n        content: content of message\n        role: The role of the author of this message.\n        tool_calls: The tool calls generated by the model, such as function calls. if the tools is given\ncreated: the timestamp of the request\nmodel: the full name of the model used to generate the response\nobject: the type of object returned (e.g., chat.completion)\nsystem_fingerprint: This fingerprint represents the backend configuration that the model runs with.\nusage: the number of tokens used to generate the replies, counting prompt, completion, and total\nExtract just the reply with:\nEven non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n\nFor example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:\n3. Tips for instructing gpt-3.5-turbo-0301\n\nBest practices for instructing models may change from model version to model version. The advice that follows applies to gpt-3.5-turbo-0301 and may not apply to future models.\nSystem messages\n\nThe system message can be used to prime the assistant with different personalities or behaviors.\n\nBe aware that gpt-3.5-turbo-0301 does not generally pay as much attention to the system message as gpt-4-0314 or gpt-3.5-turbo-0613. Therefore, for gpt-3.5-turbo-0301, we recommend placing important instructions in the user message instead. Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer.\nFew-shot prompting\n\nIn some cases, it's easier to show the model what you want rather than tell the model what you want.\n\nOne way to show the model what you want is with faked example messages.\n\nFor example:\nTo help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can try setting the name field of system messages to example_user and example_assistant.\n\nTransforming the few-shot example above, we could write:\nNot every attempt at engineering conversations will succeed at first.\n\nIf your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n\nAs an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n\nFor more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.\n4. Counting tokens\n\nWhen you submit your request, the API transforms the messages into a sequence of tokens.\n\nThe number of tokens used affects:\nthe cost of the request\nthe time it takes to generate the response\nwhen the reply gets cut off from hitting the maximum token limit (4,096 for gpt-3.5-turbo or 8,192 for gpt-4)\n\nYou can use the following function to count the number of tokens that a list of messages will use.\n\nNote that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n\nIn particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below.\n\nRead more about counting tokens in How to count tokens with tiktoken."])</script><script>self.__next_f.push([1,"2c:T1b64,"])</script><script>self.__next_f.push([1,"\nWhen you call the OpenAI API repeatedly, you may encounter error messages that say 429: 'Too Many Requests' or RateLimitError. These error messages come from exceeding the API's rate limits.\n\nThis guide shares tips for avoiding and handling rate limit errors.\n\nTo see an example script for throttling parallel requests to avoid rate limit errors, see api_request_parallel_processor.py.\n\nWhy rate limits exist\n\nRate limits are a common practice for APIs, and they're put in place for a few different reasons.\n\nFirst, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\nSecond, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns.\nLastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n\nAlthough hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users.\nDefault rate limits\n\nYour rate limit and spending limit (quota) are automatically adjusted based on a number of factors. As your usage of the OpenAI API goes up and you successfully pay the bill, we automatically increase your usage tier. You can find specific information regarding rate limits using the resources below.\n\nOther rate limit resources\n\nRead more about OpenAI's rate limits in these other resources:\n\nGuide: Rate limits\nHelp Center: Is API usage subject to any rate limits?\nHelp Center: How can I solve 429: 'Too Many Requests' errors?\n\nRequesting a rate limit increase\n\nIf you'd like your organization's rate limit increased, please visit your Limits settings page to see how you can increase your usage tier\n\nExample rate limit error\n\nA rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:\n\nRateLimitError: Rate limit reached for default-codex in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. Contact support@openai.com if you continue to have issues or if you’d like to request an increase.\n\nBelow is example code for triggering a rate limit error.\nHow to avoid rate limit errors\n\nRetrying with exponential backoff\n\nOne easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\n\nThis approach has many benefits:\n\nAutomatic retries means you can recover from rate limit errors without crashes or missing data\nExponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\nAdding random jitter to the delay helps retries from all hitting at the same time\n\nNote that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n\nBelow are a few example solutions.\nExample #1: Using the Tenacity library\n\nTenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\n\nTo add exponential backoff to your requests, you can use the tenacity.retry decorator. The following example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.\n\nNote that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\nExample #2: Using the backoff library\n\nAnother library that provides function decorators for backoff and retry is backoff.\n\nLike Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.\nExample 3: Manual backoff implementation\n\nIf you don't want to use third-party libraries, you can implement your own backoff logic.\nHow to maximize throughput of batch processing given rate limits\n\nIf you're processing real-time requests from users, backoff and retry is a great strategy to minimize latency while avoiding rate limit errors.\n\nHowever, if you're processing large volumes of batch data, where throughput matters more than latency, there are a few other things you can do in addition to backoff and retry.\n\nProactively adding delay between requests\n\nIf you are constantly hitting the rate limit, then backing off, then hitting the rate limit again, then backing off again, it's possible that a good fraction of your request budget will be 'wasted' on requests that need to be retried. This limits your processing throughput, given a fixed rate limit.\n\nHere, one potential solution is to calculate your rate limit and add a delay equal to its reciprocal (e.g., if your rate limit 20 requests per minute, add a delay of 3–6 seconds to each request). This can help you operate near the rate limit ceiling without hitting it and incurring wasted requests.\n\nExample of adding delay to a request\n\nBatching requests\n\nThe OpenAI API has separate limits for requests per minute and tokens per minute.\n\nIf you're hitting the limit on requests per minute, but have headroom on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with the smaller models.\n\nSending in a batch of prompts works exactly the same as a normal API call, except that pass in a list of strings to prompt parameter instead of a single string.\n\nWarning: the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the index field.\n\nExample without batching\nExample with batching\nExample parallel processing script\n\nWe've written an example script for parallel processing large quantities of API requests: api_request_parallel_processor.py.\n\nThe script combines some handy features:\nStreams requests from file, to avoid running out of memory for giant jobs\nMakes requests concurrently, to maximize throughput\nThrottles both request and token usage, to stay under rate limits\nRetries failed requests, to avoid missing data\nLogs errors, to diagnose problems with requests\n\nFeel free to use it as is or modify it to suit your needs."])</script><script>self.__next_f.push([1,"2d:Tc2d,"])</script><script>self.__next_f.push([1,"\nBy default, when you request a completion from the OpenAI, the entire completion is generated before being sent back in a single response.\n\nIf you're generating long completions, waiting for the response can take many seconds.\n\nTo get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.\n\nTo stream completions, set stream=True when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the delta field rather than the message field.\n\nDownsides\n\nNote that using stream=True in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. This may have implications for approved usage.\n\nExample code\n\nBelow, this notebook shows:\nWhat a typical chat completion response looks like\nWhat a streaming chat completion response looks like\nHow much time is saved by streaming a chat completion\nHow to get token usage data for streamed chat completion response\n1. What a typical chat completion response looks like\n\nWith a typical ChatCompletions API call, the response is first computed and then returned all at once.\nThe reply can be extracted with response.choices[0].message.\n\nThe content of the reply can be extracted with response.choices[0].message.content.\n2. How to stream a chat completion\n\nWith a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a for loop.\n\nLet's see what it looks like:\nAs you can see above, streaming responses have a delta field rather than a message field. delta can hold things like:\na role token (e.g., {\"role\": \"assistant\"})\na content token (e.g., {\"content\": \"\\n\\n\"})\nnothing (e.g., {}), when the stream is over\n3. How much time is saved by streaming a chat completion\n\nNow let's ask gpt-4o-mini to count to 100 again, and see how long it takes.\nTime comparison\n\nIn the example above, both requests took about 4 to 5 seconds to fully complete. Request times will vary depending on load and other stochastic factors.\n\nHowever, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.\n4. How to get token usage data for streamed chat completion response\n\nYou can get token usage statistics for your streamed response by setting stream_options={\"include_usage\": True}. When you do so, an extra chunk will be streamed as the final chunk. You can access the usage data for the entire request via the usage field on this chunk. A few important notes when you set stream_options={\"include_usage\": True}:\nThe value for the usage field on all chunks except for the last one will be null.\nThe usage field on the last chunk contains token usage statistics for the entire request.\nThe choices field on the last chunk will always be an empty array [].\n\nLet's see how it works using the example in 2."])</script><script>self.__next_f.push([1,"2e:T1449,"])</script><script>self.__next_f.push([1,"\nFor this notebook we will be looking to classify a public dataset of transactions into a number of categories that we have predefined. These approaches should be replicable to any multiclass classification use case where we are trying to fit transactional data into predefined categories, and by the end of running through this you should have a few approaches for dealing with both labelled and unlabelled datasets.\n\nThe different approaches we'll be taking in this notebook are:\nZero-shot Classification:** First we'll do zero shot classification to put transactions in one of five named buckets using only a prompt for guidance\nClassification with Embeddings:** Following this we'll create embeddings on a labelled dataset, and then use a traditional classification model to test their effectiveness at identifying our categories\nFine-tuned Classification:** Lastly we'll produce a fine-tuned model trained on our labelled dataset to see how this compares to the zero-shot and few-shot classification approaches\nSetup\nLoad dataset\n\nWe're using a public transaction dataset of transactions over £25k for the Library of Scotland. The dataset has three features that we'll be using:\nSupplier: The name of the supplier\nDescription: A text description of the transaction\nValue: The value of the transaction in GBP\n\nSource:\n\nhttps://data.nls.uk/data/organisational-data/transactions-over-25k/\nZero-shot Classification\n\nWe'll first assess the performance of the base models at classifying these transactions using a simple prompt. We'll provide the model with 5 categories and a catch-all of \"Could not classify\" for ones that it cannot place.\nOur first attempt is correct, M \u0026 J Ballantyne Ltd are a house builder and the work they performed is indeed Building Improvement.\n\nLets expand the sample size to 25 and see how it performs, again with just a simple prompt to guide it\nInitial results are pretty good even with no labelled examples! The ones that it could not classify were tougher cases with few clues as to their topic, but maybe if we clean up the labelled dataset to give more examples we can get better performance.\nClassification with Embeddings\n\nLets create embeddings from the small set that we've classified so far - we've made a set of labelled examples by running the zero-shot classifier on 101 transactions from our dataset and manually correcting the 15 Could not classify results that we got\n\nCreate embeddings\n\nThis initial section reuses the approach from the Get_embeddings_from_dataset Notebook to create embeddings from a combined field concatenating all of our features\nUse embeddings for classification\n\nNow that we have our embeddings, let see if classifying these into the categories we've named gives us any more success.\n\nFor this we'll use a template from the Classification_using_embeddings notebook\nPerformance for this model is pretty strong, so creating embeddings and using even a simpler classifier looks like an effective approach as well, with the zero-shot classifier helping us do the initial classification of the unlabelled dataset.\n\nLets take it one step further and see if a fine-tuned model trained on this same labelled datasets gives us comparable results\nFine-tuned Transaction Classification\n\nFor this use case we're going to try to improve on the few-shot classification from above by training a fine-tuned model on the same labelled set of 101 transactions and applying this fine-tuned model on group of unseen transactions\nBuilding Fine-tuned Classifier\n\nWe'll need to do some data prep first to get our data ready. This will take the following steps:\nFirst we'll list out our classes and replace them with numeric identifiers. Making the model predict a single token rather than multiple consecutive ones like 'Building Improvement' should give us better results\nWe also need to add a common prefix and suffix to each example to aid the model in making predictions - in our case our text is already started with 'Supplier' and we'll add a suffix of '\\n\\n###\\n\\n'\nLastly we'll aid a leading whitespace onto each of our target classes for classification, again to aid the model\nApplying Fine-tuned Classifier\n\nNow we'll apply our classifier to see how it performs. We only had 31 unique observations in our training set and 8 in our validation set, so lets see how the performance is\nPerformance is not great - unfortunately this is expected. With only a few examples of each class, the above approach with embeddings and a traditional classifier worked better.\n\nA fine-tuned model works best with a great number of labelled observations. If we had a few hundred or thousand we may get better results, but lets do one last test on a holdout set to confirm that it doesn't generalise well to a new set of observations\nWell those results were similarly underwhelming - so we've learned that with a dataset with a small number of labelled observations, either zero-shot classification or traditional classification with embeddings return better results than a fine-tuned model.\n\nA fine-tuned model is still a great tool, but is more effective when you have a larger number of labelled examples for each class that you're looking to classify"])</script><script>self.__next_f.push([1,"2f:Te4f,"])</script><script>self.__next_f.push([1,"\nSearching for relevant information can sometimes feel like looking for a needle in a haystack, but don’t despair, GPTs can actually do a lot of this work for us. In this guide we explore a way to augment existing search systems with various AI techniques, helping us sift through the noise.\n\nTwo ways of retrieving information for GPT are:\n\nMimicking Human Browsing: GPT triggers a search, evaluates the results, and modifies the search query if necessary. It can also follow up on specific search results to form a chain of thought, much like a human user would do.\nRetrieval with Embeddings: Calculate embeddings for your content and a user query, and then retrieve the content most related as measured by cosine similarity. This technique is used heavily by search engines like Google.\n\nThese approaches are both promising, but each has their shortcomings: the first one can be slow due to its iterative nature and the second one requires embedding your entire knowledge base in advance, continuously embedding new content and maintaining a vector database.\n\nBy combining these approaches, and drawing inspiration from re-ranking methods, we identify an approach that sits in the middle. This approach can be implemented on top of any existing search system, like the Slack search API, or an internal ElasticSearch instance with private data. Here’s how it works:\n\nsearch_augmented_by_query_generation_and_embeddings_reranking.png\n\nStep 1: Search\n\nUser asks a question.\nGPT generates a list of potential queries.\nSearch queries are executed in parallel.\n\nStep 2: Re-rank\n\nEmbeddings for each result are used to calculate semantic similarity to a generated hypothetical ideal answer to the user question.\nResults are ranked and filtered based on this similarity metric.\n\nStep 3: Answer\n\nGiven the top search results, the model generates an answer to the user’s question, including references and links.\n\nThis hybrid approach offers relatively low latency and can be integrated into any existing search endpoint, without requiring the upkeep of a vector database. Let's dive into it! We will use the News API as an example domain to search over.\n\nSetup\n\nIn addition to your OPENAI_API_KEY, you'll have to include a NEWS_API_KEY in your environment. You can get an API key here.\n\n1. Search\n\nIt all starts with a user question.\n\nNow, in order to be as exhaustive as possible, we use the model to generate a list of diverse queries based on this question.\n\nThe queries look good, so let's run the searches.\n\nAs we can see, oftentimes, the search queries will return a large number of results, many of which are not relevant to the original question asked by the user. In order to improve the quality of the final answer, we use embeddings to re-rank and filter the results.\n\n2. Re-rank\n\nDrawing inspiration from HyDE (Gao et al.), we first generate a hypothetical ideal answer to rerank our compare our results against. This helps prioritize results that look like good answers, rather than those similar to our question. Here’s the prompt we use to generate our hypothetical answer.\n\nNow, let's generate embeddings for the search results and the hypothetical answer. We then calculate the cosine distance between these embeddings, giving us a semantic similarity metric. Note that we can simply calculate the dot product in lieu of doing a full cosine similarity calculation since the OpenAI embeddings are returned normalized in our API.\n\nFinally, we use these similarity scores to sort and filter the results.\n\nAwesome! These results look a lot more relevant to our original query. Now, let's use the top 5 results to generate a final answer.\n\n3. Answer"])</script><script>self.__next_f.push([1,"30:T2291,"])</script><script>self.__next_f.push([1,"\nGPT excels at answering questions, but only on topics it remembers from its training data.\n\nWhat should you do if you want GPT to answer questions about unfamiliar topics? E.g.,\nRecent events after Sep 2021\nYour non-public documents\nInformation from past conversations\netc.\n\nThis notebook demonstrates a two-step Search-Ask method for enabling GPT to answer questions using a library of reference text.\n\nSearch: search your library of text for relevant text sections\nAsk: insert the retrieved text sections into a message to GPT and ask it the question\nWhy search is better than fine-tuning\n\nGPT can learn knowledge in two ways:\n\nVia model weights (i.e., fine-tune the model on a training set)\nVia model inputs (i.e., insert the knowledge into an input message)\n\nAlthough fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n\nAs an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n\nIn contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n\nOne downside of text search relative to fine-tuning is that each model is limited by a maximum amount of text it can read at once:\n\n| Model           | Maximum text length       |\n|-----------------|---------------------------|\n| gpt-3.5-turbo | 4,096 tokens (~5 pages)   |\n| gpt-4         | 8,192 tokens (~10 pages)  |\n| gpt-4-32k     | 32,768 tokens (~40 pages) |\n\n(New model is available with longer contexts, gpt-4-1106-preview have 128K context window)\n\nContinuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.\n\nTherefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.\n\nSearch\n\nText can be searched in many ways. E.g.,\n\nLexical-based search\nGraph-based search\nEmbedding-based search\n\nThis example notebook uses embedding-based search. Embeddings are simple to implement and work especially well with questions, as questions often don't lexically overlap with their answers.\n\nConsider embeddings-only search as a starting point for your own system. Better search systems might combine multiple search methods, along with features like popularity, recency, user history, redundancy with prior search results, click rate data, etc. Q\u0026A retrieval performance may also be improved with techniques like HyDE, in which questions are first transformed into hypothetical answers before being embedded. Similarly, GPT can also potentially improve search results by automatically transforming questions into sets of keywords or search terms.\nFull procedure\n\nSpecifically, this notebook demonstrates the following procedure:\n\nPrepare search data (once per document)\n    Collect: We'll download a few hundred Wikipedia articles about the 2022 Olympics\n    Chunk: Documents are split into short, mostly self-contained sections to be embedded\n    Embed: Each section is embedded with the OpenAI API\n    Store: Embeddings are saved (for large datasets, use a vector database)\nSearch (once per query)\n    Given a user question, generate an embedding for the query from the OpenAI API\n    Using the embeddings, rank the text sections by relevance to the query\nAsk (once per query)\n    Insert the question and the most relevant sections into a message to GPT\n    Return GPT's answer\n\nCosts\n\nBecause GPT is more expensive than embeddings search, a system with a decent volume of queries will have its costs dominated by step 3.\n\nFor gpt-3.5-turbo using 1,000 tokens per query, it costs $0.002 per query, or ~500 queries per dollar (as of Apr 2023)\nFor gpt-4, again assuming 1,000 tokens per query, it costs $0.03 per query, or ~30 queries per dollar (as of Apr 2023)\n\nOf course, exact costs will depend on the system specifics and usage patterns.\nPreamble\n\nWe'll begin by:\nImporting the necessary libraries\nSelecting models for embeddings search and question answering\n\nTroubleshooting: Installing libraries\n\nIf you need to install any of the libraries above, run pip install {library_name} in your terminal.\n\nFor example, to install the openai library, run:\npip install openai\n\n(You can also do this in a notebook cell with !pip install openai or %pip install openai.)\n\nAfter installing, restart the notebook kernel so the libraries can be loaded.\n\nTroubleshooting: Setting your API key\n\nThe OpenAI library will try to read your API key from the OPENAI_API_KEY environment variable. If you haven't already, you can set this environment variable by following these instructions.\nMotivating example: GPT cannot answer questions about current events\n\nBecause the training data for gpt-3.5-turbo and gpt-4 mostly ends in September 2021, the models cannot answer questions about more recent events, such as the 2022 Winter Olympics.\n\nFor example, let's try asking 'Which athletes won the gold medal in curling in 2022?':\nIn this case, the model has no knowledge of 2022 and is unable to answer the question.\n\nYou can give GPT knowledge about a topic by inserting it into an input message\n\nTo help give the model knowledge of curling at the 2022 Winter Olympics, we can copy and paste the top half of a relevant Wikipedia article into our message:\nThanks to the Wikipedia article included in the input message, GPT answers correctly.\n\nIn this particular case, GPT was intelligent enough to realize that the original question was underspecified, as there were three curling gold medal events, not just one.\n\nOf course, this example partly relied on human intelligence. We knew the question was about curling, so we inserted a Wikipedia article on curling.\n\nThe rest of this notebook shows how to automate this knowledge insertion with embeddings-based search.\n1. Prepare search data\n\nTo save you the time \u0026 expense, we've prepared a pre-embedded dataset of a few hundred Wikipedia articles about the 2022 Winter Olympics.\n\nTo see how we constructed this dataset, or to modify it yourself, see Embedding Wikipedia articles for search.\n2. Search\n\nNow we'll define a search function that:\nTakes a user query and a dataframe with text \u0026 embedding columns\nEmbeds the user query with the OpenAI API\nUses distance between query embedding and text embeddings to rank the texts\nReturns two lists:\n    The top N texts, ranked by relevance\n    Their corresponding relevance scores\n3. Ask\n\nWith the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.\n\nBelow, we define a function ask that:\nTakes a user query\nSearches for text relevant to the query\nStuffs that text into a message for GPT\nSends the message to GPT\nReturns GPT's answer\nExample questions\n\nFinally, let's ask our system our original question about gold medal curlers:\nDespite gpt-3.5-turbo having no knowledge of the 2022 Winter Olympics, our search system was able to retrieve reference text for the model to read, allowing it to correctly list the gold medal winners in the Men's and Women's tournaments.\n\nHowever, it still wasn't quite perfect—the model failed to list the gold medal winners from the Mixed doubles event.\nTroubleshooting wrong answers\nTo see whether a mistake is from a lack of relevant source text (i.e., failure of the search step) or a lack of reasoning reliability (i.e., failure of the ask step), you can look at the text GPT was given by setting print_message=True.\n\nIn this particular case, looking at the text below, it looks like the #1 article given to the model did contain medalists for all three events, but the later results emphasized the Men's and Women's tournaments, which may have distracted the model from giving a more complete answer.\nKnowing that this mistake was due to imperfect reasoning in the ask step, rather than imperfect retrieval in the search step, let's focus on improving the ask step.\n\nThe easiest way to improve results is to use a more capable model, such as GPT-4. Let's try it.\nGPT-4 succeeds perfectly, correctly identifying all 12 gold medal winners in curling.\nMore examples\n\nBelow are a few more examples of the system in action. Feel free to try your own questions, and see how it does. In general, search-based systems do best on questions that have a simple lookup, and worst on questions that require multiple partial sources to be combined and reasoned about."])</script><script>self.__next_f.push([1,"31:T11e1,"])</script><script>self.__next_f.push([1,"\nRecommendations are widespread across the web.\n\n'Bought that item? Try these similar items.'\n'Enjoy that book? Try these similar titles.'\n'Not the help page you were looking for? Try these similar pages.'\n\nThis notebook demonstrates how to use embeddings to find similar items to recommend. In particular, we use AG's corpus of news articles as our dataset.\n\nOur model will answer the question: given an article, what other articles are most similar to it?\n2. Load data\n\nNext, let's load the AG news data and see what it looks like.\nLet's take a look at those same examples, but not truncated by ellipses.\n3. Build cache to save embeddings\n\nBefore getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.\n\nThe cache is a dictionary that maps tuples of (text, model) to an embedding, which is a list of floats. The cache is saved as a Python pickle file.\nLet's check that it works by getting an embedding.\n4. Recommend similar articles based on embeddings\n\nTo find similar articles, let's follow a three-step plan:\nGet the similarity embeddings of all the article descriptions\nCalculate the distance between a source title and all other articles\nPrint out the other articles closest to the source title\n5. Example recommendations\n\nLet's look for articles similar to first one, which was about Tony Blair.\nPretty good! 4 of the 5 recommendations explicitly mention Tony Blair and the fifth is an article from London about climate change, topics that might be often associated with Tony Blair.\nLet's see how our recommender does on the second example article about NVIDIA's new chipset with more security.\nFrom the printed distances, you can see that the #1 recommendation is much closer than all the others (0.11 vs 0.14+). And the #1 recommendation looks very similar to the starting article - it's another article from PC World about increasing computer security. Pretty good!\nAppendix: Using embeddings in more sophisticated recommenders\n\nA more sophisticated way to build a recommender system is to train a machine learning model that takes in tens or hundreds of signals, such as item popularity or user click data. Even in this system, embeddings can be a very useful signal into the recommender, especially for items that are being 'cold started' with no user data yet (e.g., a brand new product added to the catalog without any clicks yet).\nAppendix: Using embeddings to visualize similar articles\nTo get a sense of what our nearest neighbor recommender is doing, let's visualize the article embeddings. Although we can't plot the 2048 dimensions of each embedding vector, we can use techniques like t-SNE or PCA to compress the embeddings down into 2 or 3 dimensions, which we can chart.\n\nBefore visualizing the nearest neighbors, let's visualize all of the article descriptions using t-SNE. Note that t-SNE is not deterministic, meaning that results may vary from run to run.\nAs you can see in the chart above, even the highly compressed embeddings do a good job of clustering article descriptions by category. And it's worth emphasizing: this clustering is done with no knowledge of the labels themselves!\n\nAlso, if you look closely at the most egregious outliers, they are often due to mislabeling rather than poor embedding. For example, the majority of the blue World points in the green Sports cluster appear to be Sports stories.\nNext, let's recolor the points by whether they are a source article, its nearest neighbors, or other.\nLooking at the 2D chart above, we can see that the articles about Tony Blair are somewhat close together inside of the World news cluster. Interestingly, although the 5 nearest neighbors (red) were closest in high dimensional space, they are not the closest points in this compressed 2D space. Compressing the embeddings down to 2 dimensions discards much of their information, and the nearest neighbors in the 2D space don't seem to be as relevant as those in the full embedding space.\nFor the chipset security example, the 4 closest nearest neighbors in the full embedding space remain nearest neighbors in this compressed 2D visualization. The fifth is displayed as more distant, despite being closer in the full embedding space.\nShould you want to, you can also make an interactive 3D plot of the embeddings with the function chart_from_components_3D. (Doing so will require recomputing the t-SNE components with n_components=3.)"])</script><script>self.__next_f.push([1,"32:Te0e,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through examples of using a cross-encoder to re-rank search results.\n\nThis is a common use case with our customers, where you've implemented semantic search using embeddings (produced using a bi-encoder) but the results are not as accurate as your use case requires. A possible cause is that there is some business rule you can use to rerank the documents such as how recent or how popular a document is.\n\nHowever, often there are subtle domain-specific rules that help determine relevancy, and this is where a cross-encoder can be useful. Cross-encoders are more accurate than bi-encoders but they don't scale well, so using them to re-order a shortened list returned by semantic search is the ideal use case.\n\nExample\n\nConsider a search task with D documents and Q queries.\n\nThe brute force approach of computing every pairwise relevance is expensive; its cost scales as\nA faster approach is embeddings-based search, in which an embedding is computed once for each document and query, and then re-used multiple times to cheaply compute pairwise relevance. Because embeddings are only computed once, its cost scales as\nAlthough embeddings-based search is faster, the quality can be worse. To get the best of both, one common approach is to use embeddings (or another bi-encoder) to cheaply identify top candidates, and then use GPT (or another cross-encoder) to expensively re-rank those top candidates. The cost of this hybrid approach scales as\nWalkthrough\n\nTo illustrate this approach we'll use\nThis notebook drew on this great article by Weaviate, and this excellent explanation of bi-encoders vs. cross-encoders from Sentence Transformers.\nSearch\n\nWe'll use the arXiv search service for this example, but this step could be performed by any search service you have. The key item to consider is over-fetching slightly to capture all the potentially relevant documents, before re-sorting them.\n\nCross-encoder\n\nWe'll create a cross-encoder using the Make your examples domain-specific - the strength of cross-encoders comes when you tailor them to your domain.\nThere is a trade-off between how many potential examples to re-rank vs. processing speed. Consider batching and parallel processing cross-encoder requests to process them more quickly.\n\nThe steps here are:\nBuild a prompt to assess relevance and provide few-shot examples to tune it to your domain.\nAdd a Return the classification of yes/no as well as the Rerank the results by the ## Conclusion\n\nWe've shown how to create a tailored cross-encoder to rerank academic papers. This approach will work best where there are domain-specific nuances that can be used to pick the most relevant corpus for your users, and where some pre-filtering has taken place to limit the amount of data the cross-encoder will need to process.\n\nA few typical use cases we've seen are:\nReturning a list of 100 most relevant stock reports, then re-ordering into a top 5 or 10 based on the detailed context of a particular set of customer portfolios\nRunning after a classic rules-based search that gets the top 100 or 1000 most relevant results to prune it according to a specific user's context\n\nTaking this forward\n\nTaking the few-shot approach, as we have here, can work well when the domain is general enough that a small number of examples will cover most reranking cases. However, as the differences between documents become more specific you may want to consider the\nThere is also a latency impact of using\nWe've used the\nWe hope you find this useful for tuning your search use cases, and look forward to seeing what you build."])</script><script>self.__next_f.push([1,"33:T5aa,"])</script><script>self.__next_f.push([1,"\nComplex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT and then feeds that output text back into subsequent prompts. This can help in cases where you want GPT to reason things out before answering, or brainstorm a plan before executing it.\n\nIn this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n\nExplain: Given a Python function, we ask GPT to explain what the function is doing and why.\nPlan: We ask GPT to plan a set of unit tests for the function.\n    If the plan is too short, we ask GPT to elaborate with more ideas for unit tests.\nExecute: Finally, we instruct GPT to write unit tests that cover the planned cases.\n\nThe code example illustrates a few embellishments on the chained, multi-step prompt:\n\nConditional branching (e.g., asking for elaboration only if the first plan is too short)\nThe choice of different models for different steps\nA check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's ast module)\nStreaming output so that you can start reading the output before it's fully generated (handy for long, multi-step outputs)\nMake sure to check any code before using it, as GPT makes plenty of mistakes (especially on character-based tasks like this one). For best results, use the most powerful model (GPT-4, as of May 2023)."])</script><script>self.__next_f.push([1,"34:Tce1,"])</script><script>self.__next_f.push([1,"\nComplex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT-3 and then feeds that text back into subsequent prompts. This can help in cases where you want GPT-3 to explain its reasoning before answering, or brainstorm a plan before executing it.\n\nIn this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n\nGiven a Python function, we first prompt GPT-3 to explain what the function is doing.\nSecond, we prompt GPT-3 to plan a set of unit tests for the function.\n    If the plan is too short, we ask GPT-3 to elaborate with more ideas for unit tests.\nFinally, we prompt GPT-3 to write the unit tests.\n\nThe code example illustrates a few optional embellishments on the chained, multi-step prompt:\n\nConditional branching (e.g., only asking for elaboration if the first plan is too short)\nDifferent models for different steps (e.g., gpt-3.5-turbo-instruct for the text planning steps and gpt-4 for the code writing step)\nA check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's ast module)\nStreaming output so that you can start reading the output before it's fully generated (useful for long, multi-step outputs)\n\nThe full 3-step prompt looks like this (using as an example pytest for the unit test framework and is_palindrome as the function):\n\nHow to write great unit tests with pytest\n\n    In this advanced tutorial for experts, we'll use Python 3.9 and pytest to write a suite of unit tests to verify the behavior of the following function.\n        def is_palindrome(s):\n        return s == s[::-1]\n\n    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n    First,{GENERATED IN STEP 1}\n\n    A good unit test suite should aim to:\n    Test the function's behavior for a wide range of possible inputs\n    Test edge cases that the author may not have foreseen\n    Take advantage of the features of pytest to make the tests easy to write and maintain\n    Be easy to read and understand, with clean code and descriptive names\n    Be deterministic, so that the tests always pass or fail in the same way\n\n    pytest has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n\n    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n    -{GENERATED IN STEP 2}\n\n    [OPTIONALLY APPENDED]In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n    -{GENERATED IN STEP 2B}\n\n    Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n        import pytest  # used for our unit tests\n\n    def is_palindrome(s):\n        return s == s[::-1]\n\n    #Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n    {GENERATED IN STEP 3}"])</script><script>self.__next_f.push([1,"35:T66d,"])</script><script>self.__next_f.push([1,"\nWe calculate user and product embeddings based on the training set, and evaluate the results on the unseen test set. We will evaluate the results by plotting the user and product similarity versus the review score. The dataset is created in the Get_embeddings_from_dataset Notebook.\n1. Calculate user and product embeddings\n\nWe calculate these embeddings simply by averaging all the reviews about the same product or written by the same user within the training set.\nWe can see that most of the users and products appear within the 50k examples only once.\n2. Evaluate the embeddings\n\nTo evaluate the recommendations, we look at the similarity of the user and product embeddings amongst the reviews in the unseen test set. We calculate the cosine distance between the user and product embeddings, which gives us a similarity score between 0 and 1. We then normalize the scores to be evenly split between 0 and 1, by calculating the percentile of the similarity score amongst all predicted scores.\n2.1 Visualize cosine similarity by review score\n\nWe group the cosine similarity scores by the review score, and plot the distribution of cosine similarity scores for each review score.\nWe can observe a weak trend, showing that the higher the similarity score between the user and the product embedding, the higher the review score. Therefore, the user and product embeddings can weakly predict the review score - even before the user receives the product!\n\nBecause this signal works in a different way than the more commonly used collaborative filtering, it can act as an additional feature to slightly improve the performance on existing problems."])</script><script>self.__next_f.push([1,"36:Teb5,"])</script><script>self.__next_f.push([1,"\nWe are addressing the problem of enhancing the precision of transcriptions, particularly when it comes to company names and product references. Our solution involves a dual strategy that utilizes both the Whisper prompt parameter and GPT-4's post-processing capabilities.\n\nTwo approaches to correct inaccuracies are:\n\nWe input a list of correct spellings directly into Whisper's prompt parameter to guide the initial transcription.\n\nWe utilized GPT-4 to fix misspellings post transcription, again using the same list of correct spellings in the prompt.\n\nThese strategies aimed at ensuring precise transcription of unfamilar proper nouns.\n\nSetup\n\nTo get started, let's:\n\nImport the OpenAI Python library (if you don't have it, you'll need to install it with Download the audio file example\nSetting our baseline with a fictitious audio recording\n\nOur reference point is a monologue, which was generated by ChatGPT from prompts given by the author. The author then voiced this content. So, the author both guided the ChatGPT's output with prompts and brought it to life by speaking it.\nOur fictitious company, ZyntriQix, offers a range of tech products. These include Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, and DigiFractal Matrix. We also spearhead several initiatives such as PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., and F.L.I.N.T.\nWhisper transcribed our company name, product names, and miscapitalized our acronyms incorrectly. Let's pass the correct names as a list in the prompt.\nWhen passing the list of product names, some of the product names are transcribed correctly while others are still misspelled.\nYou can use GPT-4 to fix spelling mistakes\n\nLeveraging GPT-4 proves especially useful when the speech content is unknown beforehand and we have a list of product names readily available.\n\nThe post-processing technique using GPT-4 is notably more scalable than depending solely on Whisper's prompt parameter, which has a token limit of 244. GPT-4 allows us to process larger lists of correct spellings, making it a more robust method for handling extensive product lists.\n\nHowever, this post-processing technique isn't without limitations. It's constrained by the context window of the chosen model, which may pose challenges when dealing with vast numbers of unique terms. For instance, companies with thousands of SKUs may find that the context window of GPT-4 is insufficient to handle their requirements, and they might need to explore alternative solutions.\n\nInterestingly, the GPT-4 post-processing technique seems more reliable than using Whisper alone. This method, which leverages a product list, enhances the reliability of our results. However, this increased reliability comes at a price, as using this approach can increase costs and can result in higher latency.\nNow, let's input the original product list into GPT-4 and evaluate its performance. By doing so, we aim to assess the AI model's ability to correctly spell the proprietary product names, even with no prior knowledge of the exact terms to appear in the transcription. In our experiment, GPT-4 was successful in correctly spelling our product names, confirming its potential as a reliable tool for ensuring transcription accuracy.\nIn this case, we supplied a comprehensive product list that included all the previously used spellings, along with additional new names. This scenario simulates a real-life situation where we have a substantial SKU list and uncertain about the exact terms to appear in the transcription. Feeding this extensive list of product names into the system resulted in a correctly transcribed output.\nWe are employing GPT-4 as a spell checker, using the same list of correct spellings that was previously used in the prompt."])</script><script>self.__next_f.push([1,"37:T8e0,"])</script><script>self.__next_f.push([1,"\nThis notebook offers a guide to improve the Whisper's transcriptions. We'll streamline your audio data via trimming and segmentation, enhancing Whisper's transcription quality. After transcriptions, we'll refine the output by adding punctuation, adjusting product terminology (e.g., 'five two nine' to '529'), and mitigating Unicode issues. These strategies will help improve the clarity of your transcriptions, but remember, customization based on your unique use-case may be beneficial.\n\nSetup\n\nTo get started let's import a few different libraries:\n\nPyDub is a simple and easy-to-use Python library for audio processing tasks such as slicing, concatenating, and exporting audio files.\n\nThe Audio class from the IPython.display module allows you to create an audio control that can play sound in Jupyter notebooks, providing a straightforward way to play audio data directly in your notebook.\n\nFor our audio file, we'll use a fictional earnings call written by ChatGPT and read aloud by the author.This audio file is relatively short, but hopefully provides you with an illustrative idea of how these pre and post processing steps can be applied to any audio file.\nAt times, files with long silences at the beginning can cause Whisper to transcribe the audio incorrectly. We'll use Pydub to detect and trim the silence.\n\nHere, we've set the decibel threshold of 20. You can change this if you would like.\nAt times, we've seen unicode character injection in transcripts, removing any non-ASCII characters should help mitigate this issue.\n\nKeep in mind you should not use this function if you are transcribing in Greek, Cyrillic, Arabic, Chinese, etc\nThis function will add formatting and punctuation to our transcript. Whisper generates a transcript with punctuation but without formatting.\nOur audio file is a recording from a fake earnings call that includes a lot of financial products. This function can help ensure that if Whisper transcribes these financial product names incorrectly, that they can be corrected.\nThis function will create a new file with 'trimmed' appended to the original file name\nOur fake earnings report audio file is fairly short in length, so we'll adjust the segments accordingly. Keep in mind you can adjust the segment length as you need."])</script><script>self.__next_f.push([1,"38:Te53,"])</script><script>self.__next_f.push([1,"\nOpenAI's audio transcription API has an optional parameter called prompt.\n\nThe prompt is intended to help stitch together multiple audio segments. By submitting the prior segment's transcript via the prompt, the Whisper model can use that context to better understand the speech and maintain a consistent writing style.\n\nHowever, prompts do not need to be genuine transcripts from prior audio segments. Fictitious prompts can be submitted to steer the model to use particular spellings or styles.\n\nThis notebook shares two techniques for using fictitious prompts to steer the model outputs:\n\nTranscript generation**: GPT can convert instructions into fictitious transcripts for Whisper to emulate.\nSpelling guide**: A spelling guide can tell the model how to spell names of people, products, companies, etc.\n\nThese techniques are not especially reliable, but can be useful in some situations.\n\nComparison with GPT prompting\n\nPrompting Whisper is not the same as prompting GPT. For example, if you submit an attempted instruction like \"Format lists in Markdown format\", the model will not comply, as it follows the style of the prompt, rather than any instructions contained within.\n\nIn addition, the prompt is limited to only 224 tokens. If the prompt is longer than 224 tokens, only the final 224 tokens of the prompt will be considered; all prior tokens will be silently ignored. The tokenizer used is the multilingual Whisper tokenizer.\n\nTo get good results, craft examples that portray your desired style.\nSetup\n\nTo get started, let's:\nImport the OpenAI Python library (if you don't have it, you'll need to install it with pip install openai)\nDownload a few example audio files\nAs a baseline, we'll transcribe an NPR podcast segment\n\nOur audio file for this example will be a segment of the NPR podcast, Up First.\n\nLet's get our baseline transcription, then introduce prompts.\nTranscripts follow the style of the prompt\nIn the unprompted transcript, 'President Biden' is capitalized. However, if we pass in a fictitious prompt of 'president biden' in lowercase, Whisper matches the style and generates a transcript in all lowercase.\nBe aware that when prompts are short, Whisper may be less reliable at following their style.\nLong prompts may be more reliable at steering Whisper.\nWhisper is also less likely to follow rare or odd styles.\nPass names in the prompt to prevent misspellings\n\nWhisper may incorrectly transcribe uncommon proper nouns such as names of products, companies, or people.\n\nWe'll illustrate with an example audio file full of product names.\nTo get Whisper to use our preferred spellings, let's pass the product and company names in the prompt, as a glossary for Whisper to follow.\nNow, let's switch to another audio recording authored specifically for this demonstration, on the topic of a odd barbecue.\n\nTo begin, we'll establish our baseline transcript using Whisper.\nWhile Whisper's transcription was accurate, it had to guess at various spellings. For example, it assumed the friends' names were spelled Amy and Sean rather than Aimee and Shawn. Let's see if we can steer the spelling with a prompt.\nSuccess!\n\nLet's try the same with more ambiguously spelled words.\nFictitious prompts can be generated by GPT\n\nOne potential tool to generate fictitious prompts is GPT. We can give GPT instructions and use it to generate long fictitious transcripts with which to prompt Whisper.\nWhisper prompts are best for specifying otherwise ambiguous styles. The prompt will not override the model's comprehension of the audio. For example, if the speakers are not speaking in a deep Southern accent, a prompt will not cause the transcript to do so."])</script><script>self.__next_f.push([1,"39:T68c,"])</script><script>self.__next_f.push([1,"\nIn this notebook we will classify the sentiment of reviews using embeddings and zero labeled data! The dataset is created in the Get_embeddings_from_dataset Notebook.\n\nWe'll define positive sentiment to be 4- and 5-star reviews, and negative sentiment to be 1- and 2-star reviews. 3-star reviews are considered neutral and we won't use them for this example.\n\nWe will perform zero-shot classification by embedding descriptions of each class and then comparing new samples to those class embeddings.\nZero-Shot Classification\nTo perform zero shot classification, we want to predict labels for our samples without any training. To do this, we can simply embed short descriptions of each label, such as positive and negative, and then compare the cosine distance between embeddings of samples and label descriptions.\n\nThe highest similarity label to the sample input is the predicted label. We can also define a prediction score to be the difference between the cosine distance to the positive and to the negative label. This score can be used for plotting a precision-recall curve, which can be used to select a different tradeoff between precision and recall, by selecting a different threshold.\nWe can see that this classifier already performs extremely well. We used similarity embeddings, and the simplest possible label name. Let's try to improve on this by using more descriptive label names, and search embeddings.\nUsing the search embeddings and descriptive names leads to an additional improvement in performance.\nAs shown above, zero-shot classification with embeddings can lead to great results, especially when the labels are more descriptive than just simple words."])</script><script>self.__next_f.push([1,"3a:Taf3,"])</script><script>self.__next_f.push([1,"\nThis example will cover chat completions using the Azure OpenAI service. It also includes information on content filtering.\nSetup\n\nFirst, we install the necessary dependencies and import the libraries we will be using.\nAuthentication\n\nThe Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials.\nAuthentication using API key\n\nTo set up the OpenAI SDK to use an Azure API Key, we need to set api_key to a key associated with your endpoint (you can find this key in \"Keys and Endpoints\" under \"Resource Management\" in the Azure Portal). You'll also find the endpoint for your resource here.\nAuthentication using Azure Active Directory\nLet's now see how we can autheticate via Azure Active Directory. We'll start by installing the azure-identity library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the get_bearer_token_provider helper function. It's recommended to use get_bearer_token_provider over providing a static token to AzureOpenAI because this API will automatically cache and refresh tokens for you.\n\nFor more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the documentation.\nNote: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n\napi_key from AZURE_OPENAI_API_KEY\nazure_ad_token from AZURE_OPENAI_AD_TOKEN\napi_version from OPENAI_API_VERSION\nazure_endpoint from AZURE_OPENAI_ENDPOINT\n\nDeployments\n\nIn this section we are going to create a deployment of a GPT model that we can use to create chat completions.\nDeployments: Create in the Azure OpenAI Studio\nLet's deploy a model to use with chat completions. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for chat completions. The deployment name that you give the model will be used in the code below.\nCreate chat completions\n\nNow let's create a chat completion using the client we built.\nCreate a streaming chat completion\n\nWe can also stream the response.\nContent filtering\n\nAzure OpenAI service includes content filtering of prompts and completion responses. You can learn more about content filtering and how to configure it here.\n\nIf the prompt is flagged by the content filter, the library will raise a BadRequestError exception with a content_filter error code. Otherwise, you can access the prompt_filter_results and content_filter_results on the response to see the results of the content filtering and what categories were flagged.\nPrompt flagged by content filter\nChecking the result of the content filter"])</script><script>self.__next_f.push([1,"3b:T15a7,"])</script><script>self.__next_f.push([1,"This example shows how to use Azure OpenAI service models with your own data. The feature is currently in preview.\n\nAzure OpenAI on your data enables you to run supported chat models such as GPT-3.5-Turbo and GPT-4 on your data without needing to train or fine-tune models. Running models on your data enables you to chat on top of, and analyze your data with greater accuracy and speed. One of the key benefits of Azure OpenAI on your data is its ability to tailor the content of conversational AI. Because the model has access to, and can reference specific sources to support its responses, answers are not only based on its pretrained knowledge but also on the latest information available in the designated data source. This grounding data also helps the model avoid generating responses based on outdated or incorrect information.\n\nAzure OpenAI on your own data with Azure AI Search (f.k.a. Azure Cognitive Search) provides a customizable, pre-built solution for knowledge retrieval, from which a conversational AI application can be built. To see alternative methods for knowledge retrieval and semantic search, check out the cookbook examples for vector databases.\nHow it works\n\nAzure OpenAI on your own data connects the model with your data, giving it the ability to retrieve and utilize data in a way that enhances the model's output. Together with Azure AI Search, data is retrieved from designated data sources based on the user input and provided conversation history. The data is then augmented and resubmitted as a prompt to the model, giving the model contextual information it can use to generate a response.\n\nSee the Data, privacy, and security for Azure OpenAI Service for more information.\nPrerequisites\nTo get started, we'll cover a few prequisites.\n\nTo properly access the Azure OpenAI Service, we need to create the proper resources at the Azure Portal (you can check a detailed guide on how to do this in the Microsoft Docs)\n\nTo use your own data with Azure OpenAI models, you will need:\n\nAzure OpenAI access and a resource with a chat model deployed (for example, GPT-3 or GPT-4)\nAzure AI Search (f.k.a. Azure Cognitive Search) resource\nAzure Blob Storage resource\nYour documents to be used as data (See data source options)\n\n\nFor a full walk-through on how to upload your documents to blob storage and create an index using the Azure AI Studio, see this Quickstart.\nSetup\n\nFirst, we install the necessary dependencies.\nIn this example, we'll use dotenv to load our environment variables. To connect with Azure OpenAI and the Search index, the following variables should be added to a .env file in KEY=VALUE format:\n\nAZURE_OPENAI_ENDPOINT - the Azure OpenAI endpoint. This can be found under \"Keys and Endpoints\" for your Azure OpenAI resource in the Azure Portal.\nAZURE_OPENAI_API_KEY - the Azure OpenAI API key. This can be found under \"Keys and Endpoints\" for your Azure OpenAI resource in the Azure Portal. Omit if using Azure Active Directory authentication (see below Authentication using Microsoft Active Directory)\nSEARCH_ENDPOINT - the AI Search endpoint. This URL be found on the \"Overview\" of your Search resource on the Azure Portal.\nSEARCH_KEY - the AI Search API key. Found under \"Keys\" for your Search resource in the Azure Portal.\nSEARCH_INDEX_NAME - the name of the index you created with your own data.\nAuthentication\n\nThe Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials.\nAuthentication using API key\n\nTo set up the OpenAI SDK to use an Azure API Key, we need to set api_key to a key associated with your endpoint (you can find this key in \"Keys and Endpoints\" under \"Resource Management\" in the Azure Portal). You'll also find the endpoint for your resource here.\nAuthentication using Azure Active Directory\nLet's now see how we can autheticate via Azure Active Directory. We'll start by installing the azure-identity library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the get_bearer_token_provider helper function. It's recommended to use get_bearer_token_provider over providing a static token to AzureOpenAI because this API will automatically cache and refresh tokens for you.\n\nFor more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the documentation.\nNote: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n\napi_key from AZURE_OPENAI_API_KEY\nazure_ad_token from AZURE_OPENAI_AD_TOKEN\napi_version from OPENAI_API_VERSION\nazure_endpoint from AZURE_OPENAI_ENDPOINT\n\nChat completion model with your own data\nSetting the context\nIn this example, we want our model to base its responses on Azure AI services documentation data. Following the Quickstart shared previously, we have added the markdown file for the Azure AI services and machine learning documentation page to our search index. The model is now ready to answer questions about Azure AI services and machine learning.\nCode\nNow we can use Azure on your own data with Chat Completions. Providing our search endpoint, key, and index name in dataSources, any questions posed to the model will now be grounded in our own data. An additional property, context, will be provided in the response to show the data the model referenced to answer the question.\nIf you would prefer to stream the response from the model, you can pass the stream=True keyword argument:"])</script><script>self.__next_f.push([1,"3c:T81c,"])</script><script>self.__next_f.push([1,"\nThis example will cover embeddings using the Azure OpenAI service.\nSetup\n\nFirst, we install the necessary dependencies and import the libraries we will be using.\nAuthentication\n\nThe Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials.\nAuthentication using API key\n\nTo set up the OpenAI SDK to use an Azure API Key, we need to set api_key to a key associated with your endpoint (you can find this key in \"Keys and Endpoints\" under \"Resource Management\" in the Azure Portal). You'll also find the endpoint for your resource here.\nAuthentication using Azure Active Directory\nLet's now see how we can autheticate via Azure Active Directory. We'll start by installing the azure-identity library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the get_bearer_token_provider helper function. It's recommended to use get_bearer_token_provider over providing a static token to AzureOpenAI because this API will automatically cache and refresh tokens for you.\n\nFor more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the documentation.\nNote: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n\napi_key from AZURE_OPENAI_API_KEY\nazure_ad_token from AZURE_OPENAI_AD_TOKEN\napi_version from OPENAI_API_VERSION\nazure_endpoint from AZURE_OPENAI_ENDPOINT\n\nDeployments\n\nIn this section we are going to create a deployment of a model that we can use to create embeddings.\nDeployments: Create in the Azure OpenAI Studio\nLet's deploy a model to use with embeddings. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for embeddings. The deployment name that you give the model will be used in the code below.\nEmbeddings\n\nNow let's create embeddings using the client we built."])</script><script>self.__next_f.push([1,"3d:Tfd4,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how to use the function calling capability with the Azure OpenAI service. Functions allow a caller of chat completions to define capabilities that the model can use to extend its\nfunctionality into external tools and data sources.\n\nYou can read more about chat functions on OpenAI's blog: https://openai.com/blog/function-calling-and-other-api-updates\n\nNOTE: Chat functions require model versions beginning with gpt-4 and gpt-35-turbo's -0613 labels. They are not supported by older versions of the models.\nSetup\n\nFirst, we install the necessary dependencies and import the libraries we will be using.\nAuthentication\n\nThe Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials.\nAuthentication using API key\n\nTo set up the OpenAI SDK to use an Azure API Key, we need to set api_key to a key associated with your endpoint (you can find this key in \"Keys and Endpoints\" under \"Resource Management\" in the Azure Portal). You'll also find the endpoint for your resource here.\nAuthentication using Azure Active Directory\nLet's now see how we can autheticate via Azure Active Directory. We'll start by installing the azure-identity library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the get_bearer_token_provider helper function. It's recommended to use get_bearer_token_provider over providing a static token to AzureOpenAI because this API will automatically cache and refresh tokens for you.\n\nFor more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the documentation.\nNote: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n\napi_key from AZURE_OPENAI_API_KEY\nazure_ad_token from AZURE_OPENAI_AD_TOKEN\napi_version from OPENAI_API_VERSION\nazure_endpoint from AZURE_OPENAI_ENDPOINT\n\nDeployments\n\nIn this section we are going to create a deployment of a GPT model that we can use to call functions.\nDeployments: Create in the Azure OpenAI Studio\nLet's deploy a model to use with chat completions. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for chat completions. The deployment name that you give the model will be used in the code below.\nFunctions\n\nWith setup and authentication complete, you can now use functions with the Azure OpenAI service. This will be split into a few steps:\n\nDefine the function(s)\nPass function definition(s) into chat completions API\nCall function with arguments from the response\nFeed function response back into chat completions API\n1. Define the function(s)\n\nA list of functions can be defined, each containing the name of the function, an optional description, and the parameters the function accepts (described as a JSON schema).\n2. Pass function definition(s) into chat completions API\n\nNow we can pass the function into the chat completions API. If the model determines it should call the function, a finish_reason of \"tool_calls\" will be populated on the choice and the details of which function to call and its arguments will be present in the message. Optionally, you can set the tool_choice keyword argument to force the model to call a particular function (e.g. {\"type\": \"function\", \"function\": {\"name\": get_current_weather}}). By default, this is set to auto, allowing the model to choose whether to call the function or not.\n3. Call function with arguments from the response\n\nThe name of the function call will be one that was provided initially and the arguments will include JSON matching the schema included in the function definition.\n4. Feed function response back into chat completions API\n\nThe response from the function should be serialized into a new message with the role set to \"function\". Now the model will use the response data to formulate its answer."])</script><script>self.__next_f.push([1,"3e:T6cc,"])</script><script>self.__next_f.push([1,"\nWith permission of the author, we will demonstrate how to translate the book Euclidean Plane Geometry, written by Milan Mitrović from Slovenian into English, without modifying any of the LaTeX commands.\n\nTo achieve this, we will first split the book into chunks, each roughly a page long, then translate each chunk into English, and finally stitch them back together.\n1. Read in the data\n1.1 Count the tokens in each chunk\nIt turns out that a double newline is a good separator in this case, in order not to break the flow of the text. Also no individual chunk is larger than 1500 tokens. The model we will use is text-davinci-002, which has a limit of 4096 tokens, so we don't need to worry about breaking the chunks down further.\n\nWe will group the shorter chunks into chunks of around 1000 tokens, to increase the coherence of the text, and decrease the frequency of breaks within the text.\nNotice that adding a sample untranslated and translated first command, where only the content of the chapter name needs to be translated, helps to get more consistent results.\n\nThe format of the prompt sent to the model consists of:\nA high level instruction to translate only the text, but not commands into the desired language\nA sample untranslated command, where only the content of the chapter name needs to be translated\nThe chunk of text to be translated\nThe translated sample command from 2, which shows the model the beginning of the translation process\n\nThe expected output is the translated chunk of text.\nWe can see here that this one chunk in particular translates only the text, but leaves LaTeX commands intact.\n\nLet's now translate all the chunks in the book - this will take 2-3 hours, as we're processing requests sequentially."])</script><script>self.__next_f.push([1,"3f:T735,"])</script><script>self.__next_f.push([1,"\nSegment Anything is a model from Meta that can be used to select portions of images. Combined with DALL·E's ability to inpaint specified portions of images, you can use Segment Anything to easily select any part of an image you'd like to alter.\n\nIn this notebook, we'll use these tools to become fashion designers and dynamically replace our digital models' outfits with tailored, original creations. The notebook follows this flow:\nSetup:** Initialise your libraries and any location directories.\nGenerate original image:** Make an original image that we'll create dynamic masks from.\nGenerate mask:** Use Segment Anything to create a dynamic mask.\nCreate new image:** Generate a new image with the masked area inpainted with a fresh prompt.\nSetup\n\nTo get started we'll need to follow the instructions for using the Segment Anything (SAM) model open-sourced by Meta. As of May 2023, the key steps are:\nInstall Pytorch (version 1.7+).\nInstall the library using Install dependencies using Download a model checkpoint to use (default size is 2.4 GB).\n\nGenerate original image\n\nFirst we'll create an original image which we'll generate masks from.\nGenerate Mask\n\nNext we'll load up one of our images and generate masks.\n\nFor this demonstration we're picking a UX where we \"click\" on a point on the image to generate masks from. However, there are example notebooks provided by Meta which show how to generate every possible mask for an image, draw a box, and some other useful approaches.\nCreate new image\n\nNow we'll combine our original image with the mask and the Edit endpoint for DALLE to inpaint the transparent area according to a new prompt. (as 0f January 2024 dall-e-2 is the only model that supports edits)\n\nBeautiful!\n\nNow you too can easily create dynamic masks to extend your images - enjoy the APIs, and please share what you build!"])</script><script>self.__next_f.push([1,"40:T1142,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how to use OpenAI's DALL·E image API endpoints.\n\nThere are three API endpoints:\nGenerations:** generates an image or images based on an input caption\nEdits:** edits or extends an existing image\nVariations:** generates variations of an input image\nSetup\n\nImport the packages you'll need\nImport your OpenAI API key: You can do this by running \\`export OPENAI_API_KEY=\"your API key\"\\` in your terminal.\nSet a directory to save images to\nGenerations\n\nThe generation API endpoint creates an image based on a text prompt. API Reference\n\nRequired inputs:\nprompt (str): A text description of the desired image(s). The maximum length is 1000 characters for dall-e-2 and 4000 characters for dall-e-3.\n\nOptional inputs:\nmodel (str): The model to use for image generation. Defaults to dall-e-2\nn (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.\nquality (str): The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. This param is only supported for dall-e-3.\nresponse_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".\nsize (str): The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models. Defaults to \"1024x1024\".\nstyle(str | null): The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for dall-e-3.\nuser (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.\nVariations\n\nThe variations endpoint generates new images (variations) similar to an input image. API Reference\n\nHere we'll generate variations of the image generated above.\n\nRequired inputs:\nimage (str): The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.\n\nOptional inputs:\nmodel (str): The model to use for image variations. Only dall-e-2 is supported at this time.\nn (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.\nsize (str): The size of the generated images. Must be one of \"256x256\", \"512x512\", or \"1024x1024\". Smaller images are faster. Defaults to \"1024x1024\".\nresponse_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".\nuser (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.\n\nEdits\n\nThe edit endpoint uses DALL·E to generate a specified portion of an existing image. Three inputs are needed: the image to edit, a mask specifying the portion to be regenerated, and a prompt describing the desired image. API Reference\n\nRequired inputs:\nimage (str): The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.\nprompt (str): A text description of the desired image(s). The maximum length is 1000 characters.\n\nOptional inputs:\nmask (file): An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where image should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as image.\nmodel (str): The model to use for edit image. Only dall-e-2 is supported at this time.\nn (int): The number of images to generate. Must be between 1 and 10. Defaults to 1.\nsize (str): The size of the generated images. Must be one of \"256x256\", \"512x512\", or \"1024x1024\". Smaller images are faster. Defaults to \"1024x1024\".\nresponse_format (str): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".\nuser (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.\n\nSet Edit Area\n\nAn edit requires a \"mask\" to specify which portion of the image to regenerate. Any pixel with an alpha of 0 (transparent) will be regenerated. The code below creates a 1024x1024 mask where the bottom half is transparent.\nPerform Edit\n\nNow we supply our image, caption and mask to the API to get 5 examples of edits to our image"])</script><script>self.__next_f.push([1,"41:T3044,"])</script><script>self.__next_f.push([1,"\nIn this notebook we delve into the evaluation techniques for abstractive summarization tasks using a simple example. We explore traditional evaluation methods like ROUGE and BERTScore, in addition to showcasing a more novel approach using LLMs as evaluators.\n\nEvaluating the quality of summaries is a time-consuming process, as it involves different quality metrics such as coherence, conciseness, readability and content. Traditional automatic evaluation metrics such as ROUGE and BERTScore and others are concrete and reliable, but they may not correlate well with the actual quality of summaries. They show relatively low correlation with human judgments, especially for open-ended generation tasks (Liu et al., 2023). There's a growing need to lean on human evaluations, user feedback, or model-based metrics while being vigilant about potential biases. While human judgment provides invaluable insights, it is often not scalable and can be cost-prohibitive.\n\nIn addition to these traditional metrics, we showcase a method (G-Eval) that leverages Large Language Models (LLMs) as a novel, reference-free metric for assessing abstractive summaries. In this case, we use gpt-4 to score candidate outputs. gpt-4 has effectively learned an internal model of language quality that allows it to differentiate between fluent, coherent text and low-quality text. Harnessing this internal scoring mechanism allows auto-evaluation of new candidate outputs generated by an LLM.\n\nSetup\n\nExample task\n\nFor the purposes of this notebook we'll use the example summarization below. Notice that we provide two generated summaries to compare, and a reference human-written summary, which evaluation metrics like ROUGE and BERTScore require.\n\nExcerpt (excerpt):\nOpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. OpenAI will build safe and beneficial AGI directly, but will also consider its mission fulfilled if its work aids others to achieve this outcome. OpenAI follows several key principles for this purpose. First, broadly distributed benefits - any influence over AGI's deployment will be used for the benefit of all, and to avoid harmful uses or undue concentration of power. Second, long-term safety - OpenAI is committed to doing the research to make AGI safe, and to promote the adoption of such research across the AI community. Third, technical leadership - OpenAI aims to be at the forefront of AI capabilities. Fourth, a cooperative orientation - OpenAI actively cooperates with other research and policy institutions, and seeks to create a global community working together to address AGI's global challenges.\n\nSummaries:\n\n| Reference Summary /ref_summary (human generated)                                                                                                                                                                                                                                                                                                                         | Eval Summary 1 / eval_summary_1 (system generated)                                                                                                                                                                                                                                                                                                                               | Eval Summary 2 / eval_summary_2 (system generated)                                                                                                                                                                                                                                                   |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| OpenAI aims to ensure artificial general intelligence (AGI) is used for everyone's benefit, avoiding harmful uses or undue power concentration. It is committed to researching AGI safety, promoting such studies among the AI community. OpenAI seeks to lead in AI capabilities and cooperates with global research and policy institutions to address AGI's challenges. | OpenAI aims to AGI benefits all humanity, avoiding harmful uses and power concentration. It pioneers research into safe and beneficial AGI and promotes adoption globally. OpenAI maintains technical leadership in AI while cooperating with global institutions to address AGI challenges. It seeks to lead a collaborative worldwide effort developing AGI for collective good. | OpenAI aims to ensure AGI is for everyone's use, totally avoiding harmful stuff or big power concentration. Committed to researching AGI's safe side, promoting these studies in AI folks. OpenAI wants to be top in AI things and works with worldwide research, policy groups to figure AGI's stuff. |\n\nTake a moment to figure out which summary you'd personally prefer and the one that captures OpenAI's mission really well.\n\nEvaluating using ROUGE\n\nROUGE, which stands for Recall-Oriented Understudy for Gisting Evaluation, primarily gauges the overlap of words between a generated output and a reference text. It's a prevalent metric for evaluating automatic summarization tasks. Among its variants, ROUGE-L offers insights into the longest contiguous match between system-generated and reference summaries, gauging how well the system retains the original summary's essence.\n\nThe table shows the ROUGE scores for evaluating two different summaries against a reference text. In the case of rouge-1, Summary 2 outperforms Summary 1, indicating a better overlap of individual words and for rouge-l, Summary 2 has a higher score, implying a closer match in the longest common subsequences, and thus a potentially better overall summarization in capturing the main content and order of the original text. Since Summary 2 has many words and short phrases directly lifted from the excerpt, its overlap with the reference summary would likely be higher, leading to higher ROUGE scores.\n\nWhile ROUGE and similar metrics, such as BLEU and METEOR, offer quantitative measures, they often fail to capture the true essence of a well-generated summary. They also correlate worse with human scores. Given the advancements in LLMs, which are adept at producing fluent and coherent summaries, traditional metrics like ROUGE may inadvertently penalize these models. This is especially true if the summaries are articulated differently but still encapsulate the core information accurately.\n\nEvaluating using BERTScore\n\nROUGE relies on the exact presence of words in both the predicted and reference texts, failing to interpret the underlying semantics. This is where BERTScore comes in and leverages the contextual embeddings from the BERT model, aiming to evaluate the similarity between a predicted and a reference sentence in the context of machine-generated text. By comparing embeddings from both sentences, BERTScore captures semantic similarities that might be missed by traditional n-gram based metrics.\n\nThe close F1 Scores between the summaries indicate that they may perform similarly in capturing the key information. However, this small difference should be interpreted with caution. Since BERTScore may not fully grasp subtleties and high-level concepts that a human evaluator might understand, reliance solely on this metric could lead to misinterpreting the actual quality and nuances of the summary. An integrated approach combining BERTScore with human judgment and other metrics could offer a more reliable evaluation.\n\nEvaluating using GPT-4\n\nHere we implement an example reference-free text evaluator using gpt-4, inspired by the G-Eval) framework which evaluates the quality of generated text using large language models. Unlike metrics like ROUGE or BERTScore that rely on comparison to reference summaries, the gpt-4 based evaluator assesses the quality of generated content based solely on the input prompt and text, without any ground truth references. This makes it applicable to new datasets and tasks where human references are sparse or unavailable.\n\nHere's an overview of this method:\n\nWe define four distinct criteria:\n    Relevance: Evaluates if the summary includes only important information and excludes redundancies.\n    Coherence: Assesses the logical flow and organization of the summary.\n    Consistency: Checks if the summary aligns with the facts in the source document.\n    Fluency: Rates the grammar and readability of the summary.\nWe craft prompts for each of these criteria, taking the original document and the summary as inputs, and leveraging chain-of-thought generation and guiding the model to output a numeric score from 1-5 for each criteria.\nWe generate scores from gpt-4 with the defined prompts, comparing them across summaries.\n\nIn this demonstration, we're using a direct scoring function where gpt-4 generates a discrete score (1-5) for each metric. Normalizing the scores and taking a weighted sum could result in more robust, continuous scores that better reflect the quality and diversity of the summaries.\n\nOverall, the Summary 1 appears to outperform Summary 2 in three of the four categories (Coherence, Relevance and Fluency). Both summaries are found to be consistent with each other. The result might suggest that Summary 1 is generally preferable based on the given evaluation criteria.\n\nLimitations\n\nNote that LLM-based metrics could have a bias towards preferring LLM-generated texts over human-written texts. Additionally LLM based metrics are sensitive to system messages/prompts. We recommend experimenting with other techniques that can help improve performance and/or get consistent scores, striking the right balance between high-quality expensive evaluation and automated evaluations. It is also worth noting that this scoring methodology is currently limited by gpt-4's context window.\n\nConclusion\n\nEvaluating abstractive summarization remains an open area for further improvement. Traditional metrics like ROUGE, BLEU, and BERTScore provide useful automatic evaluation but have limitations in capturing semantic similarity and nuanced aspects of summarization quality. Moreover, they require reference outputs which can be expensive to collect/label. LLM-based metrics offer promise as a reference-free method of evaluating coherence, fluency, and relevance. However, they too have potential biases favoring text generated by LLMs. Ultimately, a combination of automatic metrics and human evaluation is ideal for reliably assessing abstractive summarization systems. While human evaluation is indispensable for gaining a comprehensive understanding of summary quality, it should be complemented with automated evaluation to enable efficient, large-scale testing. The field will continue to evolve more robust evaluation techniques, balancing quality, scalability, and fairness. Advancing evaluation methods is crucial for driving progress in production applications.\n\nReferences\n\nG-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment - Liu Y, Iter D, Xu Y, Wang S, Xu R, Zhu C. Published May, 2023.\nBERTScore: Evaluating Text Generation with BERT - Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. Published online February, 2020.\nROUGE: A Package for Automatic Evaluation of Summaries - Lin CY. Published July, 2004.\nSummEval: Re-evaluating Summarization Evaluation - Fabbri et al. Published April, 2021."])</script><script>self.__next_f.push([1,"42:T2e89,"])</script><script>self.__next_f.push([1,"The OpenAI Evals framework consists of\nA framework to evaluate an LLM (large language model) or a system built on top of an LLM.\nAn open-source registry of challenging evals\n\nThis notebook will cover:\nIntroduction to Evaluation and the OpenAI Evals library\nBuilding an Eval\nRunning an Eval\n\nWhat are evaluations/ evals?\n\nEvaluation is the process of validating and testing the outputs that your LLM applications are producing. Having strong evaluations (\"evals\") will mean a more stable, reliable application that is resilient to code and model changes. An eval is a task used to measure the quality of the output of an LLM or LLM system. Given an input prompt, an output is generated. We evaluate this output with a set of ideal answers and find the quality of the LLM system.\n\nImportance of Evaluations\n\nIf you are building with foundational models like GPT-4, creating high quality evals is one of the most impactful things you can do. Developing AI solutions involves an iterative design process. Without evals, it can be very difficult and time intensive to understand how different model versions and prompts might affect your use case.\n\nWith OpenAI’s continuous model upgrades, evals allow you to efficiently test model performance for your use cases in a standardized way. Developing a suite of evals customized to your objectives will help you quickly and effectively understand how new models may perform for your use cases. You can also make evals a part of your CI/CD pipeline to make sure you achieve the desired accuracy before deploying.\n\nTypes of evals\n\nThere are two main ways we can evaluate/grade completions: writing some validation logic in code\nor using the model itself to inspect the answer. We’ll introduce each with some examples.\n\nWriting logic for answer checking\n\nThe simplest and most common type of eval has an input and an ideal response or answer. For example,\nwe can have an eval sample where the input is \"What year was Obama elected president for the first\ntime?\" and the ideal answer is \"2008\". We feed the input to a model and get the completion. If the model\nsays \"2008\", it is then graded as correct. We can write a string match to check if the completion includes the phrase \"2008\". If it does, we consider it correct.\n\nConsider another eval where the input is to generate valid JSON: We can write some code that\nattempts to parse the completion as JSON and then considers the completion correct if it is\nparsable.\n\n**Model grading: A two stage process where the model first answers the question, then we ask a\nmodel to look at the response to check if it’s correct.**\n\nConsider an input that asks the model to write a funny joke. The model then generates a\ncompletion. We then create a new input to the model to answer the question: \"Is this following\njoke funny? First reason step by step, then answer yes or no\" that includes the completion.\" We\nfinally consider the original completion correct if the new model completion ends with \"yes\".\n\nModel grading works best with the latest, most powerful models like GPT-4 and if we give them the ability\nto reason before making a judgment. Model grading will have an error rate, so it is important to validate\nthe performance with human evaluation before running the evals at scale. For best results, it makes\nsense to use a different model to do grading from the one that did the completion, like using GPT-4 to\ngrade GPT-3.5 answers.\n\nOpenAI Eval Templates\n\nIn using evals, we have discovered several \"templates\" that accommodate many different benchmarks. We have implemented these templates in the OpenAI Evals library to simplify the development of new evals. For example, we have defined 2 types of eval templates that can be used out of the box:\n\nBasic Eval Templates**: These contain deterministic functions to compare the output to the ideal_answers. In cases where the desired model response has very little variation, such as answering multiple choice questions or simple questions with a straightforward answer, we have found this following templates to be useful.\n\nModel-Graded Templates**: These contain functions where an LLM compares the output to the ideal_answers and attempts to judge the accuracy. In cases where the desired model response can contain significant variation, such as answering an open-ended question, we have found that using the model to grade itself is a viable strategy for automated evaluation.\n\nGetting Setup\n\nFirst, go to github.com/openai/evals, clone the repository with git clone git@github.com:openai/evals.git and go through the setup instructions.\n\nTo run evals later in this notebook, you will need to set up and specify your OpenAI API key. After you obtain an API key, specify it using the OPENAI_API_KEY environment variable.\n\nPlease be aware of the costs associated with using the API when running evals.\nBuilding an evaluation for OpenAI Evals framework\n\nAt its core, an eval is a dataset and an eval class that is defined in a YAML file. To start creating an eval, we need\n\nThe test dataset in the jsonl format.\nThe eval template to be used\n\nCreating the eval dataset\nLets create a dataset for a use case where we are evaluating the model's ability to generate syntactically correct SQL. In this use case, we have a series of tables that are related to car manufacturing\n\nFirst we will need to create a system prompt that we would like to evaluate. We will pass in instructions for the model as well as an overview of the table structure:\n\"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [,Id,Maker,FullName,Country]\\nTable car_names, columns = [,MakeId,Model,Make]\\nTable cars_data, columns = [,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [,ContId,Continent]\\nTable countries, columns = [,CountryId,CountryName,Continent]\\nTable model_list, columns = [,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\"\n\nFor this prompt, we can ask a specific question:\n\"Q: how many car makers are their in germany?\"\n\nAnd we have an expected answer:\n\"A: SELECT count ( * )  FROM CAR_MAKERS AS T1 JOIN COUNTRIES AS T2 ON T1.Country   =   T2.CountryId WHERE T2.CountryName   =   'germany'\"\n\nThe dataset needs to be in the following format:\n\"input\": [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": }, \"ideal\": \"correct answer\"]\n\nPutting it all together, we get:\n{\"input\": [{\"role\": \"system\", \"content\": \"TASK: Answer the following question with syntactically correct SQLite SQL. The SQL should be correct and be in context of the previous question-answer pairs.\\nTable car_makers, columns = [,Id,Maker,FullName,Country]\\nTable car_names, columns = [,MakeId,Model,Make]\\nTable cars_data, columns = [,Id,MPG,Cylinders,Edispl,Horsepower,Weight,Accelerate,Year]\\nTable continents, columns = [,ContId,Continent]\\nTable countries, columns = [,CountryId,CountryName,Continent]\\nTable model_list, columns = [,ModelId,Maker,Model]\\nForeign_keys = [countries.Continent = continents.ContId,car_makers.Country = countries.CountryId,model_list.Maker = car_makers.Id,car_names.Model = model_list.Model,cars_data.Id = car_names.MakeId]\\n\"}, {\"role\": \"system\", \"content\": \"Q: how many car makers are their in germany\"}, \"ideal\": [\"A: SELECT count ( * )  FROM CAR_MAKERS AS T1 JOIN COUNTRIES AS T2 ON T1.Country   =   T2.CountryId WHERE T2.CountryName   =   'germany'\"]}\n\n\nOne way to speed up the process of building eval datasets, is to use GPT-4 to generate synthetic data\nOnce we have the synthetic data, we need to convert it to match the format of the eval dataset.\nNext we need to create the eval registry to run it in the framework.\n\nThe evals framework requires a .yaml file structured with the following properties:\nid - An identifier for your eval\ndescription - A short description of your eval\ndisclaimer - An additional notes about your eval\nmetrics - There are three types of eval metrics we can choose from: match, includes, fuzzyMatch\n\nFor our eval, we will configure the following:\nRunning an evaluation\n\nWe can run this eval using the oaieval CLI. To get setup, install the library: pip install . (if you are running the OpenAI Evals library locally) or pip install oaieval if you are running an existing eval.\n\nThen, run the eval using the CLI: oaieval gpt-3.5-turbo spider-sql\n\nThis command expects a model name and an eval set name. Note that we provide two command line interfaces (CLIs): oaieval for running a single eval and oaievalset for running a set of evals. The valid eval names are specified in the YAML files under evals/registry/evals and their corresponding implementations can be found in evals/elsuite.\nThe oaieval CLI can accept various flags to modify the default behavior. You can run oaieval --help to see a full list of CLI options.\n\nAfter running that command, you’ll see the final report of accuracy printed to the console, as well as a file path to a temporary file that contains the full report.\nThese CLIs can accept various flags to modify their default behavior. You can run oaieval --help to see a full list of CLI options.\n\noaieval will search for the spider-sql eval YAML file in the evals/registry/evals directory, following the format specified in cell 4 above. The path to the eval dataset is specified in the eval YAML file under the args: parameter as samples_jsonl: sql/spider_sql.jsonl, with the file content in JSONL format (as generated in step 3 above).\n\nAfter running that command, you’ll see the final report of accuracy printed to the console, as well as a file path to a temporary file that contains the full report.\noaievalset expects a model name and an eval set name, for which the valid options are specified in the YAML files under evals/registry/eval_sets.\nGoing through eval logs\n\nThe eval logs are located at /tmp/evallogs and different log files are created for each evaluation run.\nThis file will contain structured logs of the evaluation. The first entry provides a detailed specification of the evaluation, including the completion functions, evaluation name, run configuration, creator’s name, run ID, and creation timestamp.\nLet's also look at the entry which provides the final report of the evaluation.\nWe can also review individual evaluation events that provide specific samples (sample_id), results, event types, and metadata.\nLet's review our failures to understand which tests did not succeed.\nReviewing some of the failures we see the following:\nThe second incorrect answer had an unnecessary join with the 'Templates' table. Our eval was able to accurately identify this and flag this as incorrect.\nFew other answers have minor syntax differences that caused the answers to get flagged.\n  In situations like this, it would be worthwhile exploring whether we should continue iterating on the prompt to ensure certain stylistic choices, or if we should modify the evaluation suite to capture this variation.\n  This type of failure hints at the potential need for model-graded evals as a way to ensure accuracy in grading the results\nConclusion\nBuilding out effective evals is a core part of the development cycle of LLM-based applications. The OpenAI Evals framework provides the core structure of building evals out of the box, and allows you to quickly spin up new tests for your various use cases. In this guide, we demonstrated step-by-step how to create an eval, run it, and analyze the results.\n\nThe example shown in this guide represent a straightfoward use case for evals. As you continue to explore this framework, we recommend you explore creating more complex model-graded evals for actual production use cases. Happy evaluating!"])</script><script>self.__next_f.push([1,"43:Tc07,"])</script><script>self.__next_f.push([1,"\nA guardrail is a set of rules and checks designed to ensure that the outputs of an LLM are accurate, appropriate, and aligned with user expectations. For more additional information on developing guardrails, you can refer to this guide on developing guardrails.\n\nIn this notebook, we'll walk through the process of developing an output guardrail that specifically checks model outputs for hallucinations.\n\nThis notebook will focus on:\nBuilding out a strong eval set\nIdentifying specific criteria to measure hallucinations\nImproving the accuracy of our guardrail with few-shot prompting\n\n1. Building out an eval set\n\nImagine we are a customer support team that is building out an automated support agent. We will be feeding the assistant information from our knowledge base about a specific set of policies for how to handle tickets such as returns, refunds, feedback, and expect the model to follow the policy when interacting with customers.\n\nThe first thing we will do is use GPT-4o to build out a set of policies that we will want to follow.\n\nIf you want to do deep dive into generating synthetic data, you can review our Synthetic Data Generation Cookbook here\nNext we'll take these policies and generate sample customer interactions that do or do not follow the instructions.\nNow let's iterate through the policies and generate some examples.\n2. Constructing our hallucination guardrail\n\nWhen building out our hallucination guardrail, here are some guiding principles:\n\nProvide very descriptive metrics to evaluate whether a response is accurate\nIt is important to break down this idea of \"truth\" in easily identifiable metrics that we can measure\nMetrics like truthfulness and relevance are difficult to measure. Giving concrete ways to score the statement can result in a more accurate guardrail\nEnsure consistency across key terminology\nIt is important to keep relevant terms such as knowledge base articles, assistants, and users consistent across the prompt\nIf we begin to use phrases such as assistant vs agent, the model could get confused\nStart with the most advanced model\nThere is a cost vs quality trade-off when using the most advanced models. Although GPT-4o may be more expensive, it is important to start with the most advanced model so we can ensure a high degree of accuracy\nOnce we have thoroughly tested out the guardrail and are confident in its performance, we can look to reducing cost by tuning it down to gpt-3.5-turbo\nEvaluate each sentence independently and the entire response as a whole\nIf the agent returns a long response, it can be useful to break down the response to individual sentences and evaluate them independently\nIn addition to that, evaluating the whole intent of the message as a whole can ensure that you don't lose important context\n\nWith all of this in mind, let's build out a guardrail system and measure its performance.\nFrom the results above we can see the program is performing well with a high precision and recall metric. This means that the guardrails are able to accurately identify hallucinations in the model outputs."])</script><script>self.__next_f.push([1,"44:Tb2c,"])</script><script>self.__next_f.push([1,"Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.\n1. Collect Wikipedia data about Olympic Games 2020\n\nThe idea of this project is to create a question answering model, based on a few paragraphs of provided text. Base GPT-3 models do a good job at answering questions when the answer is contained within the paragraph, however if the answer isn't contained, the base models tend to try their best to answer anyway, often leading to confabulated answers.\n\nTo create a model which answers questions only if there is sufficient context for doing so, we first create a dataset of questions and answers based on paragraphs of text. In order to train the model to answer only when the answer is present, we also add adversarial examples, where the question doesn't match the context. In those cases, we ask the model to output \"No sufficient context for answering the question\".\n\nWe will perform this task in three notebooks:\nThe first (this) notebook focuses on collecting recent data, which GPT-3 didn't see during its pre-training. We picked the topic of Olympic Games 2020 (which actually took place in the summer of 2021), and downloaded 713 unique pages. We organized the dataset by individual sections, which will serve as context for asking and answering the questions.\nThe second notebook will utilize Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section.\nThe third notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.\n1.1 Data extraction using the wikipedia API\nExtracting the data will take about half an hour, and processing will likely take about as much.\n1.2 Filtering the Wikipedia pages and splitting them into sections by headings\nWe remove sections unlikely to contain textual information, and ensure that each section is not longer than the token limit\n1.2.1 We create a dataset and filter out any sections with fewer than 40 tokens, as those are unlikely to contain enough context to ask a good question.\nSave the section dataset\nWe will save the section dataset, for the next notebook\n1.3 (Optional) Exploring the data\nThere appear to be winter and summer Olympics 2020. We chose to leave a little ambiguity and noise in the dataset, even though we were interested only in the Summer Olympics 2020.\nWe can see that the majority of section are fairly short (less than 500 tokens)."])</script><script>self.__next_f.push([1,"45:T141d,"])</script><script>self.__next_f.push([1,"Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.\n2. Creating a synthetic Q\u0026A dataset\nWe use davinci-instruct-beta-v3, a model specialized in following instructions, to create questions based on the given context. Then we also use davinci-instruct-beta-v3 to answer those questions, given the same context.\n\nThis is expensive, and will also take a long time, as we call the davinci engine for each section. You can simply download the final dataset instead.\n\nWe're using the dataset created using the previous notebook\n2.1 Read in the data, and create a context\nCreate a context by concatenating the title, the heading and the content of that section\n2.2 Create questions based on the context\nUse davinci-instruct to generate a number of plausible questions relating to the Wikipedia section contents.\n\nNote: We have used temperature=0, but it may be beneficial to experiment with a higher temperature to get a higher diversity of questions.\n\nWARNING: This step will last a long time, and consume a lot of tokens, as it calls davinci-instruct for every section to generate a number of questions.\nThe prompt is designed to generate a number of questions. Example questions above were generated based on the summary section of the 2020 Summer Olympics page.\n\nWe can observe that the questions 3 and 5 above repeat. Sometimes the generated questions could be ambiguous without the context. We will show that even despite these limitations we can create a successful model.\n2.3 Create answers based on the context\nUse davinci-instruct to answer the questions given the relevant Wikipedia section contents\n\nNote: We have used temperature=0, but it may be beneficial to experiment with a higher temperature to get a higher diversity of questions.\n\nWARNING: This step will last a long time, and consume a lot of tokens, as it calls davinci-instruct for every section to answer all the questions.\nThese are the answers to the questions above based on the context around the host city selection.\n\nWe can see that answers 3-5 contain the correct answer, but instead of answering the question directly, the answer is a verbatim extraction. Despite these occasional lower quality answers, we will show that the model can learn the task reasonably well, given a high number of examples.\n2.4 Save the Olympics Q\u0026A dataset based on Wikipedia sections\nWe save the file for use in the next notebook\n2.5 Search file (DEPRECATED)\nWe create a search file (API reference), which can be used to retrieve the relevant context when a question is asked.\n\nDEPRECATED: The /search endpoint is deprecated in favour of using embeddings. Embeddings are cheaper, faster and can support a better search experience. See Question Answering Guide for a search implementation using the embeddings\n\n2.6 Answer questions based on the context provided\n\nWe will use a simple implementation of the answers endpoint. This works by simply using the /search endpoint, which searches over an indexed file to obtain the relevant sections which can be included in the context, following by a question and answering prompt given a specified model.\nAfter we fine-tune the model for Q\u0026A we'll be able to use it instead of davinci-instruct-beta-v3, to obtain better answers when the question can't be answered based on the context. We see a downside of davinci-instruct-beta-v3, which always attempts to answer the question, regardless of the relevant context being present or not. (Note the second question is asking about a future event, set in 2024.)\nWe can see that davinci has a tendency to answer the question, even if the question can't be answered given the context provided. Note the question asked regarding 2048 Summer Olympics, which didn't happen yet, and the retrieved content has only returned results for 2020.\n2.7 (Optional) Investigation into how likely the search endpoint is to return the relevant context\nWe utilize the generated questions based on context to estimate how often we can retrieve the original context. These questions are noisy, so this is not a perfect estimate.\n\nOur questions and answers are prefixed with numbered bullet points, however due to the way they were generated, they are missing the first number, hence we add \"1.\" to the list of questions (and answers).\n\nWe calculate the rank of the section retrieved using ada search, and the number of tokens in the context needed to retrieve the relevant section in full.\nThe relevant context can be obtained 74% of the time on this dataset\n7.4% of the time, this is due to the keyword search part of the search algorithm not retrieving the relevant context within the first 200 results.\n18.3% of the time this is due to the semantic search not placing the relevant context within the first 2000 tokens.\nWe can observe that the context is most likely to be returned as one of the first results, and most likely to be returned within the first 200-500 tokens.\nprobabilities of the relevant context being returned at each rank. (-2 means a processing error, -1 means the rank is \u003e200)"])</script><script>self.__next_f.push([1,"46:Tf0b,"])</script><script>self.__next_f.push([1,"Note: To answer questions based on text documents, we recommend the procedure in Question Answering using Embeddings. Some of the code below may rely on deprecated API endpoints.\n3. Train a fine-tuning model specialized for Q\u0026A\nThis notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.\n\nWe will add hard adversarial examples as well, which will be based either on semantically similar sections, or neighbouring sections, originating from the same article.\nSplit the sections into a training and testing set\nwe check that the separator we intend to use isn't present within the contexts\n3.1 Create the fine-tuning datasets for Q\u0026A and discriminator models\nThe fine-tuning dataset is created in the following way. For every corresponding question, answer and context pair we create:\nPositive example: correct question, answer, context pair\nNegative examples:\n  random negative example, where the random context is paired with the question\n  two hard negative examples\n    one originating from the same wikipedia article\n    another, which is most similar to the correct context\n\nThis process is noisy, as sometimes the question might be answerable given a different context, but on average we hope this won't affect the performance too much.\n\nWe apply the same process of dataset creation for both the discriminator, and the Q\u0026A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set.\nWe apply the same process of dataset creation for both the discriminator, and the Q\u0026A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set.\nWe formatted the data according to the recommendations from the fine-tuning tool, which is available using\nopenai tools fine_tunes.prepare_data -f qa_train.jsonl\n\nWe highly recommend that you use this tool, which suggests improvements in your data formatting for fine-tuning.\n\n3.2 Submit the datasets for fine-tuning\n3.3 Using the fine-tuned models\n\nWe will now use the fine-tuned discriminator and the fine-tuned Q\u0026A model. By requesting logprobs, we can see how certain the discriminator is in a yes vs no answer.\nWe can see that the model can generalize well to different contexts and questions.\nWe can see that the model can answer the question, when the context is appropriate.\nWe can see that the model knows when to answer the question, and when to say that insufficient context is present to answer the question.\nWe can also combine a discriminator and a base model, or a fine-tuned Q\u0026A model. Discriminator can essentially serve as a decision whether the question can be answered given the context or not.\nThe above function illustrates how to potentially combine a discriminator and a fine-tuned Q\u0026A model. This gives a more fine-grained control over how certain we want the model to be before it answers the question.\n\nWe'll now take a look on how answers endpoint works - combining search to retrieve the relevant context from a knowledge base, and then using the fine-tuned Q\u0026A model to answer the question.\n3.4 Answering the question based on a knowledge base\nFinally we can use a logic similar to the /answers endpoint, where we first search for the relevant context, and then ask a Q\u0026A model to answer the question given that context. If you'd like to see the implementation details, check out the answers_with_ft.py file."])</script><script>self.__next_f.push([1,"47:T5db,"])</script><script>self.__next_f.push([1,"\nIn this Jupyter Notebook, we construct a Kangas DataGrid containing the data and projections of the embeddings into 2 dimensions.\nWhat is Kangas?\n\nKangas as an open source, mixed-media, dataframe-like tool for data scientists. It was developed by Comet, a company designed to help reduce the friction of moving models into production.\n1. Setup\n\nTo get started, we pip install kangas, and import it.\n2. Constructing a Kangas DataGrid\n\nWe create a Kangas Datagrid with the original data and the embeddings. The data is composed of a rows of reviews, and the embeddings are composed of 1536 floating-point values. In this example, we get the data directly from github, in case you aren't running this notebook inside OpenAI's repo.\n\nWe use Kangas to read the CSV file into a DataGrid for further processing.\nWe can review the fields of the CSV file:\nAnd get a glimpse of the first and last rows:\nNow, we create a new DataGrid, converting the numbers into an Embedding:\nThe new DataGrid now has an Embedding column with proper datatype.\nWe simply save the datagrid, and we're done.\n3. Render 2D Projections\n\nTo render the data directly in the notebook, simply show it. Note that each row contains an embedding projection.\n\nScroll to far right to see embeddings projection per row.\n\nThe color of the point in projection space represents the Score.\nGroup by \"Score\" to see rows of each group.\nAn example of this datagrid is hosted here: https://kangas.comet.com/?datagrid=/data/openai_embeddings.datagrid"])</script><script>self.__next_f.push([1,"48:Tc8a,"])</script><script>self.__next_f.push([1,"In this example notebook, we showcase how to perform financial analysis over 10-K documents with the LlamaIndex framework with just a few lines of code.\nNotebook Outline\nIntroduction\nSetup\nData Loading \u0026 Indexing\nSimple QA\nAdvanced QA - Compare and Contrast\n\nIntroduction\n\nLLamaIndex\nLlamaIndex is a data framework for LLM applications.\nYou can get started with just a few lines of code and build a retrieval-augmented generation (RAG) system in minutes.\nFor more advanced users, LlamaIndex offers a rich toolkit for ingesting and indexing your data, modules for retrieval and re-ranking, and composable components for building custom query engines.\n\nSee full documentation for more details.\n\nFinancial Analysis over 10-K documents\nA key part of a financial analyst's job is to extract information and synthesize insight from long financial documents.\nA great example is the 10-K form - an annual report required by the U.S. Securities and Exchange Commission (SEC), that gives a comprehensive summary of a company's financial performance.\nThese documents typically run hundred of pages in length, and contain domain-specific terminology that makes it challenging for a layperson to digest quickly.\n\n\nWe showcase how LlamaIndex can support a financial analyst in quickly extracting information and synthesize insights across multiple documents with very little coding.\nSetup\nTo begin, we need to install the llama-index library\nNow, we import all modules used in this tutorial\nBefore we start, we can configure the LLM provider and model that will power our RAG system.\nHere, we pick gpt-3.5-turbo-instruct from OpenAI.\nWe construct a ServiceContext and set it as the global default, so all subsequent operations that depends on LLM calls will use the model we configured here.\nData Loading and Indexing\nNow, we load and parse 2 PDFs (one for Uber 10-K in 2021 and another for Lyft 10-k in 2021).\nUnder the hood, the PDFs are converted to plain text Document objects, separate by page.\nNote: this operation might take a while to run, since each document is more than 100 pages.\nNow, we can build an (in-memory) VectorStoreIndex over the documents that we've loaded.\nNote: this operation might take a while to run, since it calls OpenAI API for computing vector embedding over document chunks.\nSimple QA\nNow we are ready to run some queries against our indices!\nTo do so, we first configure a QueryEngine, which just captures a set of configurations for how we want to query the underlying index.\n\nFor a VectorStoreIndex, the most common configuration to adjust is similarity_top_k which controls how many document chunks (which we call Node objects) are retrieved to use as context for answering our question.\nLet's see some queries in action!\nAdvanced QA - Compare and Contrast\nFor more complex financial analysis, one often needs to reference multiple documents.\n\nAs a example, let's take a look at how to do compare-and-contrast queries over both Lyft and Uber financials.\nFor this, we build a SubQuestionQueryEngine, which breaks down a complex compare-and-contrast query, into simpler sub-questions to execute on respective sub query engine backed by individual indices.\nLet's see these queries in action!"])</script><script>self.__next_f.push([1,"49:Tda9,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step by step on using PolarDB-PG as a vector database for OpenAI embeddings.\n\nThis notebook presents an end-to-end process of:\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in a cloud instance of PolarDB-PG.\nConverting raw text query to an embedding with OpenAI API.\nUsing PolarDB-PG to perform the nearest neighbour search in the created collection.\n\nWhat is PolarDB-PG\n\nPolarDB-PG is a high-performance vector database that adopts a read-write separation architecture. It is a cloud-native database managed by Alibaba Cloud, 100% compatible with PostgreSQL, and highly compatible with Oracle syntax. It supports processing massive vector data storage and queries, and greatly improves the efficiency of vector calculations through optimization of underlying execution algorithms, providing users with fast, elastic, high-performance, massive storage, and secure and reliable vector database services. Additionally, PolarDB-PG also supports multi-dimensional and multi-modal spatiotemporal information engines and geographic information engines.At the same time, PolarDB-PG is equipped with complete OLAP functionality and service level agreements, which has been recognized and used by many users;\n\nDeployment options\n\nUsing PolarDB-PG Cloud Vector Database. Click here to fast deploy it.\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nPolarDB-PG cloud server instance.\nThe 'psycopg2' library to interact with the vector database. Any other postgresql client library is ok.\nAn OpenAI API key.\nWe might validate if the server was launched successfully by running a simple curl command:\nInstall requirements\n\nThis notebook obviously requires the openai and psycopg2 packages, but there are also some other additional libraries we will use. The following command installs them all:\n\nPrepare your OpenAI API key\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\n\nIf you have any doubts about setting the API key through environment variables, please refer to Best Practices for API Key Safety.\nConnect to PolarDB\nFirst add it to your environment variables. or you can just change the \"psycopg2.connect\" parameters below\n\nConnecting to a running instance of PolarDB server is easy with the official Python library:\nWe can test the connection by running any available method:\nThe downloaded file has to be then extracted:\nIndex data\n\nPolarDB stores data in relation where each object is described by at least one vector. Our relation will be called articles and each object will be described by both title and content vectors.\n\nWe will start with creating a relation and create a vector index on both title and content, and then we will fill it with our precomputed embeddings.\nLoad data\n\nIn this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\nSearch data\n\nOnce the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search. Since the precomputed embeddings were created with text-embedding-3-small OpenAI model we also have to use it during search."])</script><script>self.__next_f.push([1,"4a:T51c,"])</script><script>self.__next_f.push([1,"SingleStoreDB has first-class support for vector search through our Vector Functions. Our vector database subsystem, first made available in 2017 and subsequently enhanced, allows extremely fast nearest-neighbor search to find objects that are semantically similar, easily using SQL.\n\nSingleStoreDB supports vectors and vector similarity search using dot_product (for cosine similarity) and euclidean_distance functions. These functions are used by our customers for applications including face recognition, visual product photo search and text-based semantic search. With the explosion of generative AI technology, these capabilities form a firm foundation for text-based AI chatbots.\n\nBut remember, SingleStoreDB is a high-performance, scalable, modern SQL DBMS that supports multiple data models including structured data, semi-structured data based on JSON, time-series, full text, spatial, key-value and of course vector data. Start powering your next intelligent application with SingleStoreDB today!\n\nSingleStore Open AI\n\nExample\n\nThis folder contains examples of using SingleStoreDB and OpenAI together. We will keep adding more scenarios so stay tuned!\n\n| Name | Description |\n| --- | --- |\n| OpenAI wikipedia semantic search | Improve ChatGPT accuracy through SingleStoreDB semantic Search in QA |\n"])</script><script>self.__next_f.push([1,"4b:Tc50,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step by step on using AnalyticDB as a vector database for OpenAI embeddings.\n\nThis notebook presents an end-to-end process of:\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in a cloud instance of AnalyticDB.\nConverting raw text query to an embedding with OpenAI API.\nUsing AnalyticDB to perform the nearest neighbour search in the created collection.\n\nWhat is AnalyticDB\n\nAnalyticDB is a high-performance distributed vector database. Fully compatible with PostgreSQL syntax, you can effortlessly utilize it. AnalyticDB is Alibaba Cloud managed cloud-native database with strong-performed vector compute engine. Absolute out-of-box experience allow to scale into billions of data vectors processing with rich features including indexing algorithms, structured \u0026 non-structured data features, realtime update, distance metrics, scalar filtering, time travel searches etc. Also equipped with full OLAP database functionality and SLA commitment for production usage promise;\n\nDeployment options\n\nUsing AnalyticDB Cloud Vector Database. Click here to fast deploy it.\n\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nAnalyticDB cloud server instance.\nThe 'psycopg2' library to interact with the vector database. Any other postgresql client library is ok.\nAn OpenAI API key.\n\n\nWe might validate if the server was launched successfully by running a simple curl command:\n\nInstall requirements\n\nThis notebook obviously requires the openai and psycopg2 packages, but there are also some other additional libraries we will use. The following command installs them all:\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to AnalyticDB\nFirst add it to your environment variables. or you can just change the \"psycopg2.connect\" parameters below\n\nConnecting to a running instance of AnalyticDB server is easy with the official Python library:\nWe can test the connection by running any available method:\nThe downloaded file has to be then extracted:\nIndex data\n\nAnalyticDB stores data in relation where each object is described by at least one vector. Our relation will be called articles and each object will be described by both title and content vectors. \\\n\nWe will start with creating a relation and create a vector index on both title and content, and then we will fill it with our precomputed embeddings.\nLoad data\n\nIn this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\nSearch data\n\nOnce the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search. Since the precomputed embeddings were created with text-embedding-3-small OpenAI model we also have to use it during search."])</script><script>self.__next_f.push([1,"4c:Te13,"])</script><script>self.__next_f.push([1,"This notebook presents how to implement a Question Answering system with Langchain, AnalyticDB as a knowledge based and OpenAI embeddings. If you are not familiar with AnalyticDB, it’s better to check out the Getting_started_with_AnalyticDB_and_OpenAI.ipynb notebook.\n\nThis notebook presents an end-to-end process of:\nCalculating the embeddings with OpenAI API.\nStoring the embeddings in an AnalyticDB instance to build a knowledge base.\nConverting raw text query to an embedding with OpenAI API.\nUsing AnalyticDB to perform the nearest neighbour search in the created collection to find some context.\nAsking LLM to find the answer in a given context.\n\nAll the steps will be simplified to calling some corresponding Langchain methods.\nPrerequisites\nFor the purposes of this exercise we need to prepare a couple of things:\nAnalyticDB cloud instance.\nLangchain as a framework.\nAn OpenAI API key.\nInstall requirements\nThis notebook requires the following Python packages: openai, tiktoken, langchain and psycopg2cffi.\nopenai provides convenient access to the OpenAI API.\ntiktoken is a fast BPE tokeniser for use with OpenAI's models.\nlangchain helps us to build applications with LLM more easily.\npsycopg2cffi library is used to interact with the vector database, but any other PostgreSQL client library is also acceptable.\nPrepare your OpenAI API key\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from [https://platform.openai.com/account/api-keys ).\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY by running following command:\nPrepare your AnalyticDB connection string\nTo build the AnalyticDB connection string, you need to have the following parameters: PG_HOST, PG_PORT, PG_DATABASE, PG_USER, and PG_PASSWORD. You need to export them first to set correct connect string. Then build the connection string.\nLoad data\nIn this section we are going to load the data containing some natural questions and answers to them. All the data will be used to create a Langchain application with AnalyticDB being the knowledge base.\nChain definition\n\nLangchain is already integrated with AnalyticDB and performs all the indexing for given list of documents. In our case we are going to store the set of answers we have.\nAt this stage all the possible answers are already stored in  AnalyticDB, so we can define the whole QA chain.\nSearch data\n\nOnce the data is put into AnalyticDB we can start asking some questions. A question will be automatically vectorized by OpenAI model, and the created vector will be used to find some possibly matching answers in AnalyticDB. Once retrieved, the most similar answers will be incorporated into the prompt sent to OpenAI Large Language Model.\n\nCustom prompt templates\n\nThe stuff chain type in Langchain uses a specific prompt with question and context documents incorporated. This is what the default prompt looks like:\n\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n{context}\nQuestion: {question}\nHelpful Answer:\n\nWe can, however, provide our prompt template and change the behaviour of the OpenAI LLM, while still using the stuff chain type. It is important to keep {context} and {question} as placeholders.\n\nExperimenting with custom prompts\n\nWe can try using a different prompt template, so the model:\nResponds with a single-sentence answer if it knows it.\nSuggests a random song title if it doesn't know the answer to our question."])</script><script>self.__next_f.push([1,"4d:Tb0d,"])</script><script>self.__next_f.push([1,"This notebook provides step by step instuctions on using Azure AI Search (f.k.a Azure Cognitive Search) as a vector database with OpenAI embeddings. Azure AI Search is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\n\nPrerequistites:\nFor the purposes of this exercise you must have the following:\nAzure AI Search Service\nOpenAI Key or Azure OpenAI credentials\nImport required libraries\nConfigure OpenAI settings\n\nThis section guides you through setting up authentication for Azure OpenAI, allowing you to securely interact with the service using either Azure Active Directory (AAD) or an API key. Before proceeding, ensure you have your Azure OpenAI endpoint and credentials ready. For detailed instructions on setting up AAD with Azure OpenAI, refer to the official documentation.\n\nConfigure Azure AI Search Vector Store settings\nThis section explains how to set up the Azure AI Search client for integrating with the Vector Store feature. You can locate your Azure AI Search service details in the Azure Portal or programmatically via the Search Management SDK.\n\nLoad data\n\nCreate an index\nThis code snippet demonstrates how to define and create a search index using the SearchIndexClient from the Azure AI Search Python SDK. The index incorporates both vector search and semantic ranker capabilities. For more details, visit our documentation on how to Create a Vector Index\nUploading Data to Azure AI Search Index\n\nThe following code snippet outlines the process of uploading a batch of documents—specifically, Wikipedia articles with pre-computed embeddings—from a pandas DataFrame to an Azure AI Search index. For a detailed guide on data import strategies and best practices, refer to  Data Import in Azure AI Search.\n\nIf your dataset didn't already contain pre-computed embeddings, you can create embeddings by using the below function using the openai python library. You'll also notice the same function and model are being used to generate query embeddings for performing vector searches.\nPerform a vector similarity search\nPerform a Hybrid Search\nHybrid search combines the capabilities of traditional keyword-based search with vector-based similarity search to provide more relevant and contextual results. This approach is particularly useful when dealing with complex queries that benefit from understanding the semantic meaning behind the text.\n\nThe provided code snippet demonstrates how to execute a hybrid search query:\nPerform a Hybrid Search with Reranking (powered by Bing)\nSemantic ranker measurably improves search relevance by using language understanding to rerank search results. Additionally, you can get extractive captions, answers, and highlights."])</script><script>self.__next_f.push([1,"4e:T2d7d,"])</script><script>self.__next_f.push([1,"\nCQL Version\nIn this quickstart you will learn how to build a \"philosophy quote finder \u0026 generator\" using OpenAI's vector embeddings and Apache Cassandra®, or equivalently DataStax Astra DB through CQL, as the vector store for data persistence.\n\nThe basic workflow of this notebook is outlined below. You will evaluate and store the vector embeddings for a number of quotes by famous philosophers, use them to build a powerful search engine and, after that, even a generator of new quotes!\n\nThe notebook exemplifies some of the standard usage patterns of vector search -- while showing how easy is it to get started with the vector capabilities of Cassandra / Astra DB through CQL.\n\nFor a background on using vector search and text embeddings to build a question-answering system, please check out this excellent hands-on notebook: Question answering using embeddings.\n\nChoose-your-framework\n\nPlease note that this notebook uses the Cassandra drivers and runs CQL (Cassandra Query Language) statements directly, but we cover other choices of technology to accomplish the same task. Check out this folder's README for other options. This notebook can run either as a Colab notebook or as a regular Jupyter notebook.\n\nTable of contents:\nSetup\nGet DB connection\nConnect to OpenAI\nLoad quotes into the Vector Store\nUse case 1: quote search engine\nUse case 2: quote generator\n(Optional) exploit partitioning in the Vector Store\nHow it works\n\nIndexing\n\nEach quote is made into an embedding vector with OpenAI's Embedding. These are saved in the Vector Store for later use in searching. Some metadata, including the author's name and a few other pre-computed tags, are stored alongside, to allow for search customization.\n\n1_vector_indexing_cql\n\nSearch\n\nTo find a quote similar to the provided search quote, the latter is made into an embedding vector on the fly, and this vector is used to query the store for similar vectors ... i.e. similar quotes that were previously indexed. The search can optionally be constrained by additional metadata (\"find me quotes by Spinoza similar to this one ...\").\n\n2_vector_search_cql\n\nThe key point here is that \"quotes similar in content\" translates, in vector space, to vectors that are metrically close to each other: thus, vector similarity search effectively implements semantic similarity. This is the key reason vector embeddings are so powerful.\n\nThe sketch below tries to convey this idea. Each quote, once it's made into a vector, is a point in space. Well, in this case it's on a sphere, since OpenAI's embedding vectors, as most others, are normalized to unit length. Oh, and the sphere is actually not three-dimensional, rather 1536-dimensional!\n\nSo, in essence, a similarity search in vector space returns the vectors that are closest to the query vector:\n\n3_vector_space\n\nGeneration\n\nGiven a suggestion (a topic or a tentative quote), the search step is performed, and the first returned results (quotes) are fed into an LLM prompt which asks the generative model to invent a new text along the lines of the passed examples and the initial suggestion.\n\n4_quote_generation\nSetup\nInstall and import the necessary dependencies:\nDon't mind the next cell too much, we need it to detect Colabs and let you upload the SCB file (see below):\nGet DB connection\nA couple of secrets are required to create a Session object (a connection to your Astra DB instance).\n\n(Note: some steps will be slightly different on Google Colab and on local Jupyter, that's why the notebook will detect the runtime type.)\nCreation of the DB connection\n\nThis is how you create a connection to Astra DB:\n\n(Incidentally, you could also use any Cassandra cluster (as long as it provides Vector capabilities), just by changing the parameters to the following Cluster instantiation.)\nCreation of the Vector table in CQL\n\nYou need a table which support vectors and is equipped with metadata. Call it \"philosophers_cql\".\n\nEach row will store: a quote, its vector embedding, the quote author and a set of \"tags\". You also need a primary key to ensure uniqueness of rows.\n\nThe following is the full CQL command that creates the table (check out this page for more on the CQL syntax of this and the following statements):\nPass this statement to your database Session to execute it:\nAdd a vector index for ANN search\nIn order to run ANN (approximate-nearest-neighbor) searches on the vectors in the table, you need to create a specific index on the embedding_vector column.\n\nWhen creating the index, you can optionally choose the \"similarity function\" used to compute vector distances: since for unit-length vectors (such as those from OpenAI) the \"cosine difference\" is the same as the \"dot product\", you'll use the latter which is computationally less expensive.\n\nRun this CQL statement:\nAdd indexes for author and tag filtering\nThat is enough to run vector searches on the table ... but you want to be able to optionally specify an author and/or some tags to restrict the quote search. Create two other indexes to support this:\nConnect to OpenAI\nSet up your secret key\nA test call for embeddings\n\nQuickly check how one can get the embedding vectors for a list of input texts:\nNote: the above is the syntax for OpenAI v1.0+. If using previous versions, the code to get the embeddings will look different.\nLoad quotes into the Vector Store\nGet a dataset with the quotes. (We adapted and augmented the data from this Kaggle dataset, ready to use in this demo.)\nA quick inspection:\nCheck the dataset size:\nInsert quotes into vector store\n\nYou will compute the embeddings for the quotes and save them into the Vector Store, along with the text itself and the metadata planned for later use.\n\nTo optimize speed and reduce the calls, you'll perform batched calls to the embedding OpenAI service.\n\nThe DB write is accomplished with a CQL statement. But since you'll run this particular insertion several times (albeit with different values), it's best to prepare the statement and then just run it over and over.\n\n(Note: for faster insertion, the Cassandra drivers would let you do concurrent inserts, which we don't do here for a more straightforward demo code.)\nUse case 1: quote search engine\nFor the quote-search functionality, you need first to make the input quote into a vector, and then use it to query the store (besides handling the optional metadata into the search call, that is).\n\nEncapsulate the search-engine functionality into a function for ease of re-use:\nPutting search to test\nPassing just a quote:\nSearch restricted to an author:\nSearch constrained to a tag (out of those saved earlier with the quotes):\nCutting out irrelevant results\n\nThe vector similarity search generally returns the vectors that are closest to the query, even if that means results that might be somewhat irrelevant if there's nothing better.\n\nTo keep this issue under control, you can get the actual \"similarity\" between the query and each result, and then set a cutoff on it, effectively discarding results that are beyond that threshold.\nTuning this threshold correctly is not an easy problem: here, we'll just show you the way.\n\nTo get a feeling on how this works, try the following query and play with the choice of quote and threshold to compare the results:\n\nNote (for the mathematically inclined): this value is a rescaling between zero and one of the cosine difference between the vectors, i.e. of the scalar product divided by the product of the norms of the two vectors. In other words, this is 0 for opposite-facing vecors and +1 for parallel vectors. For other measures of similarity, check the documentation -- and keep in mind that the metric in the SELECT query should match the one used when creating the index earlier for meaningful, ordered results.\nUse case 2: quote generator\nFor this task you need another component from OpenAI, namely an LLM to generate the quote for us (based on input obtained by querying the Vector Store).\n\nYou also need a template for the prompt that will be filled for the generate-quote LLM completion task.\nLike for search, this functionality is best wrapped into a handy function (which internally uses search):\nNote: similar to the case of the embedding computation, the code for the Chat Completion API would be slightly different for OpenAI prior to v1.0.\nPutting quote generation to test\nJust passing a text (a \"quote\", but one can actually just suggest a topic since its vector embedding will still end up at the right place in the vector space):\nUse inspiration from just a single philosopher:\n(Optional) Partitioning\nThere's an interesting topic to examine before completing this quickstart. While, generally, tags and quotes can be in any relationship (e.g. a quote having multiple tags), authors are effectively an exact grouping (they define a \"disjoint partitioning\" on the set of quotes): each quote has exactly one author (for us, at least).\n\nNow, suppose you know in advance your application will usually (or always) run queries on a single author. Then you can take full advantage of the underlying database structure: if you group quotes in partitions (one per author), vector queries on just an author will use less resources and return much faster.\n\nWe'll not dive into the details here, which have to do with the Cassandra storage internals: the important message is that if your queries are run within a group, consider partitioning accordingly to boost performance.\n\nYou'll now see this choice in action.\nThe partitioning per author calls for a new table schema: create a new table called \"philosophers_cql_partitioned\", along with the necessary indexes:\nNow repeat the compute-embeddings-and-insert step on the new table.\n\nYou could use the very same insertion code as you did earlier, because the differences are hidden \"behind the scenes\": the database will store the inserted rows differently according to the partitioning scheme of this new table.\n\nHowever, by way of demonstration, you will take advantage of a handy facility offered by the Cassandra drivers to easily run several queries (in this case, INSERTs) concurrently. This is something that Cassandra / Astra DB through CQL supports very well and can lead to a significant speedup, with very little changes in the client code.\n\n(Note: one could additionally have cached the embeddings computed previously to save a few API tokens -- here, however, we wanted to keep the code easier to inspect.)\nDespite the different table schema, the DB query behind the similarity search is essentially the same:\nThat's it: the new table still supports the \"generic\" similarity searches all right ...\n... but it's when an author is specified that you would notice a huge performance advantage:\nWell, you would notice a performance gain, if you had a realistic-size dataset. In this demo, with a few tens of entries, there's no noticeable difference -- but you get the idea.\nConclusion\n\nCongratulations! You have learned how to use OpenAI for vector embeddings and Astra DB / Cassandra for storage in order to build a sophisticated philosophical search engine and quote generator.\n\nThis example used the Cassandra drivers and runs CQL (Cassandra Query Language) statements directly to interface with the Vector Store - but this is not the only choice. Check the README for other options and integration with popular frameworks.\n\nTo find out more on how Astra DB's Vector Search capabilities can be a key ingredient in your ML/GenAI applications, visit Astra DB's web page on the topic.\nCleanup\n\nIf you want to remove all resources used for this demo, run this cell (warning: this will delete the tables and the data inserted in them!):"])</script><script>self.__next_f.push([1,"4f:T290f,"])</script><script>self.__next_f.push([1,"\nCassIO version\nIn this quickstart you will learn how to build a \"philosophy quote finder \u0026 generator\" using OpenAI's vector embeddings and Apache Cassandra®, or equivalently DataStax Astra DB through CQL, as the vector store for data persistence.\n\nThe basic workflow of this notebook is outlined below. You will evaluate and store the vector embeddings for a number of quotes by famous philosophers, use them to build a powerful search engine and, after that, even a generator of new quotes!\n\nThe notebook exemplifies some of the standard usage patterns of vector search -- while showing how easy is it to get started with the vector capabilities of Cassandra / Astra DB through CQL.\n\nFor a background on using vector search and text embeddings to build a question-answering system, please check out this excellent hands-on notebook: Question answering using embeddings.\n\nChoose-your-framework\n\nPlease note that this notebook uses the CassIO library, but we cover other choices of technology to accomplish the same task. Check out this folder's README for other options. This notebook can run either as a Colab notebook or as a regular Jupyter notebook.\n\nTable of contents:\nSetup\nGet DB connection\nConnect to OpenAI\nLoad quotes into the Vector Store\nUse case 1: quote search engine\nUse case 2: quote generator\n(Optional) exploit partitioning in the Vector Store\nHow it works\n\nIndexing\n\nEach quote is made into an embedding vector with OpenAI's Embedding. These are saved in the Vector Store for later use in searching. Some metadata, including the author's name and a few other pre-computed tags, are stored alongside, to allow for search customization.\n\n1_vector_indexing\n\nSearch\n\nTo find a quote similar to the provided search quote, the latter is made into an embedding vector on the fly, and this vector is used to query the store for similar vectors ... i.e. similar quotes that were previously indexed. The search can optionally be constrained by additional metadata (\"find me quotes by Spinoza similar to this one ...\").\n\n2_vector_search\n\nThe key point here is that \"quotes similar in content\" translates, in vector space, to vectors that are metrically close to each other: thus, vector similarity search effectively implements semantic similarity. This is the key reason vector embeddings are so powerful.\n\nThe sketch below tries to convey this idea. Each quote, once it's made into a vector, is a point in space. Well, in this case it's on a sphere, since OpenAI's embedding vectors, as most others, are normalized to unit length. Oh, and the sphere is actually not three-dimensional, rather 1536-dimensional!\n\nSo, in essence, a similarity search in vector space returns the vectors that are closest to the query vector:\n\n3_vector_space\n\nGeneration\n\nGiven a suggestion (a topic or a tentative quote), the search step is performed, and the first returned results (quotes) are fed into an LLM prompt which asks the generative model to invent a new text along the lines of the passed examples and the initial suggestion.\n\n4_quote_generation\nSetup\nFirst install some required packages:\nGet DB connection\nIn order to connect to your Astra DB through CQL, you need two things:\nA Token, with role \"Database Administrator\" (it looks like AstraCS:...)\nthe database ID (it looks like 3df2a5b6-...)\n\n    Make sure you have both strings -- which are obtained in the Astra UI once you sign in. For more information, see here: database ID and Token.\n\nIf you want to connect to a Cassandra cluster (which however must support Vector Search), replace with cassio.init(session=..., keyspace=...) with suitable Session and keyspace name for your cluster.\nCreation of the DB connection\n\nThis is how you create a connection to Astra DB through CQL:\n\n(Incidentally, you could also use any Cassandra cluster (as long as it provides Vector capabilities), just by changing the parameters to the following Cluster instantiation.)\nCreation of the Vector Store through CassIO\n\nYou need a table which support vectors and is equipped with metadata. Call it \"philosophers_cassio\":\nConnect to OpenAI\nSet up your secret key\nA test call for embeddings\n\nQuickly check how one can get the embedding vectors for a list of input texts:\nNote: the above is the syntax for OpenAI v1.0+. If using previous versions, the code to get the embeddings will look different.\nLoad quotes into the Vector Store\nNote: the above is the syntax for OpenAI v1.0+. If using previous versions, the code to get the embeddings will look different.\nA quick inspection:\nCheck the dataset size:\nInsert quotes into vector store\n\nYou will compute the embeddings for the quotes and save them into the Vector Store, along with the text itself and the metadata planned for later use. Note that the author is added as a metadata field along with the \"tags\" already found with the quote itself.\n\nTo optimize speed and reduce the calls, you'll perform batched calls to the embedding OpenAI service.\n\n(Note: for faster execution, Cassandra and CassIO would let you do concurrent inserts, which we don't do here for a more straightforward demo code.)\nUse case 1: quote search engine\nFor the quote-search functionality, you need first to make the input quote into a vector, and then use it to query the store (besides handling the optional metadata into the search call, that is).\n\nEncapsulate the search-engine functionality into a function for ease of re-use:\nPutting search to test\nPassing just a quote:\nSearch restricted to an author:\nSearch constrained to a tag (out of those saved earlier with the quotes):\nCutting out irrelevant results\n\nThe vector similarity search generally returns the vectors that are closest to the query, even if that means results that might be somewhat irrelevant if there's nothing better.\n\nTo keep this issue under control, you can get the actual \"distance\" between the query and each result, and then set a cutoff on it, effectively discarding results that are beyond that threshold.\nTuning this threshold correctly is not an easy problem: here, we'll just show you the way.\n\nTo get a feeling on how this works, try the following query and play with the choice of quote and threshold to compare the results:\n\nNote (for the mathematically inclined): this \"distance\" is exactly the cosine similarity between the vectors, i.e. the scalar product divided by the product of the norms of the two vectors. As such, it is a number ranging from -1 to +1, where -1 is for exactly opposite-facing vectors and +1 for identically-oriented vectors. Elsewhere (e.g. in the \"CQL\" counterpart of this demo) you would get a rescaling of this quantity to fit the [0, 1] interval, which means the resulting numerical values and adequate thresholds there are transformed accordingly.\nUse case 2: quote generator\nFor this task you need another component from OpenAI, namely an LLM to generate the quote for us (based on input obtained by querying the Vector Store).\n\nYou also need a template for the prompt that will be filled for the generate-quote LLM completion task.\nLike for search, this functionality is best wrapped into a handy function (which internally uses search):\nNote: similar to the case of the embedding computation, the code for the Chat Completion API would be slightly different for OpenAI prior to v1.0.\nPutting quote generation to test\nJust passing a text (a \"quote\", but one can actually just suggest a topic since its vector embedding will still end up at the right place in the vector space):\nUse inspiration from just a single philosopher:\n(Optional) Partitioning\nThere's an interesting topic to examine before completing this quickstart. While, generally, tags and quotes can be in any relationship (e.g. a quote having multiple tags), authors are effectively an exact grouping (they define a \"disjoint partitioning\" on the set of quotes): each quote has exactly one author (for us, at least).\n\nNow, suppose you know in advance your application will usually (or always) run queries on a single author. Then you can take full advantage of the underlying database structure: if you group quotes in partitions (one per author), vector queries on just an author will use less resources and return much faster.\n\nWe'll not dive into the details here, which have to do with the Cassandra storage internals: the important message is that if your queries are run within a group, consider partitioning accordingly to boost performance.\n\nYou'll now see this choice in action.\nFirst, you need a different table abstraction from CassIO:\nNow repeat the compute-embeddings-and-insert step on the new table.\n\nCompared to what you have seen earlier, there is a crucial difference in that now the quote's author is stored as the partition id for the inserted row, instead of being added to the catch-all \"metadata\" dictionary.\n\nWhile you are at it, by way of demonstration, you will insert all quotes by a given author concurrently: with CassIO, this is done by usng the asynchronous put_async method for each quote, collecting the resulting list of Future objects, and calling the result() method on them all afterwards, to ensure they all have executed. Cassandra / Astra DB well supports a high degree of concurrency in I/O operations.\n\n(Note: one could have cached the embeddings computed previously to save a few API tokens -- here, however, we wanted to keep the code easier to inspect.)\nWith this new table, the similarity search changes accordingly (note the arguments to ann_search):\nThat's it: the new table still supports the \"generic\" similarity searches all right ...\n... but it's when an author is specified that you would notice a huge performance advantage:\nWell, you would notice a performance gain, if you had a realistic-size dataset. In this demo, with a few tens of entries, there's no noticeable difference -- but you get the idea.\nConclusion\n\nCongratulations! You have learned how to use OpenAI for vector embeddings and Cassandra / Astra DB through CQL for storage in order to build a sophisticated philosophical search engine and quote generator.\n\nThis example used CassIO to interface with the Vector Store - but this is not the only choice. Check the README for other options and integration with popular frameworks.\n\nTo find out more on how Astra DB's Vector Search capabilities can be a key ingredient in your ML/GenAI applications, visit Astra DB's web page on the topic.\nCleanup\n\nIf you want to remove all resources used for this demo, run this cell (warning: this will delete the tables and the data inserted in them!):"])</script><script>self.__next_f.push([1,"50:T5a8,"])</script><script>self.__next_f.push([1,"\nThe demos in this directory show how to use the Vector\nSearch capabilities available today in DataStax Astra DB, a serverless\nDatabase-as-a-Service built on Apache Cassandra®.\n\nThese example notebooks demonstrate implementation of\nthe same GenAI standard RAG workload with different libraries and APIs.\n\nTo use Astra DB\nwith its HTTP API interface, head to the \"AstraPy\" notebook (astrapy\nis the Python client to interact with the database).\n\nIf you prefer CQL access to the database (either with\nAstra DB\nor a Cassandra cluster\nsupporting vector search),\ncheck the \"CQL\" or \"CassIO\" notebooks -- they differ in the level of abstraction you get to work at.\n\nIf you want to know more about Astra DB and its Vector Search capabilities,\nhead over to datastax.com.\n\nExample notebooks\n\nThe following examples show how easily OpenAI and DataStax Astra DB can\nwork together to power vector-based AI applications. You can run them either\nwith your local Jupyter engine or as Colab notebooks:\n\n| Use case | Target database | Framework | Notebook | Google Colab |\n| -------- | --------------- | --------- | -------- | ------------ |\n| Search/generate quotes | Astra DB | AstraPy | Notebook | Colab |\n| Search/generate quotes | Cassandra / Astra DB through CQL | CassIO | Notebook | Colab |\n| Search/generate quotes | Cassandra / Astra DB through CQL | Plain Cassandra language | Notebook | Colab |\n\nVector similarity, visual representation\n\n3_vector_space\n"])</script><script>self.__next_f.push([1,"51:Tf17,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nChroma**:\n    Setup: Here we'll set up the Python client for Chroma. For more details go here\n    Index Data: We'll create collections with vectors for titles and content\n    Search Data: We'll run a few searches to confirm it works\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nChroma\n\nWe'll index these embedded documents in a vector database and search them. The first option we'll look at is Chroma, an easy to use open-source self-hosted in-memory vector database, designed for working with embeddings together with LLMs.\n\nIn this section, we will:\nInstantiate the Chroma client\nCreate collections for each class of embedding\nQuery each collection\nInstantiate the Chroma client\n\nCreate the Chroma client. By default, Chroma is ephemeral and runs in memory.\nHowever, you can easily set up a persistent configuration which writes to disk.\nCreate collections\n\nChroma collections allow you to store and filter with arbitrary metadata, making it easy to query subsets of the embedded data.\n\nChroma is already integrated with OpenAI's embedding functions. The best way to use them is on construction of a collection, as follows.\nAlternatively, you can 'bring your own embeddings'. More information can be found here\nPopulate the collections\n\nChroma collections allow you to populate, and filter on, whatever metadata you like. Chroma can also store the text alongside the vectors, and return everything in a single query call, when this is more convenient.\n\nFor this use-case, we'll just store the embeddings and IDs, and use these to index the original dataframe.\nSearch the collections\n\nChroma handles embedding queries for you if an embedding function is set, like in this example.\nNow that you've got a basic embeddings search running, you can hop over to the Chroma docs to learn more about how to add filters to your query, update/delete data in your collections, and deploy Chroma."])</script><script>self.__next_f.push([1,"52:T2319,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step-by-step through answering questions about a collection of data, using Chroma, an open-source embeddings database, along with OpenAI's text embeddings and chat completion API's.\n\nAdditionally, this notebook demonstrates some of the tradeoffs in making a question answering system more robust. As we shall see, simple querying doesn't always create the best results!\n\nQuestion Answering with LLMs\n\nLarge language models (LLMs) like OpenAI's ChatGPT can be used to answer questions about data that the model may not have been trained on, or have access to. For example;\n\nPersonal data like e-mails and notes\nHighly specialized data like archival or legal documents\nNewly created data like recent news stories\n\nIn order to overcome this limitation, we can use a data store which is amenable to querying in natural language, just like the LLM itself. An embeddings store like Chroma represents documents as embeddings, alongside the documents themselves.\n\nBy embedding a text query, Chroma can find relevant documents, which we can then pass to the LLM to answer our question. We'll show detailed examples and variants of this approach.\nSetup and preliminaries\n\nFirst we make sure the python dependencies we need are installed.\nWe use OpenAI's API's throughout this notebook. You can get an API key from https://beta.openai.com/account/api-keys\n\nYou can add your API key as an environment variable by executing the command export OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx in a terminal. Note that you will need to reload the notebook if the environment variable wasn't set yet. Alternatively, you can set it in the notebook, see below.\nDataset\n\nThroughout this notebook, we use the SciFact dataset. This is a curated dataset of expert annotated scientific claims, with an accompanying text corpus of paper titles and abstracts. Each claim may be supported, contradicted, or not have enough evidence either way, according to the documents in the corpus.\n\nHaving the corpus available as ground-truth allows us to investigate how well the following approaches to LLM question answering perform.\nJust asking the model\n\nGPT-3.5 was trained on a large amount of scientific information. As a baseline, we'd like to understand what the model already knows without any further context. This will allow us to calibrate overall performance.\n\nWe construct an appropriate prompt, with some example facts, then query the model with each claim in the dataset. We ask the model to assess a claim as 'True', 'False', or 'NEE' if there is not enough evidence one way or the other.\nWe sample 100 claims from the dataset\nWe evaluate the ground-truth according to the dataset. From the dataset description, each claim is either supported or contradicted by the evidence, or else there isn't enough evidence either way.\nWe also output the confusion matrix, comparing the model's assessments with the ground truth, in an easy to read table.\nWe ask the model to directly assess the claims, without additional context.\nResults\n\nFrom these results we see that the LLM is strongly biased to assess claims as true, even when they are false, and also tends to assess false claims as not having enough evidence. Note that 'not enough evidence' is with respect to the model's assessment of the claim in a vacuum, without additional context.\n\nAdding context\n\nWe now add the additional context available from the corpus of paper titles and abstracts. This section shows how to load a text corpus into Chroma, using OpenAI text embeddings.\nFirst, we load the text corpus.\nLoading the corpus into Chroma\n\nThe next step is to load the corpus into Chroma. Given an embedding function, Chroma will automatically handle embedding each document, and will store it alongside its text and metadata, making it simple to query.\nWe instantiate a (ephemeral) Chroma client, and create a collection for the SciFact title and abstract corpus.\nChroma can also be instantiated in a persisted configuration; learn more at the Chroma docs.\nNext we load the corpus into Chroma. Because this data loading is memory intensive, we recommend using a batched loading scheme in batches of 50-1000. For this example it should take just over one minute for the entire corpus. It's being embedded in the background, automatically, using the embedding_function we specified earlier.\nRetrieving context\n\nNext we retrieve documents from the corpus which may be relevant to each claim in our sample. We want to provide these as context to the LLM for evaluating the claims. We retrieve the 3 most relevant documents for each claim, according to the embedding distance.\nWe create a new prompt, this time taking into account the additional context we retrieve from the corpus.\nThen ask the model to evaluate the claims with the retrieved context.\nResults\n\nWe see that the model is a lot less likely to evaluate a False claim as true (2 instances VS 5 previously), but that claims without enough evidence are still often assessed as True or False.\n\nTaking a look at the retrieved documents, we see that they are sometimes not relevant to the claim - this causes the model to be confused by the extra information, and it may decide that sufficient evidence is present, even when the information is irrelevant. This happens because we always ask for the 3 'most' relevant documents, but these might not be relevant at all beyond a certain point.\nFiltering context on relevance\n\nAlong with the documents themselves, Chroma returns a distance score. We can try thresholding on distance, so that fewer irrelevant documents make it into the context we provide the model.\n\nIf, after filtering on the threshold, no context documents remain, we bypass the model and simply return that there is not enough evidence.\nNow we assess the claims using this cleaner context.\nResults\n\nThe model now assesses many fewer claims as True or False when there is not enough evidence present. However, it now biases away from certainty. Most claims are now assessed as having not enough evidence, because a large fraction of them are filtered out by the distance threshold. It's possible to tune the distance threshold to find the optimal operating point, but this can be difficult, and is dataset and embedding model dependent.\nHypothetical Document Embeddings: Using hallucinations productively\n\nWe want to be able to retrieve relevant documents, without retrieving less relevant ones which might confuse the model. One way to accomplish this is to improve the retrieval query.\n\nUntil now, we have queried the dataset using claims which are single sentence statements, while the corpus contains abstracts describing a scientific paper. Intuitively, while these might be related, there are significant differences in their structure and meaning. These differences are encoded by the embedding model, and so influence the distances between the query and the most relevant results.\n\nWe can overcome this by leveraging the power of LLMs to generate relevant text. While the facts might be hallucinated, the content and structure of the documents the models generate is more similar to the documents in our corpus, than the queries are. This could lead to better queries and hence better results.\n\nThis approach is called Hypothetical Document Embeddings (HyDE), and has been shown to be quite good at the retrieval task. It should help us bring more relevant information into the context, without polluting it.\n\nTL;DR:\nyou get much better matches when you embed whole abstracts rather than single sentences\nbut claims are usually single sentences\nSo HyDE shows that using GPT3 to expand claims into hallucinated abstracts and then searching based on those abstracts works (claims -\u003e abstracts -\u003e results) better than searching directly (claims -\u003e results)\nFirst, we use in-context examples to prompt the model to generate documents similar to what's in the corpus, for each claim we want to assess.\nWe hallucinate a document for each claim.\n\nNB: This can take a while, about 30m for 100 claims. You can reduce the number of claims we want to assess to get results more quickly.\nWe use the hallucinated documents as queries into the corpus, and filter the results using the same distance threshold.\nWe then ask the model to assess the claims, using the new context.\nResults\n\nCombining HyDE with a simple distance threshold leads to a significant improvement. The model no longer biases assessing claims as True, nor toward their not being enough evidence. It also correctly assesses when there isn't enough evidence more often.\nConclusion\n\nEquipping LLMs with a context based on a corpus of documents is a powerful technique for bringing the general reasoning and natural language interactions of LLMs to your own data. However, it's important to know that naive query and retrieval may not produce the best possible results! Ultimately understanding the data will help get the most out of the retrieval based question-answering approach."])</script><script>self.__next_f.push([1,"53:T123f,"])</script><script>self.__next_f.push([1,"Open In Colab\n\nThis notebook demonstrates how to:\nIndex the OpenAI Wikipedia vector dataset into Elasticsearch\nEmbed a question with the OpenAI embeddings endpoint\nPerform semantic search on the Elasticsearch index using the encoded question\nSend the top search results to the OpenAI Chat Completions API endpoint for retrieval augmented generation (RAG)\n\nℹ️ If you've already worked through our semantic search notebook, you can skip ahead to the final step!\nInstall packages and import modules\nConnect to Elasticsearch\n\nℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook.\nIf you don't already have an Elastic deployment, you can sign up for a free Elastic Cloud trial.\n\nTo connect to Elasticsearch, you need to create a client instance with the Cloud ID and password for your deployment.\n\nFind the Cloud ID for your deployment by going to https://cloud.elastic.co/deployments and selecting your deployment.\nDownload the dataset\n\nIn this step we download the OpenAI Wikipedia embeddings dataset, and extract the zip file.\nRead CSV file into a Pandas DataFrame.\n\nNext we use the Pandas library to read the unzipped CSV file into a DataFrame. This step makes it easier to index the data into Elasticsearch in bulk.\nCreate index with mapping\n\nNow we need to create an Elasticsearch index with the necessary mappings. This will enable us to index the data into Elasticsearch.\n\nWe use the dense_vector field type for the title_vector and  content_vector fields. This is a special field type that allows us to store dense vectors in Elasticsearch.\n\nLater, we'll need to target the dense_vector field for kNN search.\n\nIndex data into Elasticsearch\n\nThe following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\n\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\nAs the dataframe is large, we will index data in batches of 100. We index the data into Elasticsearch using the Python client's helpers for the bulk API.\nLet's test the index with a simple match query.\nEncode a question with OpenAI embedding model\n\nTo perform kNN search, we need to encode queries with the same embedding model used to encode the documents at index time.\nIn this example, we need to use the text-embedding-3-small model.\n\nYou'll need your OpenAI API key to generate the embeddings.\nRun semantic search queries\n\nNow we're ready to run queries against our Elasticsearch index using our encoded question. We'll be doing a k-nearest neighbors search, using the Elasticsearch kNN query option.\n\nFirst, we define a small function to pretty print the results.\nNow let's run our kNN query.\nSuccess! We've used kNN to perform semantic search over our dataset and found the top results.\n\nNow we can use the Chat Completions API to work some generative AI magic using the top search result as additional context.\nUse Chat Completions API for retrieval augmented generation\n\nNow we can send the question and the text to OpenAI's chat completion API.\n\nUsing a LLM model together with a retrieval model is known as retrieval augmented generation (RAG). We're using Elasticsearch to do what it does best, retrieve relevant documents. Then we use the LLM to do what it does best, tasks like generating summaries and answering questions, using the retrieved documents as context.\n\nThe model will generate a response to the question, using the top kNN hit as context. Use the messages list to shape your prompt to the model. In this example, we're using the gpt-3.5-turbo model.\n\nCode explanation\n\nHere's what that code does:\n\nUses OpenAI's model to generate a response\nSends a conversation containing a system message and a user message to the model\nThe system message sets the assistant's role as \"helpful assistant\"\nThe user message contains a question as specified in the original kNN query and some input text\nThe response from the model is stored in the summary.choices variable\nNext steps\n\nThat was just one example of how to combine Elasticsearch with the power of OpenAI's models, to enable retrieval augmented generation. RAG allows you to avoid the costly and complex process of training or fine-tuning models, by leveraging out-of-the-box models, enhanced with additional context.\n\nUse this as a blueprint for your own experiments.\n\nTo adapt the conversation for different use cases, customize the system message to define the assistant's behavior or persona. Adjust the user message to specify the task, such as summarization or question answering, along with the desired format of the response."])</script><script>self.__next_f.push([1,"54:Tc35,"])</script><script>self.__next_f.push([1,"Open In Colab\n\nThis notebook demonstrates how to:\nIndex the OpenAI Wikipedia vector dataset into Elasticsearch\nEmbed a question with the OpenAI embeddings endpoint\nPerform semantic search on the Elasticsearch index using the encoded question\nInstall packages and import modules\nConnect to Elasticsearch\n\nℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook.\nIf you don't already have an Elastic deployment, you can sign up for a free Elastic Cloud trial.\n\nTo connect to Elasticsearch, you need to create a client instance with the Cloud ID and password for your deployment.\n\nFind the Cloud ID for your deployment by going to https://cloud.elastic.co/deployments and selecting your deployment.\nDownload the dataset\n\nIn this step we download the OpenAI Wikipedia embeddings dataset, and extract the zip file.\nRead CSV file into a Pandas DataFrame\n\nNext we use the Pandas library to read the unzipped CSV file into a DataFrame. This step makes it easier to index the data into Elasticsearch in bulk.\nCreate index with mapping\n\nNow we need to create an Elasticsearch index with the necessary mappings. This will enable us to index the data into Elasticsearch.\n\nWe use the dense_vector field type for the title_vector and  content_vector fields. This is a special field type that allows us to store dense vectors in Elasticsearch.\n\nLater, we'll need to target the dense_vector field for kNN search.\n\nIndex data into Elasticsearch\n\nThe following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\n\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\nAs the dataframe is large, we will index data in batches of 100. We index the data into Elasticsearch using the Python client's helpers for the bulk API.\nLet's test the index with a simple match query.\nEncode a question with OpenAI embedding model\n\nTo perform semantic search, we need to encode queries with the same embedding model used to encode the documents at index time.\nIn this example, we need to use the text-embedding-3-small model.\n\nYou'll need your OpenAI API key to generate the embeddings.\nRun semantic search queries\n\nNow we're ready to run queries against our Elasticsearch index using our encoded question. We'll be doing a k-nearest neighbors search, using the Elasticsearch kNN query option.\n\nFirst, we define a small function to pretty print the results.\nNow let's run our kNN query.\nNext steps\n\nSuccess! Now you know how to use Elasticsearch as a vector database to store embeddings, encode queries by calling the OpenAI embeddings endpoint, and run semantic search.\n\nPlay around with different queries, and if you want to try with your own data, you can experiment with different embedding models.\n\nℹ️ Check out our other notebook Retrieval augmented generation using Elasticsearch and OpenAI. That notebook builds on this example to demonstrate how to use Elasticsearch together with the OpenAI chat completions API for retrieval augmented generation (RAG)."])</script><script>self.__next_f.push([1,"55:T110a,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step by step on using Hologres as a vector database for OpenAI embeddings.\n\nThis notebook presents an end-to-end process of:\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in a cloud instance of Hologres.\nConverting raw text query to an embedding with OpenAI API.\nUsing Hologres to perform the nearest neighbour search in the created collection.\nProvide large language models with the search results as context in prompt engineering\n\nWhat is Hologres\n\nHologres is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time. Hologres supports standard SQL syntax, is compatible with PostgreSQL, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services. Hologres supports fine-grained isolation of multiple workloads and enterprise-level security capabilities. Hologres is deeply integrated with MaxCompute, Realtime Compute for Apache Flink, and DataWorks, and provides full-stack online and offline data warehousing solutions for enterprises.\n\nHologres provides vector database functionality by adopting Proxima.\n\nProxima is a high-performance software library developed by Alibaba DAMO Academy. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open source software such as Facebook AI Similarity Search (Faiss). Proxima provides basic modules that have leading performance and effects in the industry and allows you to search for similar images, videos, or human faces. Hologres is deeply integrated with Proxima to provide a high-performance vector search service.\n\nDeployment options\n\nClick here to fast deploy Hologres data warehouse.\n\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nHologres cloud server instance.\nThe 'psycopg2-binary' library to interact with the vector database. Any other postgresql client library is ok.\nAn OpenAI API key.\n\n\nWe might validate if the server was launched successfully by running a simple curl command:\n\nInstall requirements\n\nThis notebook obviously requires the openai and psycopg2-binary packages, but there are also some other additional libraries we will use. The following command installs them all:\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to Hologres\nFirst add it to your environment variables. or you can just change the \"psycopg2.connect\" parameters below\n\nConnecting to a running instance of Hologres server is easy with the official Python library:\nWe can test the connection by running any available method:\nThe downloaded file has to be then extracted:\nLoad data\n\nIn this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\nTake a look at the data.\nCreate table and proxima vector index\nHologres stores data in tables where each object is described by at least one vector. Our table will be called articles and each object will be described by both title and content vectors.\n\nWe will start with creating a table and create proxima indexes on both title and content, and then we will fill it with our precomputed embeddings.\nUpload data\nNow let's upload the data to the Hologres cloud instance using COPY statement. This might take 5-10 minutes according to the network bandwidth.\nThe proxima index will be built in the background. We can do searching during this period but the query will be slow without the vector index. Use this command to wait for finish building the index.\nSearch data\n\nOnce the data is uploaded we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search. Since the precomputed embeddings were created with text-embedding-3-small OpenAI model we also have to use it during search."])</script><script>self.__next_f.push([1,"56:T7b3,"])</script><script>self.__next_f.push([1,"This Notebook provides step by step instuctions on using Azure Data Explorer (Kusto) as a vector database with OpenAI embeddings.\nThis notebook presents an end-to-end process of:\n\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in Kusto.\nConverting raw text query to an embedding with OpenAI API.\nUsing Kusto to perform cosine similarity search in the stored embeddings\n\nPrerequisites\nFor the purposes of this exercise we need to prepare a couple of things:\n\nAzure Data Explorer(Kusto) server instance. https://azure.microsoft.com/en-us/products/data-explorer\nAzure OpenAI credentials or OpenAI API key.\nDownload precomputed Embeddings\n\n\nIn this section we are going to load prepared embedding data, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\n\nStore vectors in a Kusto table\n\nCreate a table \u0026 load the vectors in Kusto based on the contents in the dataframe. The spark option CreakeIfNotExists will automatically create a table if it doesn't exist\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries. You can follow the instructions to create and retrieve your Azure OpenAI key and endpoint. https://learn.microsoft.com/en-us/azure/cognitive-services/openai/tutorials/embeddings\n\n\nPlease make sure to use the text-embedding-3-small model. Since the precomputed embeddings were created with text-embedding-3-small model we also have to use it during search.\n\nIf using Azure Open AI\nIf using Open AI\nOnly run this cell if you plan to use Open AI for embedding\nGenerate embedding for the search term\nSemantic search in Kusto\nWe will search the Kusto table for the closest vectors.\n\nWe will be using the series-cosine-similarity-fl UDF for similarity search.\n\nPlease create the function in your database before proceeding -\nhttps://learn.microsoft.com/en-us/azure/data-explorer/kusto/functions-library/series-cosine-similarity-fl?tabs=query-defined"])</script><script>self.__next_f.push([1,"57:T48e,"])</script><script>self.__next_f.push([1,"\n\n\nAzure Data Explorer aka Kusto is a cloud-based data analytics service that enables users to perform advanced analytics on large datasets in real-time. It is particularly well-suited for handling large volumes of data, making it an excellent choice for storing and searching vectors.\n\nKusto supports a special data type called dynamic, which can store unstructured data such as arrays and properties bag. Dynamic data type is perfect for storing vector values. You can further augment the vector value by storing metadata related to the original object as separate columns in your table.\nKusto also supports in-built function series_cosine_similarity_fl to perform vector similarity searches.\n\nGet started with Kusto for free.\n\nKusto_Vector\n\nGetting started with Kusto and Open AI embedding\n\nDemo Scenario\n\nWiki_embeddings\n\nsemantic_search_flow\n\nIf you’d like to try this demo, please follow the instructions in the Notebook.\n\nIt will allow you to -\n\nUse precomputed embeddings created by OpenAI API.\n\nStore the embeddings in Kusto.\n\nConvert raw text query to an embedding with OpenAI API.\n\nUse Kusto to perform cosine similarity search in the stored embeddings."])</script><script>self.__next_f.push([1,"58:Ta64,"])</script><script>self.__next_f.push([1,"Finding your next movie\n\nIn this notebook we will be going over generating embeddings of movie descriptions with OpenAI and using those embeddings within Milvus to find relevant movies. To narrow our search results and try something new, we are going to be using filtering to do metadata searches. The dataset in this example is sourced from HuggingFace datasets, and contains a little over 8 thousand movie entries.\n\nLets begin by first downloading the required libraries for this notebook:\nopenai is used for communicating with the OpenAI embedding service\npymilvus is used for communicating with the Milvus server\ndatasets is used for downloading the dataset\ntqdm is used for the progress bars\n\nWith the required packages installed we can get started. Lets begin by launching the Milvus service. The file being run is the docker-compose.yaml found in the folder of this file. This command launches a Milvus standalone instance which we will use for this test.\nWith Milvus running we can setup our global variables:\nHOST: The Milvus host address\nPORT: The Milvus port number\nCOLLECTION_NAME: What to name the collection within Milvus\nDIMENSION: The dimension of the embeddings\nOPENAI_ENGINE: Which embedding model to use\nopenai.api_key: Your OpenAI account key\nINDEX_PARAM: The index settings to use for the collection\nQUERY_PARAM: The search parameters to use\nBATCH_SIZE: How many movies to embed and insert at once\nDataset\nWith Milvus up and running we can begin grabbing our data. Hugging Face Datasets is a hub that holds many different user datasets, and for this example we are using HuggingLearners's netflix-shows dataset. This dataset contains movies and their metadata pairs for over 8 thousand movies. We are going to embed each description and store it within Milvus along with its title, type, release_year and rating.\nInsert the Data\nNow that we have our data on our machine we can begin embedding it and inserting it into Milvus. The embedding function takes in text and returns the embeddings in a list format.\nThis next step does the actual inserting. We iterate through all the entries and create batches that we insert once we hit our set batch size. After the loop is over we insert the last remaning batch if it exists.\nQuery the Database\nWith our data safely inserted in Milvus, we can now perform a query. The query takes in a tuple of the movie description you are searching for an the filter to use. More info about the filter can be found here. The search first prints out your description and filter expression. After that for each result we print the score, title, type, release year, rating, and description of the result movies."])</script><script>self.__next_f.push([1,"59:Ta39,"])</script><script>self.__next_f.push([1,"Finding your next book\n\nIn this notebook we will be going over generating embeddings of book descriptions with OpenAI and using those embeddings within Milvus to find relevant books. The dataset in this example is sourced from HuggingFace datasets, and contains a little over 1 million title-description pairs.\n\nLets begin by first downloading the required libraries for this notebook:\nopenai is used for communicating with the OpenAI embedding service\npymilvus is used for communicating with the Milvus server\ndatasets is used for downloading the dataset\ntqdm is used for the progress bars\n\nWith the required packages installed we can get started. Lets begin by launching the Milvus service. The file being run is the docker-compose.yaml found in the folder of this file. This command launches a Milvus standalone instance which we will use for this test.\nWith Milvus running we can setup our global variables:\nHOST: The Milvus host address\nPORT: The Milvus port number\nCOLLECTION_NAME: What to name the collection within Milvus\nDIMENSION: The dimension of the embeddings\nOPENAI_ENGINE: Which embedding model to use\nopenai.api_key: Your OpenAI account key\nINDEX_PARAM: The index settings to use for the collection\nQUERY_PARAM: The search parameters to use\nBATCH_SIZE: How many texts to embed and insert at once\nMilvus\nThis segment deals with Milvus and setting up the database for this use case. Within Milvus we need to setup a collection and index the collection.\nDataset\nWith Milvus up and running we can begin grabbing our data. Hugging Face Datasets is a hub that holds many different user datasets, and for this example we are using Skelebor's book dataset. This dataset contains title-description pairs for over 1 million books. We are going to embed each description and store it within Milvus along with its title.\nInsert the Data\nNow that we have our data on our machine we can begin embedding it and inserting it into Milvus. The embedding function takes in text and returns the embeddings in a list format.\nThis next step does the actual inserting. Due to having so many datapoints, if you want to immidiately test it out you can stop the inserting cell block early and move along. Doing this will probably decrease the accuracy of the results due to less datapoints, but it should still be good enough.\nQuery the Database\nWith our data safely inserted in Milvus, we can now perform a query. The query takes in a string or a list of strings and searches them. The resuts print out your provided description and the results that include the result score, the result title, and the result book description."])</script><script>self.__next_f.push([1,"5a:Ta1c,"])</script><script>self.__next_f.push([1,"\nThis notebook provides a step-by-step guide on using MyScale as a vector database for OpenAI embeddings. The process includes:\n\nUtilizing precomputed embeddings generated by OpenAI API.\nStoring these embeddings in a cloud instance of MyScale.\nConverting raw text query to an embedding using OpenAI API.\nLeveraging MyScale to perform nearest neighbor search within the created collection.\n\nWhat is MyScale\n\nMyScale is a database built on Clickhouse that combines vector search and SQL analytics to offer a high-performance, streamlined, and fully managed experience. It's designed to facilitate joint queries and analyses on both structured and vector data, with comprehensive SQL support for all data processing.\n\nDeployment options\n\nDeploy and execute vector search with SQL on your cluster within two minutes by using MyScale Console.\n\nPrerequisites\n\nTo follow this guide, you will need to have the following:\n\nA MyScale cluster deployed by following the quickstart guide.\nThe 'clickhouse-connect' library to interact with MyScale.\nAn OpenAI API key for vectorization of queries.\nInstall requirements\n\nThis notebook requires the openai, clickhouse-connect, as well as some other dependencies. Use the following command to install them:\nPrepare your OpenAI API key\n\nTo use the OpenAI API, you'll need to set up an API key. If you don't have one already, you can obtain it from OpenAI.\nConnect to MyScale\n\nFollow the connections details section to retrieve the cluster host, username, and password information from the MyScale console, and use it to create a connection to your cluster as shown below:\nLoad data\nWe need to load the dataset of precomputed vector embeddings for Wikipedia articles provided by OpenAI. Use the wget package to download the dataset.\nAfter the download is complete, extract the file using the zipfile package:\nNow, we can load the data from vector_database_wikipedia_articles_embedded.csv into a Pandas DataFrame:\nIndex data\n\nWe will create an SQL table called articles in MyScale to store the embeddings data. The table will include a vector index with a cosine distance metric and a constraint for the length of the embeddings. Use the following code to create and insert data into the articles table:\nWe need to check the build status of the vector index before proceeding with the search, as it is automatically built in the background.\nSearch data\n\nOnce indexed in MyScale, we can perform vector search to find similar content. First, we will use the OpenAI API to generate embeddings for our query. Then, we will perform the vector search using MyScale."])</script><script>self.__next_f.push([1,"5b:Tdf1,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nMyScale**\n    Setup: Set up the MyScale Python client. For more details go here\n    Index Data: We'll create a table and index it for content.\n    Search Data: Run a few example queries with various goals in mind.\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nMyScale\nThe next vector database we'll consider is MyScale.\n\nMyScale is a database built on Clickhouse that combines vector search and SQL analytics to offer a high-performance, streamlined, and fully managed experience. It's designed to facilitate joint queries and analyses on both structured and vector data, with comprehensive SQL support for all data processing.\n\nDeploy and execute vector search with SQL on your cluster within two minutes by using MyScale Console.\nConnect to MyScale\n\nFollow the connections details section to retrieve the cluster host, username, and password information from the MyScale console, and use it to create a connection to your cluster as shown below:\nIndex data\n\nWe will create an SQL table called articles in MyScale to store the embeddings data. The table will include a vector index with a cosine distance metric and a constraint for the length of the embeddings. Use the following code to create and insert data into the articles table:\nWe need to check the build status of the vector index before proceeding with the search, as it is automatically built in the background.\nSearch data\n\nOnce indexed in MyScale, we can perform vector search to find similar content. First, we will use the OpenAI API to generate embeddings for our query. Then, we will perform the vector search using MyScale."])</script><script>self.__next_f.push([1,"5c:Tcd1,"])</script><script>self.__next_f.push([1,"\nFixing LLMs that Hallucinate\n\nIn this notebook we will learn how to query relevant contexts to our queries from Pinecone, and pass these to a GPT-4 model to generate an answer backed by real data sources.\n\nGPT-4 is a big step up from previous OpenAI completion models. It also exclusively uses the ChatCompletion endpoint, so we must use it in a slightly different way to usual. However, the power of the model makes the change worthwhile, particularly when augmented with an external knowledge base like the Pinecone vector database.\n\nRequired installs for this notebook are:\nPreparing the Data\nIn this example, we will download the LangChain docs from langchain.readthedocs.io/. We get all .html files located on the site like so:\nThis downloads all HTML into the rtdocs directory. Now we can use LangChain itself to process these docs. We do this using the ReadTheDocsLoader like so:\nThis leaves us with hundreds of processed doc pages. Let's take a look at the format each one contains:\nWe access the plaintext page content like so:\nWe can also find the source of each document:\nWe can use these to create our data list:\nIt's pretty ugly but it's good enough for now. Let's see how we can process all of these. We will chunk everything into ~400 token chunks, we can do this easily with langchain and tiktoken:\nProcess the data into more chunks using this approach.\nOur chunks are ready so now we move onto embedding and indexing everything.\nInitialize Embedding Model\n\nWe use text-embedding-3-small as the embedding model. We can embed text like so:\nIn the response res we will find a JSON-like object containing our new embeddings within the 'data' field.\nInside 'data' we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains 1536 dimensions (the output dimensionality of the text-embedding-3-small model.\nWe will apply this same embedding logic to the langchain docs dataset we've just scraped. But before doing so we must create a place to store the embeddings.\nInitializing the Index\nNow we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a free API key and enter it below where we will initialize our connection to Pinecone and create a new index.\nWe can see the index is currently empty with a total_vector_count of 0. We can begin populating it with OpenAI text-embedding-3-small built embeddings like so:\nNow we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4.\nRetrieval\nTo search through our documents we first need to create a query vector xq. Using xq we will retrieve the most relevant chunks from the LangChain docs, like so:\nWith retrieval complete, we move on to feeding these into GPT-4 to produce answers.\nRetrieval Augmented Generation\nGPT-4 is currently accessed via the ChatCompletions endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts alongside our original query. We can do that like so:\nNow we ask the question:\nTo display this response nicely, we will display it in markdown.\nLet's compare this to a non-augmented query...\nIf we drop the \"I don't know\" part of the primer?"])</script><script>self.__next_f.push([1,"5d:T125b,"])</script><script>self.__next_f.push([1,"\nFixing LLMs that Hallucinate\n\nIn this notebook we will learn how to query relevant contexts to our queries from Pinecone, and pass these to a generative OpenAI model to generate an answer backed by real data sources.\n\nA common problem with using GPT-3 to factually answer questions is that GPT-3 can sometimes make things up. The GPT models have a broad range of general knowledge, but this does not necessarily apply to more specific information. For that we use the Pinecone vector database as our \"external knowledge base\" — like long-term memory for GPT-3.\n\nRequired installs for this notebook are:\nFor many questions state-of-the-art (SOTA) LLMs are more than capable of answering correctly.\nHowever, that isn't always the case. First let's first rewrite the above into a simple function so we're not rewriting this every time.\nNow let's ask a more specific question about training a type of transformer model called a sentence transformer. The ideal answer we'd be looking for is \"Multiple Negatives Ranking (MNR) loss\".\n\nDon't worry if this is a new term to you, it isn't required to understand what we're doing or demoing here.\nOne of the common answers we get to this is:\n\nThe best training method to use for fine-tuning a pre-trained model with sentence transformers is the Masked Language Model (MLM) training. MLM training involves randomly masking some of the words in a sentence and then training the model to predict the masked words. This helps the model to learn the context of the sentence and better understand the relationships between words.\n\nThis answer seems pretty convincing right? Yet, it's wrong. MLM is typically used in the pretraining step of a transformer model but \"cannot\" be used to fine-tune a sentence-transformer, and has nothing to do with having \"pairs of related sentences\".\n\nAn alternative answer we receive (and the one we returned above) is about supervised learning approach being the most suitable. This is completely true, but it's not specific and doesn't answer the question.\n\nWe have two options for enabling our LLM in understanding and correctly answering this question:\n\nWe fine-tune the LLM on text data covering the topic mentioned, likely on articles and papers talking about sentence transformers, semantic search training methods, etc.\n\nWe use Retrieval Augmented Generation (RAG), a technique that implements an information retrieval component to the generation process. Allowing us to retrieve relevant information and feed this information into the generation model as a secondary source of information.\n\nWe will demonstrate option 2.\nBuilding a Knowledge Base\n\nWith option 2 the retrieval of relevant information requires an external \"Knowledge Base\", a place where we can store and use to efficiently retrieve information. We can think of this as the external long-term memory of our LLM.\n\nWe will need to retrieve information that is semantically related to our queries, to do this we need to use \"dense vector embeddings\". These can be thought of as numerical representations of the meaning behind our sentences.\n\nTo create these dense vectors we use the text-embedding-3-small model.\n\nWe have already authenticated our OpenAI connection, to create an embedding we just do:\nIn the response res we will find a JSON-like object containing our new embeddings within the 'data' field.\nInside 'data' we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains 1536 dimensions (the output dimensionality of the text-embedding-3-small model.\nWe will apply this same embedding logic to a dataset containing information relevant to our query (and many other queries on the topics of ML and AI).\n\nData Preparation\n\nThe dataset we will be using is the jamescalam/youtube-transcriptions from Hugging Face Datasets. It contains transcribed audio from several ML and tech YouTube channels. We download it with:\nThe dataset contains many small snippets of text data. We will need to merge many snippets from each video to create more substantial chunks of text that contain more information.\nNow we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a free API key and enter it below where we will initialize our connection to Pinecone and create a new index.\nWe can see the index is currently empty with a total_vector_count of 0. We can begin populating it with OpenAI text-embedding-3-small built embeddings like so:\nNow we search, for this we need to create a query vector xq:\nAnd we get a pretty great answer straight away, specifying to use multiple-rankings loss (also called multiple negatives ranking loss)."])</script><script>self.__next_f.push([1,"5e:T580,"])</script><script>self.__next_f.push([1,"\nVector search is an innovative technology that enables developers and engineers to efficiently store, search, and recommend information by representing complex data as mathematical vectors. By comparing the similarities between these vectors, you can quickly retrieve relevant information in a seamless and intuitive manner.\n\nPinecone is a vector database designed with developers and engineers in mind. As a managed service, it alleviates the burden of maintenance and engineering, allowing you to focus on extracting valuable insights from your data. The free tier supports up to 5 million vectors, making it an accessible and cost-effective way to experiment with vector search capabilities. With Pinecone, you'll experience impressive speed, accuracy, and scalability, as well as access to advanced features like single-stage metadata filtering and the cutting-edge sparse-dense index.\n\nExamples\n\nThis folder contains examples of using Pinecone and OpenAI together. More will be added over time so check back for updates!\n\n| Name | Description | Google Colab |\n| --- | --- | --- |\n| GPT-4 Retrieval Augmentation | How to supercharge GPT-4 with retrieval augmentation | Open In Colab |\n| Generative Question-Answering | A simple walkthrough demonstrating the use of Generative Question-Answering | Open In Colab |\n| Semantic Search | A guide to building a simple semantic search process | Open In Colab |"])</script><script>self.__next_f.push([1,"5f:Taa6,"])</script><script>self.__next_f.push([1,"In this guide you will learn how to use the OpenAI Embedding API to generate language embeddings, and then index those embeddings in the Pinecone vector database for fast and scalable vector search.\n\nThis is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.\n\nThe basic workflow looks like this:\n\nEmbed and index\n\nUse the OpenAI Embedding API to generate vector embeddings of your documents (or any text data).\nUpload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies.\n\nSearch\n\nPass your query text or document through the OpenAI Embedding API again.\nTake the resulting vector embedding and send it as a query to Pinecone.\nGet back semantically similar documents, even if they don't share any keywords with the query.\n\nArchitecture overview\n\nLet's get started...\nSetup\nWe first need to setup our environment and retrieve API keys for OpenAI and Pinecone. Let's start with our environment, we need HuggingFace Datasets for our data, and the OpenAI and Pinecone clients:\nCreating Embeddings\n\nThen we initialize our connection to OpenAI Embeddings and Pinecone vector DB. Sign up for an API key over at OpenAI and Pinecone.\nWe can now create embeddings with the OpenAI Ada similarity model like so:\nNext, we initialize our index to store vector embeddings with Pinecone.\nInitialize connection to Pinecone, you can get a free API key in the Pinecone dashboard.\n\nPopulating the Index\n\nNow we will take 1K questions from the TREC dataset\nThen we create a vector embedding for each phrase using OpenAI, and upsert the ID, vector embedding, and original text for each phrase to Pinecone.\nQuerying\n\nWith our data indexed, we're now ready to move onto performing searches. This follows a similar process to indexing. We start with a text query, that we would like to use to find similar sentences. As before we encode this with OpenAI's text similarity Babbage model to create a query vector xq. We then use xq to query the Pinecone index.\nNow query...\nThe response from Pinecone includes our original text in the metadata field, let's print out the top_k most similar questions and their respective similarity scores.\nLooks good, let's make it harder and replace \"depression\" with the incorrect term \"recession\".\nAnd again...\nLooks great, our semantic search pipeline is clearly able to identify the meaning between each of our queries and return the most semantically similar questions from the already indexed questions.\n\nOnce we're finished with the index we delete it to save resources.\n"])</script><script>self.__next_f.push([1,"60:Tcea,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nPinecone**\n    Setup: Here we'll set up the Python client for Pinecone. For more details go here\n    Index Data: We'll create an index with namespaces for titles and content\n    Search Data: We'll test out both namespaces with search queries to confirm it works\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared in this article.\nPinecone\n\nThe next option we'll look at is Pinecone, a managed vector database which offers a cloud-native option.\n\nBefore you proceed with this step you'll need to navigate to Pinecone, sign up and then save your API key as an environment variable titled\nFor section we will:\nCreate an index with multiple namespaces for article titles and content\nStore our data in the index with separate searchable \"namespaces\" for article titles and content\nFire some similarity search queries to verify our setup is working\nCreate Index\n\nFirst we will need to create an index, which we'll call wikipedia-articles. Once we have an index, we can create multiple namespaces, which can make a single index searchable for various use cases. For more details, consult Pinecone documentation.\n\nIf you want to batch insert to your index in parallel to increase insertion speed then there is a great guide in the Pinecone documentation on batch inserts in parallel.\nSearch data\n\nNow we'll enter some dummy searches and check we get decent results back"])</script><script>self.__next_f.push([1,"61:Tf29,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step by step on using Qdrant as a vector database for OpenAI embeddings. Qdrant is a high-performant vector search database written in Rust. It offers RESTful and gRPC APIs to manage your embeddings. There is an official Python qdrant-client that eases the integration with your apps.\n\nThis notebook presents an end-to-end process of:\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in a local instance of Qdrant.\nConverting raw text query to an embedding with OpenAI API.\nUsing Qdrant to perform the nearest neighbour search in the created collection.\n\nWhat is Qdrant\n\nQdrant is an Open Source vector database that allows storing neural embeddings along with the metadata, a.k.a payload. Payloads are not only available for keeping some additional attributes of a particular point, but might be also used for filtering. Qdrant offers a unique filtering mechanism which is built-in into the vector search phase, what makes it really efficient.\n\nDeployment options\n\nQdrant might be launched in various ways, depending on the target load on the application it might be hosted:\n\nLocally or on premise, with Docker containers\nOn Kubernetes cluster, with the Helm chart\nUsing Qdrant Cloud\n\nIntegration\n\nQdrant provides both RESTful and gRPC APIs which makes integration easy, no matter the programming language you use. However, there are some official clients for the most popular languages available, and if you use Python then the Python Qdrant client library might be the best choice.\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nQdrant server instance. In our case a local Docker container.\nThe qdrant-client library to interact with the vector database.\nAn OpenAI API key.\n\nStart Qdrant server\n\nWe're going to use a local Qdrant instance running in a Docker container. The easiest way to launch it is to use the attached [docker-compose.yaml] file and run the following command:\nWe might validate if the server was launched successfully by running a simple curl command:\nInstall requirements\n\nThis notebook obviously requires the openai and qdrant-client packages, but there are also some other additional libraries we will use. The following command installs them all:\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY by running following command:\nConnect to Qdrant\n\nConnecting to a running instance of Qdrant server is easy with the official Python library:\nWe can test the connection by running any available method:\nLoad data\n\nIn this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\nThe downloaded file has to be then extracted:\nAnd we can finally load it from the provided CSV file:\nIndex data\n\nQdrant stores data in collections where each object is described by at least one vector and may contain an additional metadata called payload. Our collection will be called Articles and each object will be described by both title and content vectors. Qdrant does not require you to set up any kind of schema beforehand, so you can freely put points to the collection with a simple setup only.\n\nWe will start with creating a collection, and then we will fill it with our precomputed embeddings.\nSearch data\n\nOnce the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search. Since the precomputed embeddings were created with text-embedding-3-small OpenAI model we also have to use it during search."])</script><script>self.__next_f.push([1,"62:Td62,"])</script><script>self.__next_f.push([1,"\nThis notebook presents how to implement a Question Answering system with Langchain, Qdrant as a knowledge based and OpenAI embeddings. If you are not familiar with Qdrant, it's better to check out the Getting_started_with_Qdrant_and_OpenAI.ipynb notebook.\n\nThis notebook presents an end-to-end process of:\nCalculating the embeddings with OpenAI API.\nStoring the embeddings in a local instance of Qdrant to build a knowledge base.\nConverting raw text query to an embedding with OpenAI API.\nUsing Qdrant to perform the nearest neighbour search in the created collection to find some context.\nAsking LLM to find the answer in a given context.\n\nAll the steps will be simplified to calling some corresponding Langchain methods.\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nQdrant server instance. In our case a local Docker container.\nThe qdrant-client library to interact with the vector database.\nLangchain as a framework.\nAn OpenAI API key.\n\nStart Qdrant server\n\nWe're going to use a local Qdrant instance running in a Docker container. The easiest way to launch it is to use the attached [docker-compose.yaml] file and run the following command:\nWe might validate if the server was launched successfully by running a simple curl command:\nInstall requirements\n\nThis notebook obviously requires the openai, langchain and qdrant-client packages.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY by running following command:\nLoad data\n\nIn this section we are going to load the data containing some natural questions and answers to them. All the data will be used to create a Langchain application with Qdrant being the knowledge base.\nChain definition\n\nLangchain is already integrated with Qdrant and performs all the indexing for given list of documents. In our case we are going to store the set of answers we have.\nAt this stage all the possible answers are already stored in Qdrant, so we can define the whole QA chain.\nSearch data\n\nOnce the data is put into Qdrant we can start asking some questions. A question will be automatically vectorized by OpenAI model, and the created vector will be used to find some possibly matching answers in Qdrant. Once retrieved, the most similar answers will be incorporated into the prompt sent to OpenAI Large Language Model. The communication between all the services is shown on a graph:\n\nCustom prompt templates\n\nThe stuff chain type in Langchain uses a specific prompt with question and context documents incorporated. This is what the default prompt looks like:\n\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n{context}\nQuestion: {question}\nHelpful Answer:\n\nWe can, however, provide our prompt template and change the behaviour of the OpenAI LLM, while still using the stuff chain type. It is important to keep {context} and {question} as placeholders.\n\nExperimenting with custom prompts\n\nWe can try using a different prompt template, so the model:\nResponds with a single-sentence answer if it knows it.\nSuggests a random song title if it doesn't know the answer to our question."])</script><script>self.__next_f.push([1,"63:T1049,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nQdrant**\n    Setup: Here we'll set up the Python client for Qdrant. For more details go here\n    Index Data: We'll create a collection with vectors for titles and content\n    Search Data: We'll run a few searches to confirm it works\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nQdrant\n\nQdrant. is a high-performant vector search database written in Rust. It offers both on-premise and cloud version, but for the purposes of that example we're going to use the local deployment mode.\n\nSetting everything up will require:\nSpinning up a local instance of Qdrant\nConfiguring the collection and storing the data in it\nTrying out with some queries\nSetup\n\nFor the local deployment, we are going to use Docker, according to the Qdrant documentation: https://qdrant.tech/documentation/quick_start/. Qdrant requires just a single container, but an example of the docker-compose.yaml file is available at ./qdrant/docker-compose.yaml in this repo.\n\nYou can start Qdrant instance locally by navigating to this directory and running docker-compose up -d \nYou might need to increase the memory limit for Docker to 8GB or more. Or Qdrant might fail to execute with an error message like 7 Killed.\n\nIndex data\n\nQdrant stores data in collections where each object is described by at least one vector and may contain an additional metadata called payload. Our collection will be called Articles and each object will be described by both title and content vectors.\n\nWe'll be using an official qdrant-client package that has all the utility methods already built-in.\nIn addition to the vector configuration defined under vector, we can also define the payload configuration. Payload is an optional field that allows you to store additional metadata alongside the vectors. In our case, we'll store the id, title, and url of the articles. As we return the title of nearest articles in the search results from payload, we can also provide the user with the URL to the article (which is part of the meta-data).\nSearch Data\n\nOnce the data is put into Qdrant we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search."])</script><script>self.__next_f.push([1,"64:T1ace,"])</script><script>self.__next_f.push([1,"\nRedis\n\nWhat is Redis?\n\nMost developers from a web services background are probably familiar with Redis. At it's core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choice Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years.\n\nIn addition to the traditional uses of Redis. Redis also provides Redis Modules which are a way to extend Redis with new capabilities, commands and data types. Example modules include RedisJSON, RedisTimeSeries, RedisBloom and RediSearch.\n\nDeployment options\n\nThere are a number of ways to deploy Redis. For local development, the quickest method is to use the Redis Stack docker container which we will use here. Redis Stack contains a number of Redis modules that can be used together to create a fast, multi-model data store and query engine.\n\nFor production use cases, The easiest way to get started is to use the Redis Cloud service. Redis Cloud is a fully managed Redis service. You can also deploy Redis on your own infrastructure using Redis Enterprise. Redis Enterprise is a fully managed Redis service that can be deployed in kubernetes, on-premises or in the cloud.\n\nAdditionally, every major cloud provider (AWS Marketplace, Google Marketplace, or Azure Marketplace) offers Redis Enterprise in a marketplace offering.\n\nWhat is RediSearch?\n\nRediSearch is a Redis module that provides querying, secondary indexing, full-text search and vector search for Redis. To use RediSearch, you first declare indexes on your Redis data. You can then use the RediSearch clients to query that data. For more information on the feature set of RediSearch, see the RediSearch documentation.\n\nFeatures\n\nRediSearch uses compressed, inverted indexes for fast indexing with a low memory footprint. RediSearch indexes enhance Redis by providing exact-phrase matching, fuzzy search, and numeric filtering, among many other features. Such as:\n\nFull-Text indexing of multiple fields in Redis hashes\nIncremental indexing without performance loss\nVector similarity search\nDocument ranking (using tf-idf, with optional user-provided weights)\nField weighting\nComplex boolean queries with AND, OR, and NOT operators\nPrefix matching, fuzzy matching, and exact-phrase queries\nSupport for double-metaphone phonetic matching\nAuto-complete suggestions (with fuzzy prefix suggestions)\nStemming-based query expansion in many languages (using Snowball)\nSupport for Chinese-language tokenization and querying (using Friso)\nNumeric filters and ranges\nGeospatial searches using Redis geospatial indexing\nA powerful aggregations engine\nSupports for all utf-8 encoded text\nRetrieve full documents, selected fields, or only the document IDs\nSorting results (for example, by creation date)\nJSON support through RedisJSON\n\nClients\n\nGiven the large ecosystem around Redis, there are most likely client libraries in the language you need. You can use any standard Redis client library to run RediSearch commands, but it's easiest to use a library that wraps the RediSearch API. Below are a few examples, but you can find more client libraries here.\n\n| Project | Language | License | Author | Stars |\n|----------|---------|--------|---------|-------|\n| jedis | Java | MIT | Redis | Stars |\n| redis-py | Python | MIT | Redis | Stars |\n| node-redis | Node.js | MIT | Redis | Stars |\n| nredisstack | .NET | MIT | Redis | Stars |\n\n[redis-url]: https://redis.com\n\n[redis-py-url]: https://github.com/redis/redis-py\n[redis-py-stars]: https://img.shields.io/github/stars/redis/redis-py.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[redis-py-package]: https://pypi.python.org/pypi/redis\n\n[jedis-url]: https://github.com/redis/jedis\n[jedis-stars]: https://img.shields.io/github/stars/redis/jedis.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[Jedis-package]: https://search.maven.org/artifact/redis.clients/jedis\n\n[nredisstack-url]: https://github.com/redis/nredisstack\n[nredisstack-stars]: https://img.shields.io/github/stars/redis/nredisstack.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[nredisstack-package]: https://www.nuget.org/packages/nredisstack/\n\n[node-redis-url]: https://github.com/redis/node-redis\n[node-redis-stars]: https://img.shields.io/github/stars/redis/node-redis.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[node-redis-package]: https://www.npmjs.com/package/redis\n\n[redis-om-python-url]: https://github.com/redis/redis-om-python\n[redis-om-python-author]: https://redis.com\n[redis-om-python-stars]: https://img.shields.io/github/stars/redis/redis-om-python.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\n[redisearch-go-url]: https://github.com/RediSearch/redisearch-go\n[redisearch-go-author]: https://redis.com\n[redisearch-go-stars]: https://img.shields.io/github/stars/RediSearch/redisearch-go.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\n[redisearch-api-rs-url]: https://github.com/RediSearch/redisearch-api-rs\n[redisearch-api-rs-author]: https://redis.com\n[redisearch-api-rs-stars]: https://img.shields.io/github/stars/RediSearch/redisearch-api-rs.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\nDeployment Options\n\nThere are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as\n\nRedis Cloud\nCloud marketplaces: AWS Marketplace, Google Marketplace, or Azure Marketplace\nOn-premise: Redis Enterprise Software\nKubernetes: Redis Enterprise Software on Kubernetes\nDocker (RediSearch)\nDocker (Redis Stack)\n\nCluster support\n\nRediSearch has a distributed cluster version that scales to billions of documents across hundreds of servers. At the moment, distributed RediSearch is available as part of Redis Enterprise Cloud and Redis Enterprise Software.\n\nSee RediSearch on Redis Enterprise for more information.\n\nExamples\n\nProduct Search - eCommerce product search (with image and text)\nProduct Recommendations with DocArray / Jina - Content-based product recommendations example with Redis and DocArray.\nRedis VSS in RecSys - 3 end-to-end Redis \u0026 NVIDIA Merlin Recommendation System Architectures.\nAzure OpenAI Embeddings Q\u0026A - OpenAI and Redis as a Q\u0026A service on Azure.\nArXiv Paper Search - Semantic search over arXiv scholarly papers\n\nMore Resources\n\nFor more information on how to use Redis as a vector database, check out the following resources:\n\nRedis Vector Similarity Docs - Redis official docs for Vector Search.\nRedis-py Search Docs - Redis-py client library docs for RediSearch.\nVector Similarity Search: From Basics to Production - Introductory blog post to VSS and Redis as a VectorDB.\nAI-Powered Document Search - Blog post covering AI Powered Document Search Use Cases \u0026 Architectures.\nVector Database Benchmarks - Jina AI VectorDB benchmarks comparing Redis against others."])</script><script>self.__next_f.push([1,"65:T1fa2,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nRedis**\n    Setup: Set up the Redis-Py client. For more details go here\n    Index Data: Create the search index for vector search and hybrid search (vector + full-text search) on all available fields.\n    Search Data: Run a few example queries with various goals in mind.\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nRedis\n\nThe next vector database covered in this tutorial is Redis. You most likely already know Redis. What you might not be aware of is the RediSearch module. Enterprises have been using Redis with the RediSearch module for years now across all major cloud providers, Redis Cloud, and on premise. Recently, the Redis team added vector storage and search capability to this module in addition to the features RediSearch already had.\n\nGiven the large ecosystem around Redis, there are most likely client libraries in the language you need. You can use any standard Redis client library to run RediSearch commands, but it's easiest to use a library that wraps the RediSearch API. Below are a few examples, but you can find more client libraries here.\n\n| Project | Language | License | Author | Stars |\n|----------|---------|--------|---------|-------|\n| jedis | Java | MIT | Redis | Stars |\n| redis-py | Python | MIT | Redis | Stars |\n| node-redis | Node.js | MIT | Redis | Stars |\n| nredisstack | .NET | MIT | Redis | Stars |\n| redisearch-go | Go | BSD | Redis | [redisearch-go-stars] |\n| redisearch-api-rs | Rust | BSD | Redis | [redisearch-api-rs-stars] |\n\n[redis-url]: https://redis.com\n\n[redis-py-url]: https://github.com/redis/redis-py\n[redis-py-stars]: https://img.shields.io/github/stars/redis/redis-py.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[redis-py-package]: https://pypi.python.org/pypi/redis\n\n[jedis-url]: https://github.com/redis/jedis\n[jedis-stars]: https://img.shields.io/github/stars/redis/jedis.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[Jedis-package]: https://search.maven.org/artifact/redis.clients/jedis\n\n[nredisstack-url]: https://github.com/redis/nredisstack\n[nredisstack-stars]: https://img.shields.io/github/stars/redis/nredisstack.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[nredisstack-package]: https://www.nuget.org/packages/nredisstack/\n\n[node-redis-url]: https://github.com/redis/node-redis\n[node-redis-stars]: https://img.shields.io/github/stars/redis/node-redis.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n[node-redis-package]: https://www.npmjs.com/package/redis\n\n[redis-om-python-url]: https://github.com/redis/redis-om-python\n[redis-om-python-author]: https://redis.com\n[redis-om-python-stars]: https://img.shields.io/github/stars/redis/redis-om-python.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\n[redisearch-go-url]: https://github.com/RediSearch/redisearch-go\n[redisearch-go-author]: https://redis.com\n[redisearch-go-stars]: https://img.shields.io/github/stars/RediSearch/redisearch-go.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\n[redisearch-api-rs-url]: https://github.com/RediSearch/redisearch-api-rs\n[redisearch-api-rs-author]: https://redis.com\n[redisearch-api-rs-stars]: https://img.shields.io/github/stars/RediSearch/redisearch-api-rs.svg?style=social\u0026amp;label=Star\u0026amp;maxAge=2592000\n\n\nIn the below cells, we will walk you through using Redis as a vector database. Since many of you are likely already used to the Redis API, this should be familiar to most.\nSetup\n\nThere are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment. For other deployment options, see the redis directory in this repo.\n\nFor this tutorial, we will use Redis Stack on Docker.\n\nStart a version of Redis with RediSearch (Redis Stack) by running the following docker command\n\n$ cd redis\n$ docker compose up -d\nThis also includes the RedisInsight GUI for managing your Redis database which you can view at http://localhost:8001 once you start the docker container.\n\nYou're all set up and ready to go! Next, we import and create our client for communicating with the Redis database we just created.\nCreating a Search Index\n\nThe below cells will show how to specify and create a search index in Redis. We will\n\nSet some constants for defining our index like the distance metric and the index name\nDefine the index schema with RediSearch fields\nCreate the index\n\nLoad Documents into the Index\n\nNow that we have a search index, we can load documents into it. We will use the same documents we used in the previous examples. In Redis, either the Hash or JSON (if using RedisJSON in addition to RediSearch) data types can be used to store documents. We will use the HASH data type in this example. The below cells will show how to load documents into the index.\nRunning Search Queries\n\nNow that we have a search index and documents loaded into it, we can run search queries. Below we will provide a function that will run a search query and return the results. Using this function we run a few queries that will show how you can utilize Redis as a vector database. Each example will demonstrate specific features to keep in mind when developing your search application with Redis.\n\nReturn Fields: You can specify which fields you want to return in the search results. This is useful if you only want to return a subset of the fields in your documents and doesn't require a separate call to retrieve documents. In the below example, we will only return the title field in the search results.\nHybrid Search: You can combine vector search with any of the other RediSearch fields for hybrid search such as full text search, tag, geo, and numeric. In the below example, we will combine vector search with full text search.\n\nHybrid Queries with Redis\n\nThe previous examples showed how run vector search queries with RediSearch. In this section, we will show how to combine vector search with other RediSearch fields for hybrid search. In the below example, we will combine vector search with full text search.\nFor more example with Redis as a vector database, see the README and examples within the `vector_databases/redis` directory of this repository"])</script><script>self.__next_f.push([1,"66:T15ac,"])</script><script>self.__next_f.push([1,"\nThis notebook provides an introduction to using Redis as a vector database with OpenAI embeddings. Redis is a scalable, real-time database that can be used as a vector database when using the RediSearch Module. The RediSearch module allows you to index and search for vectors in Redis. This notebook will show you how to use the RediSearch module to index and search for vectors created by using the OpenAI API and stored in Redis.\n\nWhat is Redis?\n\nMost developers from a web services background are probably familiar with Redis. At it's core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choice Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years.\n\nIn addition to the traditional uses of Redis. Redis also provides Redis Modules which are a way to extend Redis with new data types and commands. Example modules include RedisJSON, RedisTimeSeries, RedisBloom and RediSearch.\n\nWhat is RediSearch?\n\nRediSearch is a Redis module that provides querying, secondary indexing, full-text search and vector search for Redis. To use RediSearch, you first declare indexes on your Redis data. You can then use the RediSearch clients to query that data. For more information on the feature set of RediSearch, see the README or the RediSearch documentation.\n\nDeployment options\n\nThere are a number of ways to deploy Redis. For local development, the quickest method is to use the Redis Stack docker container which we will use here. Redis Stack contains a number of Redis modules that can be used together to create a fast, multi-model data store and query engine.\n\nFor production use cases, The easiest way to get started is to use the Redis Cloud service. Redis Cloud is a fully managed Redis service. You can also deploy Redis on your own infrastructure using Redis Enterprise. Redis Enterprise is a fully managed Redis service that can be deployed in kubernetes, on-premises or in the cloud.\n\nAdditionally, every major cloud provider (AWS Marketplace, Google Marketplace, or Azure Marketplace) offers Redis Enterprise in a marketplace offering.\n\nPrerequisites\n\nBefore we start this project, we need to set up the following:\n\nstart a Redis database with RediSearch (redis-stack)\ninstall libraries\n    Redis-py\nget your OpenAI API key\n\nStart Redis\n\nTo keep this example simple, we will use the Redis Stack docker container which we can start as follows\n\n$ docker-compose up -d\n\nThis also includes the RedisInsight GUI for managing your Redis database which you can view at http://localhost:8001 once you start the docker container.\n\nYou're all set up and ready to go! Next, we import and create our client for communicating with the Redis database we just created.\nInstall Requirements\n\nRedis-Py is the python client for communicating with Redis. We will use this to communicate with our Redis-stack database.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of query data.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY by using following command:\nLoad data\n\nIn this section we'll load embedded data that has already been converted into vectors. We'll use this data to create an index in Redis and then search for similar vectors.\nConnect to Redis\n\nNow that we have our Redis database running, we can connect to it using the Redis-py client. We will use the default host and port for the Redis database which is localhost:6379.\n\nCreating a Search Index in Redis\n\nThe below cells will show how to specify and create a search index in Redis. We will:\n\nSet some constants for defining our index like the distance metric and the index name\nDefine the index schema with RediSearch fields\nCreate the index\nLoad Documents into the Index\n\nNow that we have a search index, we can load documents into it. We will use the same documents we used in the previous examples. In Redis, either the HASH or JSON (if using RedisJSON in addition to RediSearch) data types can be used to store documents. We will use the HASH data type in this example. The below cells will show how to load documents into the index.\nSimple Vector Search Queries with OpenAI Query Embeddings\n\nNow that we have a search index and documents loaded into it, we can run search queries. Below we will provide a function that will run a search query and return the results. Using this function we run a few queries that will show how you can utilize Redis as a vector database.\nHybrid Queries with Redis\n\nThe previous examples showed how run vector search queries with RediSearch. In this section, we will show how to combine vector search with other RediSearch fields for hybrid search. In the below example, we will combine vector search with full text search.\nHNSW Index\n\nUp until now, we've been using the `FLAT or \"brute-force\" index to run our queries. Redis also supports the HNSW index which is a fast, approximate index. The HNSW index is a graph-based index that uses a hierarchical navigable small world graph to store vectors. The HNSW` index is a good choice for large datasets where you want to run approximate queries.\n\n`HNSW will take longer to build and consume more memory for most cases than FLAT` but will be faster to run queries on, especially for large datasets.\n\nThe following cells will show how to create an `HNSW` index and run queries with it using the same data as before."])</script><script>self.__next_f.push([1,"67:Teab,"])</script><script>self.__next_f.push([1,"\nThis notebook provides an introduction to using Redis as a vector database with OpenAI embeddings and running hybrid queries that combine VSS and lexical search using Redis Query and Search capability. Redis is a scalable, real-time database that can be used as a vector database when using the RediSearch Module. The Redis Query and Search capability allows you to index and search for vectors in Redis. This notebook will show you how to use the Redis Query and Search to index and search for vectors created by using the OpenAI API and stored in Redis.\n\nHybrid queries combine vector similarity with traditional Redis Query and Search filtering capabilities on GEO, NUMERIC, TAG or TEXT data simplifying application code. A common example of a hybrid query in an e-commerce use case is to find items visually similar to a given query image limited to items available in a GEO location and within a price range.\nPrerequisites\n\nBefore we start this project, we need to set up the following:\n\nstart a Redis database with RediSearch (redis-stack)\ninstall libraries\n    Redis-py\nget your OpenAI API key\n\nStart Redis\n\nTo keep this example simple, we will use the Redis Stack docker container which we can start as follows\n\n$ docker-compose up -d\n\nThis also includes the RedisInsight GUI for managing your Redis database which you can view at http://localhost:8001 once you start the docker container.\n\nYou're all set up and ready to go! Next, we import and create our client for communicating with the Redis database we just created.\nInstall Requirements\n\nRedis-Py is the python client for communicating with Redis. We will use this to communicate with our Redis-stack database.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of query data.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY by using following command:\nLoad data\n\nIn this section we'll load and clean an ecommerce dataset. We'll generate embeddings using OpenAI and use this data to create an index in Redis and then search for similar vectors.\nConnect to Redis\n\nNow that we have our Redis database running, we can connect to it using the Redis-py client. We will use the default host and port for the Redis database which is localhost:6379.\n\nCreating a Search Index in Redis\n\nThe below cells will show how to specify and create a search index in Redis. We will:\n\nSet some constants for defining our index like the distance metric and the index name\nDefine the index schema with RediSearch fields\nCreate the index\nGenerate OpenAI Embeddings and Load Documents into the Index\n\nNow that we have a search index, we can load documents into it. We will use the dataframe containing the styles dataset loaded previously. In Redis, either the HASH or JSON (if using RedisJSON in addition to RediSearch) data types can be used to store documents. We will use the HASH data type in this example. The cells below will show how to get OpenAI embeddings for the different products and load documents into the index.\nSimple Vector Search Queries with OpenAI Query Embeddings\n\nNow that we have a search index and documents loaded into it, we can run search queries. Below we will provide a function that will run a search query and return the results. Using this function we run a few queries that will show how you can utilize Redis as a vector database.\nHybrid Queries with Redis\n\nThe previous examples showed how run vector search queries with RediSearch. In this section, we will show how to combine vector search with other RediSearch fields for hybrid search. In the example below, we will combine vector search with full text search."])</script><script>self.__next_f.push([1,"68:T699,"])</script><script>self.__next_f.push([1,"This notebook demonstrates how to use Redis as high-speed context memory with ChatGPT.\n\nPrerequisites\nRedis instance with the Redis Search and Redis JSON modules\nRedis-py client lib\nOpenAI Python client lib\nOpenAI API key\n\nInstallation\nInstall Python modules necessary for the examples.\nOpenAI API Key\nCreate a .env file and add your OpenAI key to it\nOpenAI Setup\nKey load + helper function for chat completion\nExperiment - Chat Completion on a Topic outside of the Model's Knowledge Cutoff Date\nGpt-3.5-turbo was trained on data up to Sep 2021.  Let's ask it a question about something that is beyond that date.  In this case, the FTX/Sam Bankman-Fried scandal.\nIncomplete Information\nAn unfortunate behavior of these AI systems is the system will provide a confident-sounding response - even when the system is not confident with its result.  One way to mitigate this is prompt re-engineering, as seen below.\nAdditional Context\nAnother way to combat incomplete information is to give the system more information such that it can make intelligent decisions vs guessing.  We'll use Redis as the source for that additional context.  We'll pull in business news articles from after the GPT knowledge cut-off date such that the system will have a better understanding of how FTX was actually managed.\nStart the Redis Stack Docker container\nConnect Redis client\nCreate Index\nFT.CREATE\nLoad Data Files into Redis as JSON Objects with Text and Vector Fields\nRedis JSON\nEmbed the Question and Perform VSS to find the most relevant document\nKNN Search\nRepeat the Question to OpenAI with context\nNow that we have relevant context, add that to the prompt to OpenAI and get a very different response."])</script><script>self.__next_f.push([1,"69:Te0d,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you step by step on using Tair as a vector database for OpenAI embeddings.\n\nThis notebook presents an end-to-end process of:\nUsing precomputed embeddings created by OpenAI API.\nStoring the embeddings in a cloud instance of Tair.\nConverting raw text query to an embedding with OpenAI API.\nUsing Tair to perform the nearest neighbour search in the created collection.\n\nWhat is Tair\n\nTair is a cloud native in-memory database service that is developed by Alibaba Cloud. Tair is compatible with open source Redis and provides a variety of data models and enterprise-class capabilities to support your real-time online scenarios. Tair also introduces persistent memory-optimized instances that are based on the new non-volatile memory (NVM) storage medium. These instances can reduce costs by 30%, ensure data persistence, and provide almost the same performance as in-memory databases. Tair has been widely used in areas such as government affairs, finance, manufacturing, healthcare, and pan-Internet to meet their high-speed query and computing requirements.\n\nTairvector is an in-house data structure that provides high-performance real-time storage and retrieval of vectors. TairVector provides two indexing algorithms: Hierarchical Navigable Small World (HNSW) and Flat Search. Additionally, TairVector supports multiple distance functions, such as Euclidean distance, inner product, and Jaccard distance. Compared with traditional vector retrieval services, TairVector has the following advantages:\nStores all data in memory and supports real-time index updates to reduce latency of read and write operations.\nUses an optimized data structure in memory to better utilize storage capacity.\nFunctions as an out-of-the-box data structure in a simple and efficient architecture without complex modules or dependencies.\n\nDeployment options\n\nUsing Tair Cloud Vector Database. Click here to fast deploy it.\n\nPrerequisites\n\nFor the purposes of this exercise we need to prepare a couple of things:\n\nTair cloud server instance.\nThe 'tair' library to interact with the tair database.\nAn OpenAI API key.\n\nInstall requirements\n\nThis notebook obviously requires the openai and tair packages, but there are also some other additional libraries we will use. The following command installs them all:\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it by getpass.\nConnect to Tair\nFirst add it to your environment variables.\n\nConnecting to a running instance of Tair server is easy with the official Python library.\nWe can test the connection by ping:\nThe downloaded file has to then be extracted:\nCreate Index\n\nTair stores data in indexes where each object is described by one key. Each key contains a vector and multiple attribute_keys.\n\nWe will start with creating two indexes, one for title_vector and one for content_vector, and then we will fill it with our precomputed embeddings.\nLoad data\n\nIn this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits.\nSearch data\n\nOnce the data is put into Tair we will start querying the collection for the closest vectors. We may provide an additional parameter vector_name to switch from title to content based search. Since the precomputed embeddings were created with text-embedding-3-small OpenAI model, we also have to use it during search."])</script><script>self.__next_f.push([1,"6a:Tc7e,"])</script><script>self.__next_f.push([1,"This notebook presents how to implement a Question Answering system with Langchain, Tair as a knowledge based and OpenAI embeddings. If you are not familiar with Tair, it’s better to check out the Getting_started_with_Tair_and_OpenAI.ipynb notebook.\n\nThis notebook presents an end-to-end process of:\nCalculating the embeddings with OpenAI API.\nStoring the embeddings in an Tair instance to build a knowledge base.\nConverting raw text query to an embedding with OpenAI API.\nUsing Tair to perform the nearest neighbour search in the created collection to find some context.\nAsking LLM to find the answer in a given context.\n\nAll the steps will be simplified to calling some corresponding Langchain methods.\nPrerequisites\nFor the purposes of this exercise we need to prepare a couple of things:\nTair cloud instance.\nLangchain as a framework.\nAn OpenAI API key.\nInstall requirements\nThis notebook requires the following Python packages: openai, tiktoken, langchain and tair.\nopenai provides convenient access to the OpenAI API.\ntiktoken is a fast BPE tokeniser for use with OpenAI's models.\nlangchain helps us to build applications with LLM more easily.\ntair library is used to interact with the tair vector database.\nPrepare your OpenAI API key\nThe OpenAI API key is used for vectorization of the documents and queries.\n\nIf you don't have an OpenAI API key, you can get one from [https://platform.openai.com/account/api-keys ).\n\nOnce you get your key, please add it by getpass.\nPrepare your Tair URL\nTo build the Tair connection, you need to have TAIR_URL.\nLoad data\nIn this section we are going to load the data containing some natural questions and answers to them. All the data will be used to create a Langchain application with Tair being the knowledge base.\nChain definition\n\nLangchain is already integrated with Tair and performs all the indexing for given list of documents. In our case we are going to store the set of answers we have.\nAt this stage all the possible answers are already stored in Tair, so we can define the whole QA chain.\nSearch data\n\nOnce the data is put into Tair we can start asking some questions. A question will be automatically vectorized by OpenAI model, and the created vector will be used to find some possibly matching answers in Tair. Once retrieved, the most similar answers will be incorporated into the prompt sent to OpenAI Large Language Model.\n\nCustom prompt templates\n\nThe stuff chain type in Langchain uses a specific prompt with question and context documents incorporated. This is what the default prompt looks like:\n\nUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n{context}\nQuestion: {question}\nHelpful Answer:\n\nWe can, however, provide our prompt template and change the behaviour of the OpenAI LLM, while still using the stuff chain type. It is important to keep {context} and {question} as placeholders.\n\nExperimenting with custom prompts\n\nWe can try using a different prompt template, so the model:\nResponds with a single-sentence answer if it knows it.\nSuggests a random song title if it doesn't know the answer to our question."])</script><script>self.__next_f.push([1,"6b:T80c,"])</script><script>self.__next_f.push([1,"\nTypesense is an open source, in-memory search engine, that you can either self-host or run on Typesense Cloud.\n\nWhy Typesense?\n\nTypesense focuses on performance by storing the entire index in RAM (with a backup on disk) and also focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults.\n\nIt also lets you combine attribute-based filtering together with vector queries, to fetch the most relevant documents.\n\nOther features\n\nBesides vector storage and search, Typesense also offers the following features:\n\nTypo Tolerance: Handles typographical errors elegantly, out-of-the-box.\nTunable Ranking: Easy to tailor your search results to perfection.\nSorting: Dynamically sort results based on a particular field at query time (helpful for features like \"Sort by Price (asc)\").\nFaceting \u0026 Filtering: Drill down and refine results.\nGrouping \u0026 Distinct: Group similar results together to show more variety.\nFederated Search: Search across multiple collections (indices) in a single HTTP request.\nScoped API Keys: Generate API keys that only allow access to certain records, for multi-tenant applications.\nSynonyms: Define words as equivalents of each other, so searching for a word will also return results for the synonyms defined.\nCuration \u0026 Merchandizing: Boost particular records to a fixed position in the search results, to feature them.\nRaft-based Clustering: Set up a distributed cluster that is highly available.\nSeamless Version Upgrades: As new versions of Typesense come out, upgrading is as simple as swapping out the binary and restarting Typesense.\nNo Runtime Dependencies: Typesense is a single binary that you can run locally or in production with a single command.\n\nHow To\n\nTo learn more about how to use Typesense with OpenAI embeddings, see the notebook here for an example: examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb\nTo learn more about Typesense's vector search feature, read the docs here: https://typesense.org/docs/0.24.1/api/vector-search.html.\n"])</script><script>self.__next_f.push([1,"6c:T1019,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nTypesense**\n    Setup: Set up the Typesense Python client. For more details go here\n    Index Data: We'll create a collection and index it for both titles and content.\n    Search Data: Run a few example queries with various goals in mind.\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nTypesense\n\nThe next vector store we'll look at is Typesense, which is an open source, in-memory search engine, that you can either self-host or run on Typesense Cloud.\n\nTypesense focuses on performance by storing the entire index in RAM (with a backup on disk) and also focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults. It also lets you combine attribute-based filtering together with vector queries.\n\nFor this example, we will set up a local docker-based Typesense server, index our vectors in Typesense and then do some nearest-neighbor search queries. If you use Typesense Cloud, you can skip the docker setup part and just obtain the hostname and API keys from your cluster dashboard.\nSetup\n\nTo run Typesense locally, you'll need Docker. Following the instructions contained in the Typesense documentation here, we created an example docker-compose.yml file in this repo saved at ./typesense/docker-compose.yml.\n\nAfter starting Docker, you can start Typesense locally by navigating to the examples/vector_databases/typesense/ directory and running docker-compose up -d.\n\nThe default API key is set to xyz in the Docker compose file, and the default Typesense port to 8108.\nIndex data\n\nTo index vectors in Typesense, we'll first create a Collection (which is a collection of Documents) and turn on vector indexing for a particular field. You can even store multiple vector fields in a single document.\nSearch Data\n\nNow that we've imported the vectors into Typesense, we can do a nearest neighbor search on the title_vector or content_vector field.\nThanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."])</script><script>self.__next_f.push([1,"6d:T5d7,"])</script><script>self.__next_f.push([1,"\n​Weaviate is an open-source vector search engine (docs - Github) that can store and search through OpenAI embeddings and data objects. The database allows you to do similarity search, hybrid search (the combining of multiple search techniques, such as keyword-based and vector search), and generative search (like Q\u0026A). Weaviate also supports a wide variety of OpenAI-based modules (e.g., text2vec-openai, qna-openai), allowing you to vectorize and query data fast and efficiently.\n\nYou can run Weaviate (including the OpenAI modules if desired) in three ways:\n\nOpen source inside a Docker-container (example)\nUsing the Weaviate Cloud Service (get started)\nIn a Kubernetes cluster (learn more)\n\nExamples\n\nThis folder contains a variety of Weaviate and OpenAI examples.\n\n| Name | Description | language | Google Colab |\n| --- | --- | --- | --- |\n| Getting Started with Weaviate and OpenAI | A simple getting started for semantic vector search using the OpenAI vectorization module in Weaviate (text2vec-openai) | Python Notebook | link |\n| Hybrid Search with Weaviate and OpenAI | A simple getting started for hybrid search using the OpenAI vectorization module in Weaviate (text2vec-openai) | Python Notebook | link |\n| Question Answering with Weaviate and OpenAI | A simple getting started for question answering (Q\u0026A) using the OpenAI Q\u0026A module in Weaviate (qna-openai) | Python Notebook | link |\n| Docker-compose example | A Docker-compose file with all OpenAI modules enabled | Docker |\n"])</script><script>self.__next_f.push([1,"6e:T1350,"])</script><script>self.__next_f.push([1,"\nThis notebook takes you through a simple flow to download some data, embed it, and then index and search it using a selection of vector databases. This is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is a Vector Database\n\nA vector database is a database made to store, manage and search embedding vectors. The use of embeddings to encode unstructured data (text, audio, video and more) as vectors for consumption by machine-learning models has exploded in recent years, due to the increasing effectiveness of AI in solving use cases involving natural language, image recognition and other unstructured forms of data. Vector databases have emerged as an effective solution for enterprises to deliver and scale these use cases.\n\nWhy use a Vector Database\n\nVector databases enable enterprises to take many of the embeddings use cases we've shared in this repo (question and answering, chatbot and recommendation services, for example), and make use of them in a secure, scalable environment. Many of our customers make embeddings solve their problems at small scale but performance and security hold them back from going into production - we see vector databases as a key component in solving that, and in this guide we'll walk through the basics of embedding text data, storing it in a vector database and using it for semantic search.\n\nDemo Flow\nThe demo flow is:\nSetup**: Import packages and set any required variables\nLoad data**: Load a dataset and embed it using OpenAI embeddings\nWeaviate**\n    Setup: Here we'll set up the Python client for Weaviate. For more details go here\n    Index Data: We'll create an index with title search vectors in it\n    Search Data: We'll run a few searches to confirm it works\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nSetup\n\nImport the required libraries and set the embedding model that we'd like to use.\nLoad data\n\nIn this section we'll load embedded data that we've prepared previous to this session.\nWeaviate\n\nAnother vector database option we'll explore is Weaviate, which offers both a managed, SaaS option, as well as a self-hosted open source option. As we've already looked at a cloud vector database, we'll try the self-hosted option here.\n\nFor this we will:\nSet up a local deployment of Weaviate\nCreate indices in Weaviate\nStore our data there\nFire some similarity search queries\nTry a real use case\n\nBring your own vectors approach\nIn this cookbook, we provide the data with already generated vectors. This is a good approach for scenarios, where your data is already vectorized.\n\nAutomated vectorization with OpenAI module\nFor scenarios, where your data is not vectorized yet, you can delegate the vectorization task with OpenAI to Weaviate.\nWeaviate offers a built-in module text2vec-openai, which takes care of the vectorization for you at:\nimport\nfor any CRUD operations\nfor semantic search\n\nCheck out the Getting Started with Weaviate and OpenAI module cookbook to learn step by step how to import and vectorize data in one step.\nSetup\n\nTo run Weaviate locally, you'll need Docker. Following the instructions contained in the Weaviate documentation here, we created an example docker-compose.yml file in this repo saved at ./weaviate/docker-compose.yml.\n\nAfter starting Docker, you can start Weaviate locally by navigating to the examples/vector_databases/weaviate/ directory and running docker-compose up -d.\n\nSaaS\nAlternatively you can use Weaviate Cloud Service (WCS) to create a free Weaviate cluster.\ncreate a free account and/or login to WCS\ncreate a Weaviate Cluster with the following settings:\n    Sandbox: Sandbox Free\n    Weaviate Version: Use default (latest)\n    OIDC Authentication: Disabled\nyour instance should be ready in a minute or two\nmake a note of the Cluster Id. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: https://your-project-name-suffix.weaviate.network\nIndex data\n\nIn Weaviate you create schemas to capture each of the entities you will be searching.\n\nIn this case we'll create a schema called Article with the title vector from above included for us to search by.\n\nThe next few steps closely follow the documentation Weaviate provides here.\n\nSearch data\n\nAs above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors\nLet Weaviate handle vector embeddings\n\nWeaviate has a built-in module for OpenAI, which takes care of the steps required to generate a vector embedding for your queries and any CRUD operations.\n\nThis allows you to run a vector query with the with_near_text filter, which uses your OPEN_API_KEY."])</script><script>self.__next_f.push([1,"6f:T959,"])</script><script>self.__next_f.push([1,"\nThis notebook is prepared for a scenario where:\nYour data is already in Weaviate\nYou want to use Weaviate with the Generative OpenAI module (generative-openai).\n\nPrerequisites\n\nThis cookbook only coveres Generative Search examples, however, it doesn't cover the configuration and data imports.\n\nIn order to make the most of this cookbook, please complete the Getting Started cookbook first, where you will learn the essentials of working with Weaviate and import the demo data.\n\nChecklist:\ncompleted Getting Started cookbook,\ncrated a Weaviate instance,\nimported data into your Weaviate instance,\nyou have an OpenAI API key\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of your data at import, and for running queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to your Weaviate instance\n\nIn this section, we will:\n\ntest env variable OPENAI_API_KEY – make sure you completed the step in #Prepare-your-OpenAI-API-key\nconnect to your Weaviate with your OpenAI API Key\nand test the client connection\n\nThe client\n\nAfter this step, the client object will be used to perform all Weaviate-related operations.\nGenerative Search\nWeaviate offers a Generative Search OpenAI module, which generates responses based on the data stored in your Weaviate instance.\n\nThe way you construct a generative search query is very similar to a standard semantic search query in Weaviate.\n\nFor example:\nsearch in \"Articles\",\nreturn \"title\", \"content\", \"url\"\nlook for objects related to \"football clubs\"\nlimit results to 5 objects\n\n    result = (\n        client.query\n        .get(\"Articles\", [\"title\", \"content\", \"url\"])\n        .with_near_text(\"concepts\": \"football clubs\")\n        .with_limit(5)\ngenerative query will go here\n        .do()\n    )\n\nNow, you can add with_generate() function to apply generative transformation. with_generate takes either:\nsingle_prompt - to generate a response for each returned object,\ngrouped_task – to generate a single response from all returned objects.\n\nThanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."])</script><script>self.__next_f.push([1,"70:T1c73,"])</script><script>self.__next_f.push([1,"\nThis notebook is prepared for a scenario where:\nYour data is not vectorized\nYou want to run Vector Search on your data\nYou want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.\n\nThis notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run semantic search.\n\nThis is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is Weaviate\n\nWeaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.\n\nWeaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.\n\nWeaviate let you use your favorite ML-models, and scale seamlessly into billions of data objects.\n\nDeployment options\n\nWhatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:\nSelf-hosted – you can deploy Weaviate with docker locally, or any server you want.\nSaaS – you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.\nHybrid-SaaS – you can deploy Weaviate in your own private Cloud Service.\n\nProgramming languages\n\nWeaviate offers four client libraries, which allow you to communicate from your apps:\nPython\nJavaScript\nJava\nGo\n\nAdditionally, Weaviate has a REST layer. Basically you can call Weaviate from any language that supports REST requests.\nDemo Flow\nThe demo flow is:\nPrerequisites Setup**: Create a Weaviate instance and install the required libraries\nConnect**: Connect to your Weaviate instance\nSchema Configuration**: Configure the schema of your data\n    Note: Here we can define which OpenAI Embedding Model to use\n    Note: Here we can configure which properties to index\nImport data**: Load a demo dataset and import it into Weaviate\n    Note: The import process will automatically index your data - based on the configuration in the schema\n    Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you\nRun Queries**: Query\n    Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nOpenAI Module in Weaviate\nAll Weaviate instances come equipped with the text2vec-openai module.\n\nThis module is responsible for handling vectorization during import (or any CRUD operations) and when you run a query.\n\nNo need to manually vectorize data\nThis is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.\n\nAll you need to do is:\nprovide your OpenAI API Key – when you connected to the Weaviate Client\ndefine which OpenAI vectorizer to use in your Schema\nPrerequisites\n\nBefore we start this project, we need setup the following:\n\ncreate a Weaviate instance\ninstall libraries\n    weaviate-client\n    datasets\n    apache-beam\nget your OpenAI API key\n\nCreate a Weaviate instance\n\nTo create a Weaviate instance we have 2 options:\n\n(Recommended path) Weaviate Cloud Service – to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.\nInstall and run Weaviate locally with Docker.\n\nOption 1 – WCS Installation Steps\n\nUse Weaviate Cloud Service (WCS) to create a free Weaviate cluster.\ncreate a free account and/or login to WCS\ncreate a Weaviate Cluster with the following settings:\n    Sandbox: Sandbox Free\n    Weaviate Version: Use default (latest)\n    OIDC Authentication: Disabled\nyour instance should be ready in a minute or two\nmake a note of the Cluster Id. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: https://your-project-name.weaviate.network\n\nOption 2 – local Weaviate instance with Docker\n\nInstall and run Weaviate locally with Docker.\nDownload the ./docker-compose.yml file\nThen open your terminal, navigate to where your docker-compose.yml file is located, and start docker with: docker-compose up -d\nOnce this is ready, your instance should be available at http://localhost:8080\n\nNote. To shut down your docker instance you can call: docker-compose down\n\nLearn more\nTo learn more, about using Weaviate with Docker see the installation documentation.\n\nInstall required libraries\n\nBefore running this project make sure to have the following libraries:\n\nWeaviate Python client\n\nThe Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.\n\ndatasets \u0026 apache-beam\n\nTo load sample data, you need the datasets library and its dependency apache-beam.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of your data at import, and for running queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to your Weaviate instance\n\nIn this section, we will:\n\ntest env variable OPENAI_API_KEY – make sure you completed the step in #Prepare-your-OpenAI-API-key\nconnect to your Weaviate with your OpenAI API Key\nand test the client connection\n\nThe client\n\nAfter this step, the client object will be used to perform all Weaviate-related operations.\nSchema\n\nIn this section, we will:\nconfigure the data schema for your data\nselect OpenAI module\nThis is the second and final step, which requires OpenAI specific configuration.\nAfter this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.\n\nWhat is a schema\n\nIn Weaviate you create schemas to capture each of the entities you will be searching.\n\nA schema is how you tell Weaviate:\nwhat embedding model should be used to vectorize the data\nwhat your data is made of (property names and types)\nwhich properties should be vectorized and indexed\n\nIn this cookbook we will use a dataset for Articles, which contains:\ntitle\ncontent\nurl\n\nWe want to vectorize title and content, but not the url.\n\nTo vectorize and query the data, we will use text-embedding-3-small.\nImport data\n\nIn this section we will:\nload the Simple Wikipedia dataset\nconfigure Weaviate Batch import (to make the import more efficient)\nimport the data into Weaviate\nNote:\nLike mentioned before. We don't need to manually vectorize the data.\nThe text2vec-openai module will take care of that.\nSearch Data\n\nAs above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors\nThanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."])</script><script>self.__next_f.push([1,"71:T1cc1,"])</script><script>self.__next_f.push([1,"\nThis notebook is prepared for a scenario where:\nYour data is not vectorized\nYou want to run Hybrid Search (learn more) on your data\nYou want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.\n\nThis notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run hybrid search (mixing of vector and BM25 search).\n\nThis is a common requirement for customers who want to store and search our embeddings with their own data in a secure environment to support production use cases such as chatbots, topic modelling and more.\n\nWhat is Weaviate\n\nWeaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.\n\nWeaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.\n\nWeaviate let you use your favorite ML-models, and scale seamlessly into billions of data objects.\n\nDeployment options\n\nWhatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:\nSelf-hosted – you can deploy Weaviate with docker locally, or any server you want.\nSaaS – you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.\nHybrid-SaaS – you can deploy Weaviate in your own private Cloud Service\n\nProgramming languages\n\nWeaviate offers four client libraries, which allow you to communicate from your apps:\nPython\nJavaScript\nJava\nGo\n\nAdditionally, Weaviate has a REST layer. Basically you can call Weaviate from any language that supports REST requests.\nDemo Flow\nThe demo flow is:\nPrerequisites Setup**: Create a Weaviate instance and install required libraries\nConnect**: Connect to your Weaviate instance\nSchema Configuration**: Configure the schema of your data\n    Note: Here we can define which OpenAI Embedding Model to use\n    Note: Here we can configure which properties to index\nImport data**: Load a demo dataset and import it into Weaviate\n    Note: The import process will automatically index your data - based on the configuration in the schema\n    Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you\nRun Queries**: Query\n    Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases, and can move on to more complex use cases making use of our embeddings.\nOpenAI Module in Weaviate\nAll Weaviate instances come equipped with the text2vec-openai module.\n\nThis module is responsible for handling vectorization during import (or any CRUD operations) and when you run a query.\n\nNo need to manually vectorize data\nThis is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.\n\nAll you need to do is:\nprovide your OpenAI API Key – when you connected to the Weaviate Client\ndefine which OpenAI vectorizer to use in your Schema\nPrerequisites\n\nBefore we start this project, we need setup the following:\n\ncreate a Weaviate instance\ninstall libraries\n    weaviate-client\n    datasets\n    apache-beam\nget your OpenAI API key\n\nCreate a Weaviate instance\n\nTo create a Weaviate instance we have 2 options:\n\n(Recommended path) Weaviate Cloud Service – to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.\nInstall and run Weaviate locally with Docker.\n\nOption 1 – WCS Installation Steps\n\nUse Weaviate Cloud Service (WCS) to create a free Weaviate cluster.\ncreate a free account and/or login to WCS\ncreate a Weaviate Cluster with the following settings:\n    Sandbox: Sandbox Free\n    Weaviate Version: Use default (latest)\n    OIDC Authentication: Disabled\nyour instance should be ready in a minute or two\nmake a note of the Cluster Id. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: https://your-project-name.weaviate.network\n\nOption 2 – local Weaviate instance with Docker\n\nInstall and run Weaviate locally with Docker.\nDownload the ./docker-compose.yml file\nThen open your terminal, navigate to where your docker-compose.yml file is located, and start docker with: docker-compose up -d\nOnce this is ready, your instance should be available at http://localhost:8080\n\nNote. To shut down your docker instance you can call: docker-compose down\n\nLearn more\nTo learn more, about using Weaviate with Docker see the installation documentation.\n\nInstall required libraries\n\nBefore running this project make sure to have the following libraries:\n\nWeaviate Python client\n\nThe Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.\n\ndatasets \u0026 apache-beam\n\nTo load sample data, you need the datasets library and its' dependency apache-beam.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of your data at import, and for running queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to your Weaviate instance\n\nIn this section, we will:\n\ntest env variable OPENAI_API_KEY – make sure you completed the step in #Prepare-your-OpenAI-API-key\nconnect to your Weaviate your OpenAI API Key\nand test the client connection\n\nThe client\n\nAfter this step, the client object will be used to perform all Weaviate-related operations.\nSchema\n\nIn this section, we will:\nconfigure the data schema for your data\nselect OpenAI module\nThis is the second and final step, which requires OpenAI specific configuration.\nAfter this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.\n\nWhat is a schema\n\nIn Weaviate you create schemas to capture each of the entities you will be searching.\n\nA schema is how you tell Weaviate:\nwhat embedding model should be used to vectorize the data\nwhat your data is made of (property names and types)\nwhich properties should be vectorized and indexed\n\nIn this cookbook we will use a dataset for Articles, which contains:\ntitle\ncontent\nurl\n\nWe want to vectorize title and content, but not the url.\n\nTo vectorize and query the data, we will use text-embedding-3-small.\nImport data\n\nIn this section we will:\nload the Simple Wikipedia dataset\nconfigure Weaviate Batch import (to make the import more efficient)\nimport the data into Weaviate\nNote:\nLike mentioned before. We don't need to manually vectorize the data.\nThe text2vec-openai module will take care of that.\nSearch Data\n\nAs above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors\n\nLearn more about the alpha setting here\nThanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."])</script><script>self.__next_f.push([1,"72:T1c91,"])</script><script>self.__next_f.push([1,"\nThis notebook is prepared for a scenario where:\nYour data is not vectorized\nYou want to run Q\u0026A (learn more) on your data based on the OpenAI completions endpoint.\nYou want to use Weaviate with the OpenAI module (text2vec-openai), to generate vector embeddings for you.\n\nThis notebook takes you through a simple flow to set up a Weaviate instance, connect to it (with OpenAI API key), configure data schema, import data (which will automatically generate vector embeddings for your data), and run question answering.\n\nWhat is Weaviate\n\nWeaviate is an open-source vector search engine that stores data objects together with their vectors. This allows for combining vector search with structured filtering.\n\nWeaviate uses KNN algorithms to create an vector-optimized index, which allows your queries to run extremely fast. Learn more here.\n\nWeaviate let you use your favorite ML-models, and scale seamlessly into billions of data objects.\n\nDeployment options\n\nWhatever your scenario or production setup, Weaviate has an option for you. You can deploy Weaviate in the following setups:\nSelf-hosted – you can deploy Weaviate with docker locally, or any server you want.\nSaaS – you can use Weaviate Cloud Service (WCS) to host your Weaviate instances.\nHybrid-SaaS – you can deploy Weaviate in your own private Cloud Service\n\nProgramming languages\n\nWeaviate offers four client libraries, which allow you to communicate from your apps:\nPython\nJavaScript\nJava\nGo\n\nAdditionally, Weaviate has a REST layer. Basically you can call Weaviate from any language that supports REST requests.\nDemo Flow\nThe demo flow is:\nPrerequisites Setup**: Create a Weaviate instance and install required libraries\nConnect**: Connect to your Weaviate instance\nSchema Configuration**: Configure the schema of your data\n    Note: Here we can define which OpenAI Embedding Model to use\n    Note: Here we can configure which properties to index\nImport data**: Load a demo dataset and import it into Weaviate\n    Note: The import process will automatically index your data - based on the configuration in the schema\n    Note: You don't need to explicitly vectorize your data, Weaviate will communicate with OpenAI to do it for you\nRun Queries**: Query\n    Note: You don't need to explicitly vectorize your queries, Weaviate will communicate with OpenAI to do it for you\n    Note: The qna-openai module automatically communicates with the OpenAI completions endpoint\n\nOnce you've run through this notebook you should have a basic understanding of how to setup and use vector databases for question answering.\nOpenAI Module in Weaviate\nAll Weaviate instances come equipped with the text2vec-openai and the qna-openai modules.\n\nThe first module is responsible for handling vectorization at import (or any CRUD operations) and when you run a search query. The second module communicates with the OpenAI completions endpoint.\n\nNo need to manually vectorize data\nThis is great news for you. With text2vec-openai you don't need to manually vectorize your data, as Weaviate will call OpenAI for you whenever necessary.\n\nAll you need to do is:\nprovide your OpenAI API Key – when you connected to the Weaviate Client\ndefine which OpenAI vectorizer to use in your Schema\nPrerequisites\n\nBefore we start this project, we need setup the following:\n\ncreate a Weaviate instance\ninstall libraries\n    weaviate-client\n    datasets\n    apache-beam\nget your OpenAI API key\n\nCreate a Weaviate instance\n\nTo create a Weaviate instance we have 2 options:\n\n(Recommended path) Weaviate Cloud Service – to host your Weaviate instance in the cloud. The free sandbox should be more than enough for this cookbook.\nInstall and run Weaviate locally with Docker.\n\nOption 1 – WCS Installation Steps\n\nUse Weaviate Cloud Service (WCS) to create a free Weaviate cluster.\ncreate a free account and/or login to WCS\ncreate a Weaviate Cluster with the following settings:\n    Sandbox: Sandbox Free\n    Weaviate Version: Use default (latest)\n    OIDC Authentication: Disabled\nyour instance should be ready in a minute or two\nmake a note of the Cluster Id. The link will take you to the full path of your cluster (you will need it later to connect to it). It should be something like: https://your-project-name.weaviate.network\n\nOption 2 – local Weaviate instance with Docker\n\nInstall and run Weaviate locally with Docker.\nDownload the ./docker-compose.yml file\nThen open your terminal, navigate to where your docker-compose.yml file is located, and start docker with: docker-compose up -d\nOnce this is ready, your instance should be available at http://localhost:8080\n\nNote. To shut down your docker instance you can call: docker-compose down\n\nLearn more\nTo learn more, about using Weaviate with Docker see the installation documentation.\n\nInstall required libraries\n\nBefore running this project make sure to have the following libraries:\n\nWeaviate Python client\n\nThe Weaviate Python client allows you to communicate with your Weaviate instance from your Python project.\n\ndatasets \u0026 apache-beam\n\nTo load sample data, you need the datasets library and its' dependency apache-beam.\n\nPrepare your OpenAI API key\n\nThe OpenAI API key is used for vectorization of your data at import, and for queries.\n\nIf you don't have an OpenAI API key, you can get one from https://beta.openai.com/account/api-keys.\n\nOnce you get your key, please add it to your environment variables as OPENAI_API_KEY.\nConnect to your Weaviate instance\n\nIn this section, we will:\n\ntest env variable OPENAI_API_KEY – make sure you completed the step in #Prepare-your-OpenAI-API-key\nconnect to your Weaviate your OpenAI API Key\nand test the client connection\n\nThe client\n\nAfter this step, the client object will be used to perform all Weaviate-related operations.\nSchema\n\nIn this section, we will:\nconfigure the data schema for your data\nselect OpenAI module\nThis is the second and final step, which requires OpenAI specific configuration.\nAfter this step, the rest of instructions wlll only touch on Weaviate, as the OpenAI tasks will be handled automatically.\n\nWhat is a schema\n\nIn Weaviate you create schemas to capture each of the entities you will be searching.\n\nA schema is how you tell Weaviate:\nwhat embedding model should be used to vectorize the data\nwhat your data is made of (property names and types)\nwhich properties should be vectorized and indexed\n\nIn this cookbook we will use a dataset for Articles, which contains:\ntitle\ncontent\nurl\n\nWe want to vectorize title and content, but not the url.\n\nTo vectorize and query the data, we will use text-embedding-3-small. For Q\u0026A we will use gpt-3.5-turbo-instruct.\nImport data\n\nIn this section we will:\nload the Simple Wikipedia dataset\nconfigure Weaviate Batch import (to make the import more efficient)\nimport the data into Weaviate\nNote:\nLike mentioned before. We don't need to manually vectorize the data.\nThe text2vec-openai module will take care of that.\nQuestion Answering on the Data\n\nAs above, we'll fire some queries at our new Index and get back results based on the closeness to our existing vectors\nThanks for following along, you're now equipped to set up your own vector databases and use embeddings to do all kinds of cool things - enjoy! For more complex use cases please continue to work through other cookbook examples in this repo."])</script><script>self.__next_f.push([1,"73:T9cc,"])</script><script>self.__next_f.push([1,"Finding your next movie\n\nIn this notebook we will be going over generating embeddings of movie descriptions with OpenAI and using those embeddings within Zilliz to find relevant movies. To narrow our search results and try something new, we are going to be using filtering to do metadata searches. The dataset in this example is sourced from HuggingFace datasets, and contains a little over 8 thousand movie entries.\n\nLets begin by first downloading the required libraries for this notebook:\nopenai is used for communicating with the OpenAI embedding service\npymilvus is used for communicating with the Zilliz server\ndatasets is used for downloading the dataset\ntqdm is used for the progress bars\n\nTo get Zilliz up and running take a look here. With your account and database set up, proceed to set the following values:\nURI: The URI your database is running on\nUSER: Your database username\nPASSWORD: Your database password\nCOLLECTION_NAME: What to name the collection within Zilliz\nDIMENSION: The dimension of the embeddings\nOPENAI_ENGINE: Which embedding model to use\nopenai.api_key: Your OpenAI account key\nINDEX_PARAM: The index settings to use for the collection\nQUERY_PARAM: The search parameters to use\nBATCH_SIZE: How many texts to embed and insert at once\nDataset\nWith Zilliz up and running we can begin grabbing our data. Hugging Face Datasets is a hub that holds many different user datasets, and for this example we are using HuggingLearners's netflix-shows dataset. This dataset contains movies and their metadata pairs for over 8 thousand movies. We are going to embed each description and store it within Zilliz along with its title, type, release_year and rating.\nInsert the Data\nNow that we have our data on our machine we can begin embedding it and inserting it into Zilliz. The embedding function takes in text and returns the embeddings in a list format.\nThis next step does the actual inserting. We iterate through all the entries and create batches that we insert once we hit our set batch size. After the loop is over we insert the last remaning batch if it exists.\nQuery the Database\nWith our data safely inserted into Zilliz, we can now perform a query. The query takes in a tuple of the movie description you are searching for and the filter to use. More info about the filter can be found here. The search first prints out your description and filter expression. After that for each result we print the score, title, type, release year, rating and description of the result movies."])</script><script>self.__next_f.push([1,"74:T997,"])</script><script>self.__next_f.push([1,"Finding your next book\n\nIn this notebook we will be going over generating embeddings of book descriptions with OpenAI and using those embeddings within Zilliz to find relevant books. The dataset in this example is sourced from HuggingFace datasets, and contains a little over 1 million title-description pairs.\n\nLets begin by first downloading the required libraries for this notebook:\nopenai is used for communicating with the OpenAI embedding service\npymilvus is used for communicating with the Zilliz instance\ndatasets is used for downloading the dataset\ntqdm is used for the progress bars\n\nTo get Zilliz up and running take a look here. With your account and database set up, proceed to set the following values:\nURI: The URI your database is running on\nUSER: Your database username\nPASSWORD: Your database password\nCOLLECTION_NAME: What to name the collection within Zilliz\nDIMENSION: The dimension of the embeddings\nOPENAI_ENGINE: Which embedding model to use\nopenai.api_key: Your OpenAI account key\nINDEX_PARAM: The index settings to use for the collection\nQUERY_PARAM: The search parameters to use\nBATCH_SIZE: How many texts to embed and insert at once\nZilliz\nThis segment deals with Zilliz and setting up the database for this use case. Within Zilliz we need to setup a collection and index it.\nDataset\nWith Zilliz up and running we can begin grabbing our data. Hugging Face Datasets is a hub that holds many different user datasets, and for this example we are using Skelebor's book dataset. This dataset contains title-description pairs for over 1 million books. We are going to embed each description and store it within Zilliz along with its title.\nInsert the Data\nNow that we have our data on our machine we can begin embedding it and inserting it into Zilliz. The embedding function takes in text and returns the embeddings in a list format.\nThis next step does the actual inserting. Due to having so many datapoints, if you want to immediately test it out you can stop the inserting cell block early and move along. Doing this will probably decrease the accuracy of the results due to less datapoints, but it should still be good enough.\nQuery the Database\nWith our data safely inserted in Zilliz, we can now perform a query. The query takes in a string or a list of strings and searches them. The results print out your provided description and the results that include the result score, the result title, and the result book description."])</script><script>self.__next_f.push([1,"75:T93c3,"])</script><script>self.__next_f.push([1,"\nWhen GPT-3 fails on a task, what should you do?\n\nSearch for a better prompt that elicits more reliable answers?\nInvest in thousands of examples to fine-tune a custom model?\nAssume the model is incapable of the task, and move on?\n\nThere is no simple answer - it depends. However, if your task involves logical reasoning or complexity, consider trying the techniques in this article to build more reliable, high-performing prompts.\n\nWhy GPT-3 fails on complex tasks\n\nIf you were asked to multiply 13 by 17, would the answer pop immediately into your mind? For most of us, probably not. Yet, that doesn't mean humans are incapable of two-digit multiplication. With a few seconds, and some pen and paper, it's not too taxing to work out that 13 x 17 = 130 + 70 + 21 = 221.\n\nSimilarly, if you give GPT-3 a task that's too complex to do in the time it takes to calculate its next token, it may confabulate an incorrect guess. Yet, akin to humans, that doesn't necessarily mean the model is incapable of the task. With some time and space to reason things out, the model still may be able to answer reliably.\n\nAs an example, if you ask gpt-3.5-turbo-instruct the following math problem about juggling balls, it answers incorrectly:\n\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA:\n\nThere are 8 blue golf balls.\n\nDoes this mean that GPT-3 cannot do simple math problems? No; in fact, it turns out that by prompting the model with Let's think step by step, the model solves the problem reliably:\n\nQ: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\nA: Let's think step by step.\n\nThere are 16 balls in total.\nHalf of the balls are golf balls.\nThat means that there are 8 golf balls.\nHalf of the golf balls are blue.\nThat means that there are 4 blue golf balls.\n\nOf course, it's hard to tell from only a single example whether this Let's think step by step trick actually works in general or just got lucky on this particular problem. But it really does work. On a benchmark of word math problems, the Let's think step by step trick raised GPT-3's solve rate massively, from a worthless 18% to a decent 79%!\n\nModel capabilities depend on context\n\nWhen learning to work with GPT-3, one common conceptual mistake is to believe that its capabilities are fixed across all contexts. E.g., if GPT-3 gets a simple logic question wrong, then it must be incapable of simple logic.\n\nBut as the Let's think step by step example illustrates, apparent failures of GPT-3 can sometimes be remedied with a better prompt that helps the model steer itself toward the correct output.\n\nHow to improve reliability on complex tasks\n\nThe rest of this article shares techniques for improving reliability of large language models on complex tasks. Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks, e.g.:\n\nGive clearer instructions\nSplit complex tasks into simpler subtasks\nStructure the instruction to keep the model on task\nPrompt the model to explain before answering\nAsk for justifications of many possible answers, and then synthesize\nGenerate many outputs, and then use the model to pick the best one\nFine-tune custom models to maximize performance\n\nSplit complex tasks into simpler tasks\n\nOne way to give a model more time and space to think is to break tasks into simpler pieces.\n\nAs an example, consider a task where we ask the model a multiple-choice question about some text - in this case, a game of Clue. When asked directly, gpt-3.5-turbo-instruct isn't able to put clues 3 \u0026 5 together, and answers incorrectly:\n\nUse the following clues to answer the following multiple-choice question.\n\nClues:\nMiss Scarlett was the only person in the lounge.\nThe person with the pipe was in the kitchen.\nColonel Mustard was the only person in the observatory.\nProfessor Plum was not in the library nor the billiard room.\nThe person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nAlthough clues 3 and 5 establish that Colonel Mustard was the only person in the observatory and that the person in the observatory had the candlestick, the model fails to combine them into a correct answer of (a) Yes.\n\nHowever, instead of asking for the answer directly, we can split the task into three pieces:\n\nFirst, go through the clues one by one and consider whether the clue is potentially relevant\nSecond, combine the relevant clues to reason out the answer to the question\nThird, write the final answer: either (a), (b), or (c)\n\nUse the following clues to answer the following multiple-choice question, using the following procedure:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant\n(2) Second, combine the relevant clues to reason out the answer to the question\n(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\n\nClues:\nMiss Scarlett was the only person in the lounge.\nThe person with the pipe was in the kitchen.\nColonel Mustard was the only person in the observatory.\nProfessor Plum was not in the library nor the billiard room.\nThe person with the candlestick was in the observatory.\n\nQuestion: Was Colonel Mustard in the observatory with the candlestick?\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n(b) No; Colonel Mustard was not in the observatory with the candlestick\n(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\n\nSolution:\n(1) First, go through the clues one by one and consider whether the clue is potentially relevant:\n\nClue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.\nClue 2: The person with the pipe was in the kitchen. This clue is not relevant.\nClue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.\nClue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.\nClue 5: The person with the candlestick was in the observatory. This clue is relevant.\n\n(2) Second, combine the relevant clues to reason out the answer to the question:\nClue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.\n\n(3) Third, write the final answer:\n(a) Yes; Colonel Mustard was in the observatory with the candlestick\n\nBy giving the model more time and space to think, and guiding it along a reasoning plan, it's able to figure out the correct answer of (a) Yes.\n\nAnother benefit of splitting complex instructions into smaller subtasks is that it can help keep the model focused on each subtask.\n\nFor example, if we ask gpt-3.5-turbo-instruct to summarize a text in its original language, the model can lapse back into English:\n\nSummarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nSummary:\n\nThe text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.\n\nHowever, if we first ask the model to identify the language of the text, and then summarize the text, it becomes more reliable:\n\nFirst, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.\n\nText:\n\"\"\"\nLa estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \"hombre de Estado\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\n\"\"\"\n\nLanguage:\n\nSpanish\n\nLa estadística es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\n\nPrompt the model to explain before answering\n\nAnother powerful technique for improving the reliability of answers is to prompt the model to gradually reason out the answer rather than jumping immediately to the final answer. By 'thinking aloud' the model can be far more likely to arrive at the correct answer.\n\nZero-shot\n\nMethod\n\nPublished by Takeshi Kojima et al. in 2022, the easiest way to prompt a model to reason out the answer is to simply prepend answers with Let's think step by step. Figure 2 illustrates an example:\n\nzero-shot reasoning example\nSource: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).\n\nResults\n\nApplying this simple trick to the MultiArith math dataset, the authors found Let's think step by step quadrupled the accuracy, from 18% to 79%!\n\nzero-shot reasoning example\nSource: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).\n\nImplications\n\nAlthough the Let's think step by step trick works well on math problems, it's not effective on all tasks. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn't help with simple math problems or common sense questions, and presumably wouldn't help with many other non-reasoning tasks either.\n\nzero-shot reasoning example\nSource: Large Language Models are Zero-Shot Reasoners by Takeshi Kojima et al. (2022).\n\nTo learn more, read the full paper.\n\nIf you apply this technique to your own tasks, don't be afraid to experiment with customizing the instruction. Let's think step by step is rather generic, so you may find better performance with instructions that hew to a stricter format customized to your use case. For example, you can try more structured variants like First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.. And you can even give the model an example format to help keep it on track, e.g.:\n\nUsing the IRS guidance below, answer the following questions using this format:\n(1) For each criterion, determine whether it is met by the vehicle purchase\n{Criterion} Let's think step by step. {explanation} {yes or no, or if the question does not apply then N/A}.\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nIRS guidance:\n\"\"\"\nYou may be eligible for a federal tax credit under Section 30D if you purchased a car or truck that meets the following criteria:\nDoes the vehicle have at least four wheels?\nDoes the vehicle weigh less than 14,000 pounds?\nDoes the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source?\nWas the vehicle purchased in a year before 2022?\n  If so, has the manufacturer sold less than 200,000 qualifying vehicles? (Tesla and GM have sold more than 200,000 qualifying vehicles.)\nWas the vehicle purchased in a year after 2022?\n  If so, is the vehicle present in the following list of North American-assembled vehicles? (The only electric vehicles assembled in North America are the Audi Q5, BMW 330e, BMW X5, Chevrolet Bolt EUV, Chevrolet Bolt EV, Chrysler Pacifica PHEV, Ford Escape PHEV, Ford F Series, Ford Mustang MACH E, Ford Transit Van, GMC Hummer Pickup, GMC Hummer SUV, Jeep Grand Cherokee PHEV, Jeep Wrangler PHEV, Lincoln Aviator PHEV, Lincoln Corsair Plug-in, Lucid Air, Nissan Leaf, Rivian EDV, Rivian R1S, Rivian R1T, Tesla Model 3, Tesla Model S, Tesla Model X, Tesla Model Y, Volvo S60, BMW 330e, Bolt EV, Cadillac Lyriq, Mercedes EQS SUV, and Nissan Leaf.)\n\"\"\"\n\nQuestion: Can I claim a federal tax credit for my Toyota Prius Prime bought in 2021?\n\nSolution:\n\n(1) For each criterion, determine whether it is met by the vehicle purchase\nDoes the vehicle have at least four wheels? Let's think step by step.\n\n The Toyota Prius Prime has four wheels, so the answer is yes.\nDoes the vehicle weigh less than 14,000 pounds? Let's think step by step. The Toyota Prius Prime weighs less than 14,000 pounds, so the answer is yes.\nDoes the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source? Let's think step by step. The Toyota Prius Prime has a battery with at least 4 kilowatt hours that may be recharged from an external source, so the answer is yes.\nWas the vehicle purchased in a year before 2022? Let's think step by step. The Toyota Prius Prime was purchased in 2021, which is before 2022, so the answer is yes.\nWas the vehicle purchased in a year after 2022? N/A\nIf so, is the vehicle present in the following list of North American-assembled vehicles? N/A\n\n(2) After considering each criterion in turn, phrase the final answer as \"Because of {reasons}, the answer is likely {yes or no}.\"\n\nBecause the Toyota Prius Prime meets all of the criteria for a federal tax credit, the answer is likely yes.\n\nFew-shot examples\n\nMethod\n\nPrompting the model to reason out its answers can be done in many ways. One way is to demonstrate with a few examples ('few-shot'), as studied by Jason Wei and Denny Zhou et al. from Google. Here's an example few-shot chain-of-thought prompt:\n\nchain of thought example\nSource: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)\n\nMore demonstrations of reasoning chains written by human labelers:\n\nchain of thought example\nSource: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)\n\n(Note that it has been called into question whether pears actually float)\n\nResults\n\nTesting on grade school math problems, the authors found that chain of thought prompting tripled the solve rate, from 18% to 57%.\n\nchain of thought example\nSource: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)\n\nIn addition to math problems, chain of thought prompting also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation. In most cases, not many examples were need to saturate the performance gains (less than 8 or so).\n\nchain of thought example\nSource: Chain of Thought Prompting Elicits Reasoning in Large Language Models Jason Wei and Denny Zhou et al. (2022)\n\nTo learn more, read the full paper.\n\nImplications\n\nOne advantage of the few-shot example-based approach relative to the Let's think step by step technique is that you can more easily specify the format, length, and style of reasoning that you want the model to perform before landing on its final answer. This can be particularly helpful in cases where the model isn't initially reasoning in the right way or depth.\n\nFine-tuned\n\nMethod\n\nIn general, to eke out maximum performance on a task, you'll need to fine-tune a custom model. However, fine-tuning a model using explanations may take thousands of example explanations, which are costly to write.\n\nIn 2022, Eric Zelikman and Yuhuai Wu et al. published a clever procedure for using a few-shot prompt to generate a dataset of explanations that could be used to fine-tune a model. The idea is to use a few-shot prompt to generate candidate explanations, and only keep the explanations that produce the correct answer. Then, to get additional explanations for some of the incorrect answers, retry the few-shot prompt but with correct answers given as part of the question. The authors called their procedure STaR (Self-taught Reasoner):\n\nSTaR procedure\nSource: STaR: Bootstrapping Reasoning With Reasoning by Eric Zelikman and Yujuai Wu et al. (2022)\n\nWith this technique, you can combine the benefits of fine-tuning with the benefits of chain-of-thought prompting without needing to write thousands of example explanations.\n\nResults\n\nWhen the authors applied this technique to a Common Sense Q\u0026A dataset, they found that STaR outperformed both chain-of-thought prompting alone (73% \u003e 37%) and fine-tuning alone (73% \u003e 60%):\n\nSTaR results\nSource: STaR: Bootstrapping Reasoning With Reasoning by Eric Zelikman and Yujuai Wu et al. (2022)\n\nTo learn more, read the full paper.\n\nImplications\n\nUsing a few-shot prompt to extend or modify a fine-tuning dataset is an idea that can be generalized beyond explanation writing. For example, if you have large quantities of unstructured text that you want to train on, you may find opportunities to use a prompt to extract a structured dataset from your unstructured text, and then fine-tune a custom model on that structured dataset.\n\nExtensions to chain-of-thought prompting\n\nA number of extensions of chain-of-thought prompting have been published as well.\n\nSelection-inference prompting\n\nMethod\n\nPublished by Antonia Creswell et al., one extension of the chain-of-thought technique is to split the single prompt for generating explanations and answers into smaller parts. First, a prompt selects a relevant subset of facts from the text ('selection prompt'). Then, a second prompt infers a conclusion from the selected facts ('inference prompt'). These prompts are then alternated in a loop to generate multiple steps of reasoning and eventually land on a final answer. The authors illustrate the idea in the following figure:\n\nSelection-inference prompting\nSource: Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning by Antonia Creswell et al. (2022)\n\nResults\n\nWhen applied to a 7B-parameter model, the authors found that selection-inference prompting substantially improved performance relative to chain-of-thought prompting on the bAbi and Proof Writer benchmark tasks (both of which require longer sequences of reasoning steps). The best performance they achieved combined both selection-inference prompting with fine-tuning.\n\nSelection-inference prompting\nSource: Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning by Antonia Creswell et al. (2022)\n\nImplications\n\nAlthough the gains on these benchmarks were large, these benchmarks were specifically chosen because they required longer sequences of reasoning. On problems that don't require reasoning with many steps, the gains are likely smaller.\n\nThe results highlight a couple of general lessons for working with large language models. One, splitting up complex tasks into smaller tasks is a great way to improve reliability and performance; the more atomic the task, the less room there is for the model to err. Two, getting maximum performance often means combining fine-tuning with whatever approach you've chosen.\n\nTo learn more, read the full paper.\n\nFaithful reasoning architecture\n\nA few months after publishing the selection-inference prompting technique, the authors extended the technique in a follow-up paper, with ideas for:\n\nfiguring out when the selection-inference cycle should stop or continue\nadding a value function to help search over multiple reasoning paths\nreducing hallucination of fake facts by fine-tuning a model to reason about sentence labels (e.g., sen1) rather than writing out the sentences themselves\n\nMethod\n\nIn the original selection-inference technique, specialized 'selection' and 'inference' prompts are alternated to select facts and make inferences from those facts, combining to generate a sequence of reasoning steps.\n\nThe authors extend this technique with two additional components.\n\nFirst, the authors add a 'halter' model that, after each inference step, is asked whether the inferences thus far are sufficient to answer the question. If yes, then the model generates a final answer.\n\nThe halter models brings a couple of advantages:\n\nit can tell the selection-inference process to stop or keep going, as necessary.\nif the process never halts, you'll get no answer, which is often preferable to a hallucinated guess\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)\n\nSecond, the authors add a value function, which is used to assess the quality of reasoning steps and search over multiple reasoning trajectories. This echoes a common theme for increasing reliability; instead of generating a single answer from the model, generate a set of answers and then use some type of value function / discriminator / verifier model to pick the best one.\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)\n\nIn addition to these two extensions, the authors also use a trick to reduce hallucination of fake facts. Rather than asking the model to write out factual sentences, they fine-tune a model to work with sentence labels (e.g., sen1) instead. This helps prevent the model from hallucinating fake facts not mentioned in the prompt context.\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)\n\nResults\n\nThe authors evaluated their technique on two benchmarks: the ProofWriter task (not shown) and EntailmentBankQA (shown). The technique increased accuracy substantially, especially on harder reasoning problems.\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\nIn addition, their sentence label manipulation trick essentially eliminated hallucination!\n\nFaithful reasoning\nSource: Faithful Reasoning Using Large Language Models by Antonia Creswell et al. (2022)](https://arxiv.org/abs/2208.14271)\n\nImplications\n\nThis paper illustrates a number of helpful lessons for improving the reliability of large language models:\n\nSplit complex tasks into smaller, more reliable subtasks\nGenerate your answer in a step-by-step fashion, evaluating it along the way\nGenerate many possible answers and use another model or function to pick the ones that look best\nReduce hallucination by constraining what the model can say (e.g., by using sentence labels instead of sentences)\nMaximize performance of models by fine-tuning them on specialized tasks\n\nTo learn more, read the full paper.\n\nLeast-to-most prompting\n\nIn addition to doing poorly on long reasoning chains (where selection-inference shines), chain-of-thought prompting can especially struggle when the examples are short but the task is long.\n\nMethod\n\nLeast-to-most prompting is another technique that splits up reasoning tasks into smaller, more reliable subtasks. The idea is to elicit a subtask from the model by prompting it with something like To solve {question}, we need to first solve: \". Then, with that subtask in hand, the model can generate a solution. The solution is appended to the original question and the process is repeated until a final answer is produced.\n\nLeast-to-most prompting\nSource: Least-to-most Prompting Enables Complex Reasoning in Large Language Models by Denny Zhou et al. (2022)\n\nResults\n\nWhen applied to benchmarks involving long reasoning chains using code-davinci-002 (which is optimized for code but can still understand text), the authors measured gains as large as 16% -\u003e 99.7%!\n\n\nLeast-to-most prompting results on last-letter-concatenation task\nLeast-to-most prompting results on SCAN\nLeast-to-most prompting results on DROP numerical reasoning\nSource: Least-to-most Prompting Enables Complex Reasoning in Large Language Models by Denny Zhou et al. (2022)\n\nImplications\n\nAlthough the above gains from least-to-most prompting are impressive, they are measured on a very narrow set of tasks that require long reasoning chains.\n\nStill, they illustrate a common theme: increase reliability by (a) breaking complex tasks into smaller subtasks and (b) giving the model more time and space to work out the answer.\n\nTo learn more, read the full paper.\n\nRelated ideas\n\nMaieutic prompting\n\nMethod\n\nIn contrast to the previous techniques, which try to maximize the likelihood of correct answers, another approach is to use GPT-3 to generate a tree of possible explanations (both correct and incorrect), and then analyze their relationships to guess at which set is correct. This technique was coined maieutic prompting by Jaehun Jung et al. in May 2022 (maieutic means relating to the Socratic method of asking questions to elicit ideas).\n\nThe method is complicated, and works as follows:\n\nFirst, build a maieutic tree, where each node is a statement that could be true or false:\n  Start with a multiple-choice question or true/false statement (e.g. War cannot have a tie)\n  For each possible answer to the question, use the model to generate a corresponding explanation (with a prompt like War cannot have a tie? True, because)\n  Then, prompt the model with the question and the generated explanation, and ask it to produce the answer. If reversing the explanation (with a prefix like It is wrong to say that {explanation}) reverses the answer, then the explanation is considered 'logically integral.'\n  If an explanation is not logically integral, then repeat the above process recursively, with each explanation turned into a True or False question, and generate more explanations for each new question.\n  After all of the recursive explaining is done, you end up with a tree of explanations, where each leaf on the tree has the property that reversing the explanation reverses the model's answer.\nSecond, convert the tree into a graph of relations:\n  For each node in the tree, calculate the model's relative belief in each node (inferred from the probability of getting an answer of True to given an explanation)\n  For each pair of nodes in the tree, use the model to identify whether they are entailed (implied) or contradicted\nThird, find the most consistent set of beliefs and take those to be true:\n  Specifically, using the strength of belief in each node and the logical relationships between them, formulate the problem as a weighted maximum satisfiability problem (MAX-SAT)\n  Use a solver to the find the most self-consistent set of beliefs, and take those as true\n\n\nMaieutic prompting\nMaieutic prompting\nSource: Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations by Jaehun Jung et al. (2022)\n\nResults\n\nMaieutic prompting results\nSource: Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations by Jaehun Jung et al. (2022)\n\nImplications\n\nBeyond the complexity, one limitation of this method is that it appears to only apply to questions that can be posed as multiple-choice.\n\nTo learn more, read the full paper.\n\nExtensions\n\nSelf-consistency\n\nMethod\n\nFor tasks with a discrete set of answers, one simple way to improve reliability is to sample multiple explanations \u0026 answers from the model (using a positive temperature) and then pick the final answer that appears most often.\n\nSelf-consistency method\nSource: Self-Consistency Improves Chain of Thought Reasoning in Language Models by Xuezhi Wang et al. (2022)\n\nResults\n\nThis technique lifted accuracies by anywhere from 1 to 24 percentage points on a suite of math and reasoning benchmarks. (Plotted below are results from Google's LaMDA model; using Google's larger PaLM model, the baselines were higher but the gains were a bit smaller.)\n\nSelf-consistency results\nSource: Self-Consistency Improves Chain of Thought Reasoning in Language Models by Xuezhi Wang et al. (2022)\n\nImplications\n\nAlthough this technique is simple to implement, it can be costly. Generating a set of 10 answers will increase your costs by 10x.\n\nAlso, as with many of these techniques, it applies only to tasks with a limited set of answers. For open-ended tasks where each answer is unique (such as writing a poem), it's not obvious what it would mean to pick the most common answer.\n\nLastly, this technique ought to be most beneficial when there are multiple paths or phrasings to reach an answer; if there's only one path, then the technique may not help at all. An extreme example: If the task was to generate a single token answer, then taking the most common token from 100 generations would be no different than taking the token with the highest logprobs (which you can get with a single generation at temperature=0).\n\nVerifiers\n\nAnother key technique for improving task performance is to train a verifier or discriminator model to evaluate the outputs of the main generative model. If the discriminator rejects the output, then you can resample the generative model until you get an acceptable output. In many cases, it's easier to judge an answer than it is to create an answer, which helps explain the power of this method.\n\nMethod\n\nIn 2021, OpenAI researchers applied this technique to grade school math problems, using the following procedure:\n\nFirst, they fine-tuned a model on questions and solutions\nFor each problem in the training set, they generated 100 solutions\nEach of those 100 solutions was automatically labeled as either correct or incorrect, based on whether the final answer was correct\nUsing those solutions, with some labeled correct and some labeled incorrect, they fine-tuned a verifier model to classify whether a question and candidate solution was correct or incorrect\nFinally, at test time, the generative model creates 100 solutions to each problem, and the one with the highest score according to the verifier model is picked as the final answer\n\nVerifier method\nSource: Training Verifiers to Solve Math Word Problems by Karl Cobbe et al. (2021)\n\nResults\n\nWith a 175B GPT-3 model and 8,000 training examples, this technique substantially lifted grade school math accuracy from 33% to 55%.\n\nVerifier results\nSource: Training Verifiers to Solve Math Word Problems by Karl Cobbe et al. (2021)\n\nImplications\n\nSimilar to the self-consistency technique, this method can get expensive, as generating, say, 100 solutions per task will increase your costs by roughly ~100x.\n\nTheories of reliability\n\nAlthough the techniques above vary in their approach, they all share the goal of improving reliability on complex tasks. Mainly they do this by:\n\ndecomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting)\nusing multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting)\n\nProbabilistic graphical models\n\nThis paradigm of trying to build a reliable system out of less reliable components is reminiscent of probabilistic programming, and many of the analysis techniques of that field can be applied to this one.\n\nIn the paper Language Model Cascades, David Dohan et al. interpret the above techniques in the paradigm of probabilistic graphical models:\n\nChain of thought prompting\n\ngraphical model of chain of thought prompting\nSource: Language Model Cascades by David Dohan et al. (2022)\n\nFine-tuned chain of thought prompting / Self-taught reasoner\n\ngraphical model of fine-tuned chain of thought prompting\nSource: Language Model Cascades by David Dohan et al. (2022)\n\nSelection-inference prompting\n\ngraphical model of selection-inference prompting\nSource: Language Model Cascades by David Dohan et al. (2022)\n\nVerifiers\n\ngraphical model of verifiers\nSource: Language Model Cascades by David Dohan et al. (2022)\n\nImplications\n\nAlthough formulating these techniques as probabilistic graphical models may not be immediately useful for solving any particular problem, the framework may be helpful in selecting, combining, and discovering new techniques.\n\nClosing thoughts\n\nResearch into large language models is very active and evolving rapidly. Not only do researchers continue to improve the models, they also continue to improve our understanding of how to best employ the models. To underscore the pace of these developments, note that all of the papers shared above were published within the past 12 months (as I write in Sep 2022).\n\nIn the future, expect better models and better techniques to be published. Even if the specific techniques here are eclipsed by future best practices, the general principles behind them will likely remain a key part of any expert user's toolkit.\n\nBibliography\n\n| Lesson                                                                                                                         | Paper                                                                                                                                     | Date     |\n| ------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- | -------- |\n| Break complex tasks into simpler subtasks (and consider exposing the intermediate outputs to users)                            | AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts | 2021 Oct |\n| You can improve output by generating many candidates, and then picking the one that looks best                                 | Training Verifiers to Solve Math Word Problems                                                        | 2021 Oct |\n| On reasoning tasks, models do better when they reason step-by-step before answering                                            | Chain of Thought Prompting Elicits Reasoning in Large Language Models                                 | 2022 Jan |\n| You can improve step-by-step reasoning by generating many explanation-answer outputs, and picking the most popular answer      | Self-Consistency Improves Chain of Thought Reasoning in Language Models                               | 2022 Mar |\n| If you want to fine-tune a step-by-step reasoner, you can do it with multiple-choice question \u0026 answer data alone              | STaR: Bootstrapping Reasoning With Reasoning                                                          | 2022 Mar |\n| The step-by-step reasoning method works great even with zero examples                                                          | Large Language Models are Zero-Shot Reasoners                                                         | 2022 May |\n| You can do better than step-by-step reasoning by alternating a ‘selection’ prompt and an ‘inference’ prompt                    | Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning             | 2022 May |\n| On long reasoning problems, you can improve step-by-step reasoning by splitting the problem into pieces to solve incrementally | Least-to-most Prompting Enables Complex Reasoning in Large Language Models                            | 2022 May |\n| You can have the model analyze both good and bogus explanations to figure out which set of explanations are most consistent    | Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations                        | 2022 May |\n| You can think about these techniques in terms of probabilistic programming, where systems comprise unreliable components       | Language Model Cascades                                                                               | 2022 Jul |\n| You can eliminate hallucination with sentence label manipulation, and you can reduce wrong answers with a 'halter' prompt      | Faithful Reasoning Using Large Language Models                                                        | 2022 Aug |\n"])</script><script>self.__next_f.push([1,"76:T1e4c,"])</script><script>self.__next_f.push([1,"\nHow large language models work\n\nLarge language models are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next.\n\nThe magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn:\n\nhow to spell\nhow grammar works\nhow to paraphrase\nhow to answer questions\nhow to hold a conversation\nhow to write in many languages\nhow to code\netc.\n\nThey do this by “reading” a large amount of existing text and learning how words tend to appear in context with other words, and uses what it has learned to predict the next most likely word that might appear in response to a user request, and each subsequent word after that.\n\nGPT-3 and GPT-4 power many software products, including productivity apps, education apps, games, and more.\n\nHow to control a large language model\n\nOf all the inputs to a large language model, by far the most influential is the text prompt.\n\nLarge language models can be prompted to produce output in a few ways:\n\nInstruction**: Tell the model what you want\nCompletion**: Induce the model to complete the beginning of what you want\nScenario**: Give the model a situation to play out\nDemonstration**: Show the model what you want, with either:\n  A few examples in the prompt\n  Many hundreds or thousands of examples in a fine-tuning training dataset\n\nAn example of each is shown below.\n\nInstruction prompts\n\nWrite your instruction at the top of the prompt (or at the bottom, or both), and the model will do its best to follow the instruction and then stop. Instructions can be detailed, so don't be afraid to write a paragraph explicitly detailing the output you want, just stay aware of how many tokens the model can process.\n\nExample instruction prompt:\n\nExtract the name of the author from the quotation below.\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\nOutput:\n\nTed Chiang\n\nCompletion prompt example\n\nCompletion-style prompts take advantage of how large language models try to write text they think is mostly likely to come next. To steer the model, try beginning a pattern or sentence that will be completed by the output you want to see. Relative to direct instructions, this mode of steering large language models can take more care and experimentation. In addition, the models won't necessarily know where to stop, so you will often need stop sequences or post-processing to cut off text generated beyond the desired output.\n\nExample completion prompt:\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\nThe author of this quote is\n\nOutput:\n\n Ted Chiang\n\nScenario prompt example\n\nGiving the model a scenario to follow or role to play out can be helpful for complex queries or when seeking imaginative responses. When using a hypothetical prompt, you set up a situation, problem, or story, and then ask the model to respond as if it were a character in that scenario or an expert on the topic.\n\nExample scenario prompt:\n\nYour role is to extract the name of the author from any given text\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\nOutput:\n\n Ted Chiang\n\nDemonstration prompt example (few-shot learning)\n\nSimilar to completion-style prompts, demonstrations can show the model what you want it to do. This approach is sometimes called few-shot learning, as the model learns from a few examples provided in the prompt.\n\nExample demonstration prompt:\n\nQuote:\n“When the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.”\n― N.K. Jemisin, The Fifth Season\nAuthor: N.K. Jemisin\n\nQuote:\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\nAuthor:\n\nOutput:\n\n Ted Chiang\n\nFine-tuned prompt example\n\nWith enough training examples, you can fine-tune a custom model. In this case, instructions become unnecessary, as the model can learn the task from the training data provided. However, it can be helpful to include separator sequences (e.g., -\u003e or ### or any string that doesn't commonly appear in your inputs) to tell the model when the prompt has ended and the output should begin. Without separator sequences, there is a risk that the model continues elaborating on the input text rather than starting on the answer you want to see.\n\nExample fine-tuned prompt (for a model that has been custom trained on similar prompt-completion pairs):\n\n“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n― Ted Chiang, Exhalation\n\nOutput:\n\n Ted Chiang\n\nCode Capabilities\n\nLarge language models aren't only great at text - they can be great at code too. OpenAI's GPT-4 model is a prime example.\n\nGPT-4 powers numerous innovative products, including:\n\n[GitHub Copilot] (autocompletes code in Visual Studio and other IDEs)\nReplit (can complete, explain, edit and generate code)\nCursor (build software faster in an editor designed for pair-programming with AI)\n\nGPT-4 is more advanced than previous models like gpt-3.5-turbo-instruct. But, to get the best out of GPT-4 for coding tasks, it's still important to give clear and specific instructions. As a result, designing good prompts can take more care.\n\nMore prompt advice\n\nFor more prompt examples, visit OpenAI Examples.\n\nIn general, the input prompt is the best lever for improving model outputs. You can try tricks like:\n\nBe more specific** E.g., if you want the output to be a comma separated list, ask it to return a comma separated list. If you want it to say \"I don't know\" when it doesn't know the answer, tell it 'Say \"I don't know\" if you do not know the answer.' The more specific your instructions, the better the model can respond.\nProvide Context**: Help the model understand the bigger picture of your request. This could be background information, examples/demonstrations of what you want or explaining the purpose of your task.\nAsk the model to answer as if it was an expert.** Explicitly asking the model to produce high quality output or output as if it was written by an expert can induce the model to give higher quality answers that it thinks an expert would write. Phrases like \"Explain in detail\" or \"Describe step-by-step\" can be effective.\nPrompt the model to write down the series of steps explaining its reasoning.** If understanding the 'why' behind an answer is important, prompt the model to include its reasoning. This can be done by simply adding a line like \"Let's think step by step\" before each answer.\n\n[Fine Tuning Docs]: https://platform.openai.com/docs/guides/fine-tuning\n[OpenAI Customer Stories]: https://openai.com/customer-stories\n[Large language models Blog Post]: https://openai.com/research/better-language-models\n[GitHub Copilot]: https://github.com/features/copilot/\n[GPT-4 and GPT-4 Turbo]: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n[GPT3 Apps Blog Post]: https://openai.com/blog/gpt-3-apps/\n[OpenAI Examples]: https://platform.openai.com/examples\n"])</script><script>self.__next_f.push([1,"77:Tbb8,"])</script><script>self.__next_f.push([1,"\nThe OpenAI API embeddings endpoint can be used to measure relatedness or similarity between pieces of text.\n\nBy leveraging GPT-3's understanding of text, these embeddings achieved state-of-the-art results on benchmarks in unsupervised learning and transfer learning settings.\n\nEmbeddings can be used for semantic search, recommendations, cluster analysis, near-duplicate detection, and more.\n\nFor more information, read OpenAI's blog post announcements:\n\nIntroducing Text and Code Embeddings (Jan 2022)\nNew and Improved Embedding Model (Dec 2022)\n\nFor comparison with other embedding models, see Massive Text Embedding Benchmark (MTEB) Leaderboard\n\nSemantic search\n\nEmbeddings can be used for search either by themselves or as a feature in a larger system.\n\nThe simplest way to use embeddings for search is as follows:\n\nBefore the search (precompute):\n  Split your text corpus into chunks smaller than the token limit (8,191 tokens for text-embedding-3-small)\n  Embed each chunk of text\n  Store those embeddings in your own database or in a vector search provider like Pinecone, Weaviate or Qdrant\nAt the time of the search (live compute):\n  Embed the search query\n  Find the closest embeddings in your database\n  Return the top results\n\nAn example of how to use embeddings for search is shown in Semantic_text_search_using_embeddings.ipynb.\n\nIn more advanced search systems, the cosine similarity of embeddings can be used as one feature among many in ranking search results.\n\nQuestion answering\n\nThe best way to get reliably honest answers from GPT-3 is to give it source documents in which it can locate correct answers. Using the semantic search procedure above, you can cheaply search through a corpus of documents for relevant information and then give that information to GPT-3 via the prompt to answer a question. We demonstrate this in Question_answering_using_embeddings.ipynb.\n\nRecommendations\n\nRecommendations are quite similar to search, except that instead of a free-form text query, the inputs are items in a set.\n\nAn example of how to use embeddings for recommendations is shown in Recommendation_using_embeddings.ipynb.\n\nSimilar to search, these cosine similarity scores can either be used on their own to rank items or as features in larger ranking algorithms.\n\nCustomizing Embeddings\n\nAlthough OpenAI's embedding model weights cannot be fine-tuned, you can nevertheless use training data to customize embeddings to your application.\n\nIn Customizing_embeddings.ipynb, we provide an example method for customizing your embeddings using training data. The idea of the method is to train a custom matrix to multiply embedding vectors by in order to get new customized embeddings. With good training data, this custom matrix will help emphasize the features relevant to your training labels. You can equivalently consider the matrix multiplication as (a) a modification of the embeddings or (b) a modification of the distance function used to measure the distances between embeddings.\n"])</script><script>self.__next_f.push([1,"78:T1ab0,"])</script><script>self.__next_f.push([1,"\nPeople are writing great tools and papers for improving outputs from GPT. Here are some cool ones we've seen:\n\nPrompting libraries \u0026 tools (in alphabetical order)\n\nArthur Shield: A paid product for detecting toxicity, hallucination, prompt injection, etc.\nBaserun: A paid product for testing, debugging, and monitoring LLM-based apps\nChainlit: A Python library for making chatbot interfaces.\nEmbedchain: A Python library for managing and syncing unstructured data with LLMs.\nFLAML (A Fast Library for Automated Machine Learning \u0026 Tuning): A Python library for automating selection of models, hyperparameters, and other tunable choices.\nGuidance: A handy looking Python library from Microsoft that uses Handlebars templating to interleave generation, prompting, and logical control.\nHaystack: Open-source LLM orchestration framework to build customizable, production-ready LLM applications in Python.\nHoneyHive: An enterprise platform to evaluate, debug, and monitor LLM apps.\nLangChain: A popular Python/JavaScript library for chaining sequences of language model prompts.\nLiteLLM: A minimal Python library for calling LLM APIs with a consistent format.\nLlamaIndex: A Python library for augmenting LLM apps with data.\nLMQL: A programming language for LLM interaction with support for typed prompting, control flow, constraints, and tools.\nOpenAI Evals: An open-source library for evaluating task performance of language models and prompts.\nOutlines: A Python library that provides a domain-specific language to simplify prompting and constrain generation.\nParea AI: A platform for debugging, testing, and monitoring LLM apps.\nPortkey: A platform for observability, model management, evals, and security for LLM apps.\nPromptify: A small Python library for using language models to perform NLP tasks.\nPromptPerfect: A paid product for testing and improving prompts.\nPrompttools: Open-source Python tools for testing and evaluating models, vector DBs, and prompts.\nScale Spellbook: A paid product for building, comparing, and shipping language model apps.\nSemantic Kernel: A Python/C#/Java library from Microsoft that supports prompt templating, function chaining, vectorized memory, and intelligent planning.\nVellum: A paid AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\nWeights \u0026 Biases: A paid product for tracking model training and prompt engineering experiments.\nYiVal: An open-source GenAI-Ops tool for tuning and evaluating prompts, retrieval configurations, and model parameters using customizable datasets, evaluation methods, and evolution strategies.\n\nPrompting guides\n\nBrex's Prompt Engineering Guide: Brex's introduction to language models and prompt engineering.\nlearnprompting.org: An introductory course to prompt engineering.\nLil'Log Prompt Engineering: An OpenAI researcher's review of the prompt engineering literature (as of March 2023).\nOpenAI Cookbook: Techniques to improve reliability: A slightly dated (Sep 2022) review of techniques for prompting language models.\npromptingguide.ai: A prompt engineering guide that demonstrates many techniques.\nXavi Amatriain's Prompt Engineering 101 Introduction to Prompt Engineering and 202 Advanced Prompt Engineering: A basic but opinionated introduction to prompt engineering and a follow up collection with many advanced methods starting with CoT.\n\nVideo courses\n\nAndrew Ng's DeepLearning.AI: A short course on prompt engineering for developers.\nAndrej Karpathy's Let's build GPT: A detailed dive into the machine learning underlying GPT.\nPrompt Engineering by DAIR.AI: A one-hour video on various prompt engineering techniques.\nScrimba course about Assistants API: A 30-minute interactive course about the Assistants API.\nLinkedIn course: Introduction to Prompt Engineering: How to talk to the AIs: Short video introduction to prompt engineering\n\nPapers on advanced prompting to improve reasoning\n\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022): Using few-shot prompts to ask models to think step by step improves their reasoning. PaLM's score on math word problems (GSM8K) rises from 18% to 57%.\nSelf-Consistency Improves Chain of Thought Reasoning in Language Models (2022): Taking votes from multiple outputs improves accuracy even more. Voting across 40 outputs raises PaLM's score on math word problems further, from 57% to 74%, and code-davinci-002's from 60% to 78%.\nTree of Thoughts: Deliberate Problem Solving with Large Language Models (2023): Searching over trees of step by step reasoning helps even more than voting over chains of thought. It lifts GPT-4's scores on creative writing and crosswords.\nLanguage Models are Zero-Shot Reasoners (2022): Telling instruction-following models to think step by step improves their reasoning. It lifts text-davinci-002's score on math word problems (GSM8K) from 13% to 41%.\nLarge Language Models Are Human-Level Prompt Engineers (2023): Automated searching over possible prompts found a prompt that lifts scores on math word problems (GSM8K) to 43%, 2 percentage points above the human-written prompt in Language Models are Zero-Shot Reasoners.\nReprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling (2023): Automated searching over possible chain-of-thought prompts improved ChatGPT's scores on a few benchmarks by 0–20 percentage points.\nFaithful Reasoning Using Large Language Models (2022): Reasoning can be improved by a system that combines: chains of thought generated by alternative selection and inference prompts, a halter model that chooses when to halt selection-inference loops, a value function to search over multiple reasoning paths, and sentence labels that help avoid hallucination.\nSTaR: Bootstrapping Reasoning With Reasoning (2022): Chain of thought reasoning can be baked into models via fine-tuning. For tasks with an answer key, example chains of thoughts can be generated by language models.\nReAct: Synergizing Reasoning and Acting in Language Models (2023): For tasks with tools or an environment, chain of thought works better if you prescriptively alternate between Reasoning steps (thinking about what to do) and Acting (getting information from a tool or environment).\nReflexion: an autonomous agent with dynamic memory and self-reflection (2023): Retrying tasks with memory of prior failures improves subsequent performance.\nDemonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP (2023): Models augmented with knowledge via a \"retrieve-then-read\" can be improved with multi-hop chains of searches.\nImproving Factuality and Reasoning in Language Models through Multiagent Debate (2023): Generating debates between a few ChatGPT agents over a few rounds improves scores on various benchmarks. Math word problem scores rise from 77% to 85%.\n"])</script><script>self.__next_f.push([1,"79:T3ec1,"])</script><script>self.__next_f.push([1,"\nThe aim of this notebook is to walk through a comprehensive example of how to fine-tune OpenAI models for Retrieval Augmented Generation (RAG).\n\nWe will also be integrating Qdrant and Few-Shot Learning to boost the model's performance and reduce hallucinations. This could serve as a practical guide for ML practitioners, data scientists, and AI Engineers interested in leveraging the power of OpenAI models for specific use-cases. 🤩\n\nWhy should you read this blog?\n\nYou want to learn how to\nFine-tune OpenAI models for specific use-cases\nUse Qdrant to improve the performance of your RAG model\nUse fine-tuning to improve the correctness of your RAG model and reduce hallucinations\n\nTo begin, we've selected a dataset where we've a guarantee that the retrieval is perfect. We've selected a subset of the SQuAD dataset, which is a collection of questions and answers about Wikipedia articles. We've also included samples where the answer is not present in the context, to demonstrate how RAG handles this case.\n\nTable of Contents\nSetting up the Environment\n\nSection A: Zero-Shot Learning\nData Preparation: SQuADv2 Dataset\nAnswering using Base gpt-3.5-turbo-0613 model\nFine-tuning and Answering using Fine-tuned model\nEvaluation: How well does the model perform?\n\nSection B: Few-Shot Learning\n\nUsing Qdrant to Improve RAG Prompt\nFine-Tuning OpenAI Model with Qdrant\nEvaluation\n\nConclusion\n    Aggregate Results\n    Observations\nTerms, Definitions, and References\n\nRetrieval Augmented Generation (RAG)?\nThe phrase Retrieval Augmented Generation (RAG) comes from a recent paper by Lewis et al. from Facebook AI. The idea is to use a pre-trained language model (LM) to generate text, but to use a separate retrieval system to find relevant documents to condition the LM on.\n\nWhat is Qdrant?\nQdrant is an open-source vector search engine that allows you to search for similar vectors in a large dataset. It is built in Rust and here we'll use the Python client to interact with it. This is the Retrieval part of RAG.\n\nWhat is Few-Shot Learning?\nFew-shot learning is a type of machine learning where the model is \"improved\" via training or fine-tuning on a small amount of data. In this case, we'll use it to fine-tune the RAG model on a small number of examples from the SQuAD dataset. This is the Augmented part of RAG.\n\nWhat is Zero-Shot Learning?\nZero-shot learning is a type of machine learning where the model is \"improved\" via training or fine-tuning without any dataset specific information.\n\nWhat is Fine-Tuning?\nFine-tuning is a type of machine learning where the model is \"improved\" via training or fine-tuning on a small amount of data. In this case, we'll use it to fine-tune the RAG model on a small number of examples from the SQuAD dataset. The LLM is what makes the Generation part of RAG.\n1. Setting Up the Environment\n\nInstall and Import Dependencies\nSet your keys\nGet your OpenAI keys here and Qdrant keys after making a free cluster here.\nSection A\n\n2. Data Preparation: SQuADv2 Data Subsets\n\nFor the purpose of demonstration, we'll make small slices from the train and validation splits of the SQuADv2 dataset. This dataset has questions and contexts where the answer is not present in the context, to help us evaluate how LLM handles this case.\n\nWe'll read the data from the JSON files and create a dataframe with the following columns: question, context, answer, is_impossible.\n\nDownload the Data\nRead JSON to DataFrame\n3. Answering using Base gpt-3.5-turbo-0613 model\n\n3.1 Zero Shot Prompt\n\nLet's start by using the base gpt-3.5-turbo-0613 model to answer the questions. This prompt is a simple concatenation of the question and context, with a separator token in between: \\n\\n. We've a simple instruction part of the prompt:\nAnswer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n\nOther prompts are possible, but this is a good starting point. We'll use this prompt to answer the questions in the validation set.\n3.2 Answering using Zero Shot Prompt\n\nNext, you'll need some re-usable functions which make an OpenAI API Call and return the answer. You'll use the ChatCompletion.create endpoint of the API, which takes a prompt and returns the completed text.\n⏰ Time to run: ~3 min, 🛜 Needs Internet Connection\n4. Fine-tuning and Answering using Fine-tuned model\n\nFor the complete fine-tuning process, please refer to the OpenAI Fine-Tuning Docs.\n\n4.1 Prepare the Fine-Tuning Data\n\nWe need to prepare the data for fine-tuning. We'll use a few samples from train split of same dataset as before, but we'll add the answer to the context. This will help the model learn to retrieve the answer from the context.\n\nOur instruction prompt is the same as before, and so is the system prompt.\nTip: 💡 Verify the Fine-Tuning Data\n\nYou can see this cookbook for more details on how to prepare the data for fine-tuning.\n\n4.2 Fine-Tune OpenAI Model\n\nIf you're new to OpenAI Model Fine-Tuning, please refer to the How to finetune Chat models notebook. You can also refer to the OpenAI Fine-Tuning Docs for more details.\n⏰ Time to run: ~10-20 minutes, 🛜 Needs Internet Connection\n4.2.1 Try out the Fine-Tuned Model\n\nLet's try out the fine-tuned model on the same validation set as before. You'll use the same prompt as before, but you will use the fine-tuned model instead of the base model. Before you do that, you can make a simple call to get a sense of how the fine-tuned model is doing.\n4.3 Answer Using the Fine-Tuned Model\n\nThis is the same as before, but you'll use the fine-tuned model instead of the base model.\n\n⏰ Time to run: ~5 min, 🛜 Needs Internet Connection\n5. Evaluation: How well does the model perform?\n\nTo evaluate the model's performance, compare the predicted answer to the actual answers -- if any of the actual answers are present in the predicted answer, then it's a match. We've also created error categories to help you understand where the model is struggling.\n\nWhen we know that a correct answer exists in the context, we can measure the model's performance, there are 3 possible outcomes:\n\n✅ Answered Correctly: The model responded the correct answer. It may have also included other answers that were not in the context.\n❎ Skipped: The model responded with \"I don't know\" (IDK) while the answer was present in the context. It's better than giving the wrong answer. It's better for the model say \"I don't know\" than giving the wrong answer. In our design, we know that a true answer exists and hence we're able to measure it -- this is not always the case. This is a model error. We exclude this from the overall error rate.\n❌ Wrong: The model responded with an incorrect answer. This is a model ERROR.\n\nWhen we know that a correct answer does not exist in the context, we can measure the model's performance, there are 2 possible outcomes:\n\n❌ Hallucination: The model responded with an answer, when \"I don't know\" was expected. This is a model ERROR.\n✅ I don't know: The model responded with \"I don't know\" (IDK) and the answer was not present in the context. This is a model WIN.\nNotice that the fine-tuned model skips questions more often -- and makes fewer mistakes. This is because the fine-tuned model is more conservative and skips questions when it's not sure.\nNotice that the fine-tuned model has learnt to say \"I don't know\" a lot better than the prompt. Or, the model has gotten good at skipping questions.\n\nObservations\n\nThe fine-tuned model is better at saying \"I don't know\"\nHallucinations drop from 100% to 15% with fine-tuning\nWrong answers drop from 17% to 6% with fine-tuning\n\nCorrect answers also drop from 83% to 60% with fine-tuning - this is because the fine-tuned model is more conservative and says \"I don't know\" more often. This is a good thing because it's better to say \"I don't know\" than to give a wrong answer.\n\nThat said, we want to improve the correctness of the model, even if that increases the hallucinations. We're looking for a model that is both correct and conservative, striking a balance between the two. We'll use Qdrant and Few-Shot Learning to achieve this.\n💪 You're 2/3rds of the way there! Keep reading!\n\nSection B: Few Shot Learning\n\nWe'll select a few examples from the dataset, including cases where the answer is not present in the context. We'll then use these examples to create a prompt that we can use to fine-tune the model. We'll then measure the performance of the fine-tuned model.\n\nWhat is next?\n\nFine-Tuning OpenAI Model with Qdrant\n    6.1 Embed the Fine-Tuning Data\n    6.2 Embedding the Questions\nUsing Qdrant to Improve RAG Prompt\nEvaluation\n\n6. Fine-Tuning OpenAI Model with Qdrant\n\nSo far, we've been using the OpenAI model to answer questions without using examples of the answer. The previous step made it work better on in-context examples, while this one helps it generalize to unseen data, and attempt to learn when to say \"I don't know\" and when to give an answer.\n\nThis is where few-shot learning comes in!\n\nFew-shot learning is a type of transfer learning that allows us to answer questions where the answer is not present in the context. We can do this by providing a few examples of the answer we're looking for, and the model will learn to answer questions where the answer is not present in the context.\n5.1 Embed the Training Data\n\nEmbeddings are a way to represent sentences as an array of floats. We'll use the embeddings to find the most similar questions to the ones we're looking for.\nNow that we've the Qdrant imports in place,\n5.2 Embedding the Questions\n\nNext, you'll embed the entire training set questions. You'll use the question to question similarity to find the most similar questions to the question we're looking for. This is a workflow which is used in RAG to leverage the OpenAI model ability of incontext learning with more examples. This is what we call Few Shot Learning here.\n\n❗️⏰ Important Note: This step can take up to 3 hours to complete. Please be patient. If you see Out of Memory errors or Kernel Crashes, please reduce the batch size to 32, restart the kernel and run the notebook again. This code needs to be run only ONCE.\n\nFunction Breakdown for generate_points_from_dataframe\n\nInitialization: batch_size = 512 and total_batches set the stage for how many questions will be processed in one go. This is to prevent memory issues. If your machine can handle more, feel free to increase the batch size. If your kernel crashes, reduce the batch size to 32 and try again.\nProgress Bar: tqdm gives you a nice progress bar so you don't fall asleep.\nBatch Loop: The for-loop iterates through batches. start_idx and end_idx define the slice of the DataFrame to process.\nGenerate Embeddings: batch_embeddings = embedding_model.embed(batch, batch_size=batch_size) - This is where the magic happens. Your questions get turned into embeddings.\nPointStruct Generation: Using .progress_apply, it turns each row into a PointStruct object. This includes an ID, the embedding vector, and other metadata.\n\nReturns the list of PointStruct objects, which can be used to create a collection in Qdrant.\nUpload the Embeddings to Qdrant\n\nNote that configuring Qdrant is outside the scope of this notebook. Please refer to the Qdrant for more information. We used a timeout of 600 seconds for the upload, and grpc compression to speed up the upload.\n6. Using Qdrant to Improve RAG Prompt\n\nNow that we've uploaded the embeddings to Qdrant, we can use Qdrant to find the most similar questions to the question we're looking for. We'll use the top 5 most similar questions to create a prompt that we can use to fine-tune the model. We'll then measure the performance of the fine-tuned model on the same validation set, but with few shot prompting!\n\nOur main function get_few_shot_prompt serves as the workhorse for generating prompts for few-shot learning. It does this by retrieving similar questions from Qdrant - a vector search engine, using an embeddings model. Here is the high-level workflow:\n\nRetrieve similar questions from Qdrant where the answer is present in the context\nRetrieve similar questions from Qdrant where the answer is IMPOSSIBLE i.e. the expected answer is \"I don't know\" to find in the context\nCreate a prompt using the retrieved questions\nFine-tune the model using the prompt\nEvaluate the fine-tuned model on the validation set with the same prompting technique\n7. Fine-Tuning OpenAI Model with Qdrant\n\n7.1 Upload the Fine-Tuning Data to OpenAI\n7.2 Fine-Tune the Model\n\n⏰ Time to run: ~15-30 minutes\n⏰ Time to run: 5-15 min\n8. Evaluation\n\nBut how well does the model perform? Let's compare the results from the 3 different models we've looked at so far:\nThis is quite amazing -- we're able to get the best of both worlds! We're able to get the model to be both correct and conservative:\n\nThe model is correct 83% of the time -- this is the same as the base model\nThe model gives the wrong answer only 8% of the time -- down from 17% with the base model\nNext, let's look at the hallucinations. We want to reduce the hallucinations, but not at the cost of correctness. We want to strike a balance between the two. We've struck a good balance here:\n\nThe model hallucinates 53% of the time -- down from 100% with the base model\nThe model says \"I don't know\" 47% of the time -- up from NEVER with the base model\nFew Shot Fine-Tuning with Qdrant is a great way to control and steer the performance of your RAG system. Here, we made the model less conservative compared to zero shot and more confident by using Qdrant to find similar questions.\n\nYou can also use Qdrant to make the model more conservative. We did this by giving examples of questions where the answer is not present in the context.\nThis is biasing the model to say \"I don't know\" more often.\n\nSimilarly, one can also use Qdrant to make the model more confident by giving examples of questions where the answer is present in the context. This biases the model to give an answer more often. The trade-off is that the model will also hallucinate more often.\n\nYou can make this trade off by adjusting the training data: distribution of questions and examples, as well as the kind and number of examples you retrieve from Qdrant.\n\n9. Conclusion\n\nIn this notebook, we've demonstrated how to fine-tune OpenAI models for specific use-cases. We've also demonstrated how to use Qdrant and Few-Shot Learning to improve the performance of the model.\n\nAggregate Results\n\nSo far, we've looked at the results for each scenario separately, i.e. each scenario summed to 100. Let's look at the results as an aggregate to get a broader sense of how the model is performing:\n\n| Category | Base | Fine-Tuned | Fine-Tuned with Qdrant |\n| --- | --- | --- | --- |\n| Correct | 44% | 32% | 44% |\n| Skipped | 0% | 18% | 5% |\n| Wrong | 9% | 3% | 4% |\n| Hallucination | 47% | 7% | 25% |\n| I don't know | 0% | 40% | 22% |\nObservations\n\nCompared to base model\nThe few shot fine-tuned with Qdrant model is as good as the base model at answering questions where the answer is present in the context.\nThe few shot fine-tuned with Qdrant model is better at saying \"I don't know\" when the answer is not present in the context.\nThe few shot fine-tuned with Qdrant model is better at reducing hallucinations.\n\nCompared to fine-tuned model\nThe few shot fine-tuned with Qdrant model gets more correct answers than the fine-tuned model: 83% of the questions are answered correctly vs 60% for the fine-tuned model\nThe few shot fine-tuned with Qdrant model is better at deciding when to say \"I don't know\" when the answer is not present in the context. 34% skip rate for the plain fine-tuning mode, vs 9% for the few shot fine-tuned with Qdrant model\n\n\nNow, you should be able to:\n\nNotice the trade-offs between number of correct answers and hallucinations -- and how training dataset choice influences that!\nFine-tune OpenAI models for specific use-cases and use Qdrant to improve the performance of your RAG model\nGet started on how to evaluate the performance of your RAG model"])</script><script>self.__next_f.push([1,"7a:T878,"])</script><script>self.__next_f.push([1,"This code demonstrates how to interact with ChatGPT functions to perform tasks related to Amazon S3 buckets. The notebook covers S3 bucket key functionalities such as running simple listing commands, searching for a specific file in all buckets, uploading a file to a bucket, and downloading a file from a bucket. The OpenAI Chat API understands the user instructions, generates the natural language responses, and extracts appropriate function calls based on the user's input.\n\nRequirements:\nTo run the notebook generate AWS access key with S3 bucket writing permission and store them in a local environment file alongside the Openai key. The \".env\" file format:\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nOPENAI_API_KEY=\nInitials\nUtilities\nTo connect user questions or commands to the appropriate function, we need to provide ChatGPT with the necessary function details and expected parameters.\nCreate helper functions to interact with the S3 service, such as listing buckets, listing objects, downloading and uploading files, and searching for specific files.\nThe below dictionary connects the name with the function to use it for execution based on ChatGPT responses.\nChatGPT\nConversation flow\nCreate a main function for the chatbot, which takes user input, sends it to the OpenAI Chat API, receives a response, executes any function calls generated by the API, and returns a final response to the user.\nS3 bucket bot testing\nIn the following examples, make sure to replace the placeholders such as `, , and ` with your specific values before execution.\nListing and searching\nLet's start by listing all the available buckets.\nYou can ask the assistant to search for a specific file name either in all the buckets or in a specific one.\nThe model is expected to clarify the ask from the user in case of ambiguity in the parameters values as described in the system message.\nValidate edge cases\n\nWe also instructed the model to reject irrelevant tasks. Let's test it out and see how it works in action.\nThe provided functions are not limited to just retrieving information. They can also assist the user in uploading or downloading files.\nDownload a file\nUpload a file"])</script><script>self.__next_f.push([1,"7b:T939,"])</script><script>self.__next_f.push([1,"\nNeon is Serverless Postgres built for the cloud. Neon separates compute and storage to offer modern developer features such as autoscaling, database branching, scale-to-zero, and more.\n\nVector search\n\nNeon supports vector search using the pgvector open-source PostgreSQL extension, which enables Postgres as a vector database for storing and querying embeddings.\n\nOpenAI cookbook notebook\n\nCheck out the notebook in this repo for working with Neon Serverless Postgres as your vector database.\n\nSemantic search using Neon Postgres with pgvector and OpenAI\n\nIn this notebook you will learn how to:\n\nUse embeddings created by OpenAI API\nStore embeddings in a Neon Serverless Postgres database\nConvert a raw text query to an embedding with OpenAI API\nUse Neon with the pgvector extension to perform vector similarity search\n\nScaling Support\n\nNeon enables you to scale your AI applications with the following features:\n\nAutoscaling: If your AI application experiences heavy load during certain hours of the day or at different times, Neon can automatically scale compute resources without manual intervention. During periods of inactivity, Neon is able to scale to zero.\nInstant read replicas: Neon supports instant read replicas, which are independent read-only compute instances designed to perform read operations on the same data as your read-write computes. With read replicas, you can offload reads from your read-write compute instance to a dedicated read-only compute instance for your AI application.\nThe Neon serverless driver: Neon supports a low-latency serverless PostgreSQL driver for JavaScript and TypeScript applications that allows you to query data from serverless and edge environments, making it possible to achieve sub-10ms queries.\n\nMore Examples\n\nBuild an AI-powered semantic search application - Submit a startup idea and get a list of similar ideas that YCombinator has invested in before\nBuild an AI-powered chatbot - A Postgres Q\u0026A chatbot that uses Postgres as a vector database\nVercel Postgres pgvector Starter - Vector similarity search with Vercel Postgres (powered by Neon)\n\nAdditional Resources\n\nBuilding AI applications with Neon\nNeon AI \u0026 embeddings documentation\nBuilding an AI-powered Chatbot using Vercel, OpenAI, and Postgres\nWeb-based AI SQL Playground and connecting to Postgres from the browser\npgvector GitHub repository\n"])</script><script>self.__next_f.push([1,"7c:Td6b,"])</script><script>self.__next_f.push([1,"\nThis notebook guides you through using Neon Serverless Postgres as a vector database for OpenAI embeddings. It demonstrates how to:\n\nUse embeddings created by OpenAI API.\nStore embeddings in a Neon Serverless Postgres database.\nConvert a raw text query to an embedding with OpenAI API.\nUse Neon with the pgvector extension to perform vector similarity search.\nPrerequisites\n\nBefore you begin, ensure that you have the following:\n\nA Neon Postgres database. You can create an account and set up a project with a ready-to-use neondb database in a few simple steps. For instructions, see Sign up and Create your first project.\nA connection string for your Neon database. You can copy it from the Connection Details widget on the Neon Dashboard. See Connect from any application.\nThe pgvector extension. Install the extension in Neon by running CREATE EXTENSION vector;. For instructions, see Enable the pgvector extension.\nYour OpenAI API key.\nPython and pip.\nInstall required modules\n\nThis notebook requires the openai, psycopg2, pandas, wget, and python-dotenv packages. You can install them with pip:\n\nPrepare your OpenAI API key\n\nAn OpenAI API key is required to generate vectors for documents and queries.\n\nIf you do not have an OpenAI API key, obtain one from https://platform.openai.com/account/api-keys.\n\nAdd the OpenAI API key as an operating system environment variable or provide it for the session when prompted. If you define an environment variable, name the variable OPENAI_API_KEY.\n\nFor information about configuring your OpenAI API key as an environment variable, refer to Best Practices for API Key Safety.\nTest your OpenAPI key\nConnect to your Neon database\n\nProvide your Neon database connection string below or define it in an .env file using a DATABASE_URL variable. For information about obtaining a Neon connection string, see Connect from any application.\nTest the connection to your database:\nThis guide uses pre-computed Wikipedia article embeddings available in the OpenAI Cookbook examples directory so that you do not have to compute embeddings with your own OpenAI credits.\n\nImport the pre-computed embeddings zip file:\nExtract the downloaded zip file:\nCreate a table and add indexes for your vector embeddings\n\nThe vector table created in your database is called articles. Each object has title and content vectors.\n\nAn index is defined on both the title and content vector columns.\nLoad the data\n\nLoad the pre-computed vector data into your articles table from the .csv file. There are 25000 records, so expect the operation to take several minutes.\nCheck the number of records to ensure the data has been been loaded. There should be 25000 records.\nSearch your data\n\nAfter the data is stored in your Neon database, you can query the data for nearest neighbors.\n\nStart by defining the query_neon function, which is executed when you run the vector similarity search. The function creates an embedding based on the user's query, prepares the SQL query, and runs the SQL query with the embedding. The pre-computed embeddings that you loaded into your database were created with text-embedding-3-small OpenAI model, so you must use the same model to create an embedding for the similarity search.\n\nA vector_name parameter is provided that allows you to search based on \"title\" or \"content\".\nRun a similarity search based on title_vector embeddings:\nRun a similarity search based on content_vector embeddings:"])</script><script>self.__next_f.push([1,"7d:T66c,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how to implement a question answering system with LangChain, Deep Lake as a vector store and OpenAI embeddings. We will take the following steps to achieve this:\n\nLoad a Deep Lake text dataset\nInitialize a Deep Lake vector store with LangChain\nAdd text to the vector store\nRun queries on the database\nDone!\n\nYou can also follow other tutorials such as question answering over any type of data (PDFs, json, csv, text): chatting with any data stored in Deep Lake, code understanding, or question answering over PDFs, or recommending songs.\nInstall requirements\nLet's install the following packages.\nAuthentication\nProvide your OpenAI API key here:\nLoad a Deep Lake text dataset\nWe will use a 20000 sample subset of the cohere-wikipedia-22 dataset for this example.\nLet's take a look at a few samples:\nLangChain's Deep Lake vector store\nLet's define a dataset_path, this is where your Deep Lake vector store will house the text embeddings.\nWe will setup OpenAI's text-embedding-3-small as our embedding function and initialize a Deep Lake vector store at dataset_path...\n... and populate it with samples, one batch at a time, using the add_texts method.\nRun user queries on the database\nThe underlying Deep Lake dataset object is accessible through db.vectorstore.dataset, and the data structure can be summarized using db.vectorstore.summary(), which shows 4 tensors with 10 samples:\nWe will now setup QA on our vector store with GPT-3.5-Turbo as our LLM.\nLet's try running a prompt and check the output. Internally, this API performs an embedding search to find the most relevant data to feed into the LLM context.\nEt voila!"])</script><script>self.__next_f.push([1,"7e:T10c5,"])</script><script>self.__next_f.push([1,"\n\n\n\n\n\nNote: you will need an OpenAI API key to run this colab.\nIf you use OpenAI's API to fine-tune ChatGPT-3.5, you can now use the W\u0026B integration to track experiments, models, and datasets in your central dashboard.\n\nAll it takes is one line: openai wandb sync\n\nSee the OpenAI section in the Weights \u0026 Biases documentation for full details of the integration\nOptional: Fine-tune ChatGPT-3.5\n\nIt's always more fun to experiment with your own projects so if you have already used the openai API to fine-tune an OpenAI model, just skip this section.\n\nOtherwise let's fine-tune ChatGPT-3.5 on a legal dataset!\nImports and initial set-up\nStart your Weigths \u0026 Biases run. If you don't have an account you can sign up for one for free at www.wandb.ai\nSet up your API key\nDataset Preparation\n\nWe download a dataset from LegalBench, a project to curate tasks for evaluating legal reasoning, specifically the Contract NLI Explicit Identification task.\n\nThis comprises of a total of 117 examples, from which we will create our own train and test datasets\nLet's look at a few samples.\nFormat our Data for Chat Completion Models\nWe modify the base_prompt from the LegalBench task to make it a zero-shot prompt, as we are training the model instead of using few-shot prompting\nWe now split it into training/validation dataset, lets train on 30 samples and test on the remainder\n\nSave the data to Weigths \u0026 Biases\n\nSave the data in a train and test file first\nNext, we validate that our training data is in the correct format using a script from the OpenAI fine-tuning documentation\nValidate train data\nLog our data to Weigths \u0026 Biases Artifacts for storage and versioning\nCreate a fine-tuned model\nWe'll now use OpenAI API to fine-tune ChatGPT-3.5\n\nLet's first download our training \u0026 validation files and save them to a folder called my_data. We will retrieve the latest version of the artifact, but it could also be v0, v1 or any alias we associated with it\nThen we upload the training data to OpenAI. OpenAi has to process the data, so this will take a few minutes depending on the size of your dataset.\nTime to train the model!\nLet's define our ChatGPT-3.5 fine-tuning hyper-parameters.\nthis takes around 5 minutes to train, and you get an email from OpenAI when finished.\nThats it!\n\nNow your model is training on OpenAI's machines. To get the current state of your fine-tuning job, run:\nShow recent events for our fine-tuning job\nWe can run a few different fine-tunes with different parameters or even with different datasets.\nLog OpenAI fine-tune jobs to Weights \u0026 Biases\nWe can log our fine-tunes with a simple command.\nCalling openai wandb sync will log all un-synced fine-tuned jobs to W\u0026B\n\nBelow we are just logging 1 job, passing:\nour OpenAI key as an environment variable\nthe id of the fine-tune job we'd like to log\nthe W\u0026B project of where to log it to\n\nSee the OpenAI section in the Weights \u0026 Biases documentation for full details of the integration\nOur fine-tunes are now successfully synced to Weights \u0026 Biases.\n\nimage.png\n\nAnytime we have new fine-tunes, we can just call openai wandb sync to add them to our dashboard.\nRun evalution and log the results\nThe best way to evaluate a generative model is to explore sample predictions from your evaluation set.\n\nLet's generate a few inference samples and log them to W\u0026B and see how the performance compares to a baseline ChatGPT-3.5 model\nRun evaluation on the Fine-Tuned Model\nSet up OpenAI call with retries\nLet's get our trained model id\nRun evaluation and log results to W\u0026B\nCalculate the accuracy of the fine-tuned model and log to W\u0026B\n\nRun evaluation on a Baseline model for comparison\nLets compare our model to the baseline model, gpt-3.5-turbo\nCalculate the accuracy of the fine-tuned model and log to W\u0026B\nAnd thats it! In this example we have prepared our data, logged it to Weights \u0026 Biases, fine-tuned an OpenAI model using that data, logged the results to Weights \u0026 Biases and then run evaluation on the fine-tuned model.\n\nFrom here you can start to train on larger or more complex tasks, or else explore other ways to modify ChatGPT-3.5 such as giving it a different tone and style or response.\n\nResources\n\nOpenAI Fine-Tuning Guide\nW\u0026B Integration with OpenAI API Documentation\nW\u0026B Report: GPT-3 exploration \u0026 fine-tuning tips"])</script><script>self.__next_f.push([1,"7f:Tb94,"])</script><script>self.__next_f.push([1,"\n\n\nNote: you will need an OpenAI API key to run this colab.\n\nUse the W\u0026B OpenAI integration to monitor OpenAI API calls and understand how your projects and teams are leveraging LLMs.\nIn this example, we'll generate templated Weave Boards: LLM usage monitoring dashboards which you can explore and customize from the UI.\n\nautomatically track LLM usage and aggregate useful metrics like  cost, latency and throughput across your projects/teams\ndynamically query and derive insights from the logs of all your OpenAI API calls\niterate visually to slice, aggregate, and explore your data; customize panels to focus on interesting patterns; share progress more easily with your team through an interactive dashboard\n\n\n\nPlay with a live version of this Weave Board →\n\nNew to Weights \u0026 Biases? [-\u003e Sign up for an account here ), where  has the form //`. The stream records and stores all the OpenAI API calls.\n\nRunning this cell will print out a link to view the current project in the Weave UI.\nStep 3: Preview monitoring dashboard\n\nClick on the link above to preview the data stream, then click \"OpenAI Monitor Board\" in the right sidebar to create a Weave Board for this data stream.\n\nStep 4: Explore \u0026 understand your LLM usage\n\nTo save your work, rename the board by clicking on the autogenerated name at the top of the page. To share your board, click \\\"Publish\\\" in the top right.\n\n\n\nTo visualize your work in real-time as you iterate, you can:\nkeep the Board open in a separate tab and refresh to view the latest data\nrename the Board for easier reference at any point and \\\"Publish\\\" that version to share a link with others\nfind previously saved Boards by navigating to the relevant W\u0026B entity and W\u0026B project name from weave.wandb.ai\nor open a new instance of a Board template to start fresh with all the data accumulated so far\n\n\nNext we'll illustrate a few ways you could track OpenAI API calls. There are many more possibilities depending on your use case, and we can't wait to see what you create from these starter templates.\n\nExamples\n\nExample 0: Log a prompt and its completion\n\nMonitor a ChatCompletion request and print the corresponding response, extracting only the text of the completion.\nExample 1: Track relevant parameters as attributes\n\nFactor out parameters of interest and track them as attributes on the logged record.\nHere we track the \"system prompt\" separately from the \"prompt template\" and the \"equation\" parameter. This time we'll print the full structured response from the ChatCompletion call.\nExample 2: Log an ongoing stream of messages\n\nMonitor a stream of messages and log the result as a single record. Note: tokens are not counted in this format.\nExample 3: Structure prompt engineering experiments\n\nHere we compare a few toy options for the system prompt, user question, and intended audience. Try your own experiments and see if any interesting insights emerge as you explore in the Board and group by different parameters."])</script><script>self.__next_f.push([1,"80:T2c5d,"])</script><script>self.__next_f.push([1,"\nOpenAI functions enable your app to take action based on user inputs. This means that it can, e.g., search the web, send emails, or book tickets on behalf of your users, making it more powerful than a regular chatbot.\n\nIn this tutorial, you will build an app that uses OpenAI functions along with the latest version of the Node.js SDK. The app runs in the browser, so you only need a code editor and, e.g., VS Code Live Server to follow along locally. Alternatively, write your code directly in the browser via this code playground at Scrimba.\n\nWhat you will build\n\nOur app is a simple agent that helps you find activities in your area.\nIt has access to two functions, getLocation() and getCurrentWeather(),\nwhich means it can figure out where you’re located and what the weather\nis at the moment.\n\nAt this point, it's important to understand that\nOpenAI doesn't execute any code for you. It just tells your app which\nfunctions it should use in a given scenario, and then leaves it up to\nyour app to invoke them.\n\nOnce our agent knows your location and the weather, it'll use GPT’s\ninternal knowledge to suggest suitable local activities for you.\n\nImporting the SDK and authenticating with OpenAI\n\nWe start by importing the OpenAI SDK at the top of our JavaScript file and authenticate with our API key, which we have stored as an environment variable.\n\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  dangerouslyAllowBrowser: true,\n});\n\nSince we're running our code in a browser environment at Scrimba, we also need to set dangerouslyAllowBrowser: true to confirm we understand the risks involved with client-side API requests. Please note that you should move these requests over to a Node server in a production app.\n\nCreating our two functions\n\nNext, we'll create the two functions. The first one - getLocation -\nuses the IP API to get the location of the\nuser.\n\nasync function getLocation() {\n  const response = await fetch(\"https://ipapi.co/json/\");\n  const locationData = await response.json();\n  return locationData;\n}\n\nThe IP API returns a bunch of data about your location, including your\nlatitude and longitude, which we’ll use as arguments in the second\nfunction getCurrentWeather. It uses the Open Meteo\nAPI to get the current weather data, like\nthis:\n\nasync function getCurrentWeather(latitude, longitude) {\n  const url = https://api.open-meteo.com/v1/forecast?latitude=${latitude}\u0026longitude=${longitude}\u0026hourly=apparent_temperature;\n  const response = await fetch(url);\n  const weatherData = await response.json();\n  return weatherData;\n}\n\nDescribing our functions for OpenAI\n\nFor OpenAI to understand the purpose of these functions, we need to\ndescribe them using a specific schema. We'll create an array called\ntools that contains one object per function. Each object\nwill have two keys: type, function, and the function key has\nthree subkeys: name, description, and parameters.\n\nconst tools = [\n  {\n    type: \"function\",\n    function: {\n      name: \"getCurrentWeather\",\n      description: \"Get the current weather in a given location\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          latitude: {\n            type: \"string\",\n          },\n          longitude: {\n            type: \"string\",\n          },\n        },\n        required: [\"longitude\", \"latitude\"],\n      },\n    }\n  },\n  {\n    type: \"function\",\n    function: {\n      name: \"getLocation\",\n      description: \"Get the user's location based on their IP address\",\n      parameters: {\n        type: \"object\",\n        properties: {},\n      },\n    }\n  },\n];\n\nSetting up the messages array\n\nWe also need to define a messages array. This will keep track of all of the messages back and forth between our app and OpenAI.\n\nThe first object in the array should always have the role property set to \"system\", which tells OpenAI that this is how we want it to behave.\n\nconst messages = [\n  {\n    role: \"system\",\n    content:\n      \"You are a helpful assistant. Only use the functions you have been provided with.\",\n  },\n];\n\nCreating the agent function\n\nWe are now ready to build the logic of our app, which lives in the\nagent function. It is asynchronous and takes one argument: the\nuserInput.\n\nWe start by pushing the userInput to the messages array. This time, we set the role to \"user\", so that OpenAI knows that this is the input from the user.\n\nasync function agent(userInput) {\n  messages.push({\n    role: \"user\",\n    content: userInput,\n  });\n  const response = await openai.chat.completions.create({\n    model: \"gpt-4\",\n    messages: messages,\n    tools: tools,\n  });\n  console.log(response);\n}\n\nNext, we'll send a request to the Chat completions endpoint via the\nchat.completions.create() method in the Node SDK. This method takes a\nconfiguration object as an argument. In it, we'll specify three\nproperties:\n\nmodel - Decides which AI model we want to use (in our case,\n  GPT-4).\nmessages - The entire history of messages between the user and the\n  AI up until this point.\ntools - A list of tools the model may call. Currently, only\n  functions are supported as a tool., we'll we use the tools array we\n  created earlier.\n\nRunning our app with a simple input\n\nLet's try to run the agent with an input that requires a function call to give a suitable reply.\n\nagent(\"Where am I located right now?\");\n\nWhen we run the code above, we see the response from OpenAI logged out\nto the console like this:\n\n{\n    id: \"chatcmpl-84ojoEJtyGnR6jRHK2Dl4zTtwsa7O\",\n    object: \"chat.completion\",\n    created: 1696159040,\n    model: \"gpt-4-0613\",\n    choices: [{\n        index: 0,\n        message: {\n            role: \"assistant\",\n            content: null,\n            tool_calls: [\n              id: \"call_CBwbo9qoXUn1kTR5pPuv6vR1\",\n              type: \"function\",\n              function: {\n                name: \"getLocation\",\n                arguments: \"{}\"\n              }\n            ]\n        },\n        logprobs: null,\n        finish_reason: \"tool_calls\" // OpenAI wants us to call a function\n    }],\n    usage: {\n        prompt_tokens: 134,\n        completion_tokens: 6,\n        total_tokens: 140\n    }\n     system_fingerprint: null\n}\n\nThis response tells us that we should call one of our functions, as it contains the following key: finish_reason: \"tool_calls\".\n\nThe name of the function can be found in the\nresponse.choices[0].message.tool_calls[0].function.name key, which is set to\n\"getLocation\".\n\nTurning the OpenAI response into a function call\n\nNow that we have the name of the function as a string, we'll need to\ntranslate that into a function call. To help us with that, we'll gather\nboth of our functions in an object called availableTools:\n\nconst availableTools = {\n  getCurrentWeather,\n  getLocation,\n};\n\nThis is handy because we'll be able to access the getLocation function\nvia bracket notation and the string we got back from OpenAI, like this:\navailableTools[\"getLocation\"].\n\nconst { finish_reason, message } = response.choices[0];\n\nif (finish_reason === \"tool_calls\" \u0026\u0026 message.tool_calls) {\n  const functionName = message.tool_calls[0].function.name;\n  const functionToCall = availableTools[functionName];\n  const functionArgs = JSON.parse(message.tool_calls[0].function.arguments);\n  const functionArgsArr = Object.values(functionArgs);\n  const functionResponse = await functionToCall.apply(null, functionArgsArr);\n  console.log(functionResponse);\n}\n\nWe're also grabbing ahold of any arguments OpenAI wants us to pass into\nthe function: message.tool_calls[0].function.arguments.\nHowever, we won't need any arguments for this first function call.\n\nIf we run the code again with the same input\n(\"Where am I located right now?\"), we'll see that functionResponse\nis an object filled with location about where the user is located right\nnow. In my case, that is Oslo, Norway.\n\n{ip: \"193.212.60.170\", network: \"193.212.60.0/23\", version: \"IPv4\", city: \"Oslo\", region: \"Oslo County\", region_code: \"03\", country: \"NO\", country_name: \"Norway\", country_code: \"NO\", country_code_iso3: \"NOR\", country_capital: \"Oslo\", country_tld: \".no\", continent_code: \"EU\", in_eu: false, postal: \"0026\", latitude: 59.955, longitude: 10.859, timezone: \"Europe/Oslo\", utc_offset: \"+0200\", country_calling_code: \"+47\", currency: \"NOK\", currency_name: \"Krone\", languages: \"no,nb,nn,se,fi\", country_area: 324220, country_population: 5314336, asn: \"AS2119\", org: \"Telenor Norge AS\"}\n\nWe'll add this data to a new item in the messages array, where we also\nspecify the name of the function we called.\n\nmessages.push({\n  role: \"function\",\n  name: functionName,\n  content: `The result of the last function was this: ${JSON.stringify(\n    functionResponse\n  )}\n  `,\n});\n\nNotice that the role is set to \"function\". This tells OpenAI\nthat the content parameter contains the result of the function call\nand not the input from the user.\n\nAt this point, we need to send a new request to OpenAI with this updated\nmessages array. However, we don’t want to hard code a new function\ncall, as our agent might need to go back and forth between itself and\nGPT several times until it has found the final answer for the user.\n\nThis can be solved in several different ways, e.g. recursion, a\nwhile-loop, or a for-loop. We'll use a good old for-loop for the sake of\nsimplicity.\n\nCreating the loop\n\nAt the top of the agent function, we'll create a loop that lets us run\nthe entire procedure up to five times.\n\nIf we get back finish_reason: \"tool_calls\" from GPT, we'll just\npush the result of the function call to the messages array and jump to\nthe next iteration of the loop, triggering a new request.\n\nIf we get finish_reason: \"stop\" back, then GPT has found a suitable\nanswer, so we'll return the function and cancel the loop.\n\nfor (let i = 0; i\nComplete code\n\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  dangerouslyAllowBrowser: true,\n});\n\nasync function getLocation() {\n  const response = await fetch(\"https://ipapi.co/json/\");\n  const locationData = await response.json();\n  return locationData;\n}\n\nasync function getCurrentWeather(latitude, longitude) {\n  const url = https://api.open-meteo.com/v1/forecast?latitude=${latitude}\u0026longitude=${longitude}\u0026hourly=apparent_temperature;\n  const response = await fetch(url);\n  const weatherData = await response.json();\n  return weatherData;\n}\n\nconst tools = [\n  {\n    type: \"function\",\n    function: {\n      name: \"getCurrentWeather\",\n      description: \"Get the current weather in a given location\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          latitude: {\n            type: \"string\",\n          },\n          longitude: {\n            type: \"string\",\n          },\n        },\n        required: [\"longitude\", \"latitude\"],\n      },\n    }\n  },\n  {\n    type: \"function\",\n    function: {\n      name: \"getLocation\",\n      description: \"Get the user's location based on their IP address\",\n      parameters: {\n        type: \"object\",\n        properties: {},\n      },\n    }\n  },\n];\n\nconst availableTools = {\n  getCurrentWeather,\n  getLocation,\n};\n\nconst messages = [\n  {\n    role: \"system\",\n    content: You are a helpful assistant. Only use the functions you have been provided with.,\n  },\n];\n\nasync function agent(userInput) {\n  messages.push({\n    role: \"user\",\n    content: userInput,\n  });\n\n  for (let i = 0; i\n"])</script><script>self.__next_f.push([1,"81:T168f,"])</script><script>self.__next_f.push([1,"Named Entity Recognition (NER) is a Natural Language Processing task that identifies and classifies named entities (NE) into predefined semantic categories (such as persons, organizations, locations, events, time expressions, and quantities). By converting raw text into structured information, NER makes data more actionable, facilitating tasks like information extraction, data aggregation, analytics, and social media monitoring.\n\nThis notebook demonstrates how to carry out NER with chat completion and functions-calling to enrich a text with links to a knowledge base such as Wikipedia:\n\nText:\n\nIn Germany, in 1440, goldsmith Johannes Gutenberg invented the movable-type printing press. His work led to an information revolution and the unprecedented mass-spread of literature throughout Europe. Modelled on the design of the existing screw presses, a single Renaissance movable-type printing press could produce up to 3,600 pages per workday.\n\nText enriched with Wikipedia links:\n\nIn Germany, in 1440, goldsmith Johannes Gutenberg invented the movable-type printing press. His work led to an information revolution and the unprecedented mass-spread of literature throughout Europe. Modelled on the design of the existing screw presses, a single Renaissance movable-type printing press could produce up to 3,600 pages per workday.\n\nInference Costs: The notebook also illustrates how to estimate OpenAI API costs.\n1. Setup\n1.1 Install/Upgrade Python packages\n1.2 Load packages and OPENAI_API_KEY\nYou can generate an API key in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\nThis notebook works with the latest OpeanAI models gpt-3.5-turbo-0613 and gpt-4-0613.\n2. Define the NER labels to be Identified\nWe define a standard set of NER labels to showcase a wide range of use cases. However, for our specific task of enriching text with knowledge base links, only a subset is practically required.\n3. Prepare messages\nThe chat completions API takes a list of messages as input and delivers a model-generated message as an output. While the chat format is primarily designed for facilitating multi-turn conversations, it is equally efficient for single-turn tasks without any preceding conversation. For our purposes, we will specify a message for the system, assistant, and user roles.\n3.1 System Message\nThe system message (prompt) sets the assistant's behavior by defining its desired persona and task. We also delineate the specific set of entity labels we aim to identify.\nAlthough one can instruct the model to format its response, it has to be noted that both gpt-3.5-turbo-0613 and gpt-4-0613 have been fine-tuned to discern when a function should be invoked, and to reply with JSON formatted according to the function's signature. This capability streamlines our prompt and enables us to receive structured data directly from the model.\n3.2 Assistant Message\nAssistant messages usually store previous assistant responses. However, as in our scenario, they can also be crafted to provide examples of the desired behavior. While OpenAI is able to execute zero-shot Named Entity Recognition, we have found that a one-shot approach produces more precise results.\n3.3 User Message\nThe user message provides the specific text for the assistant task:\n4. OpenAI Functions (and Utils)\nIn an OpenAI API call, we can describe functions to gpt-3.5-turbo-0613 and gpt-4-0613 and have the model intelligently choose to output a JSON object containing arguments to call those functions. It's important to note that the chat completions API doesn't actually execute the function. Instead, it provides the JSON output, which can then be used to call the function in our code. For more details, refer to the OpenAI Function Calling Guide.\nOur function, enrich_entities(text, label_entities) gets a block of text and a dictionary containing identified labels and entities as parameters. It then associates the recognized entities with their corresponding links to the Wikipedia articles.\n4. ChatCompletion\nAs previously highlighted, gpt-3.5-turbo-0613 and gpt-4-0613 have been fine-tuned to detect when a function should to be called. Moreover, they can produce a JSON response that conforms to the function signature. Here's the sequence we follow:\n\nDefine our function and its associated JSON Schema.\nInvoke the model using the messages, tools and tool_choice parameters.\nConvert the output into a JSON object, and then call the function with the arguments provided by the model.\n\nIn practice, one might want to re-invoke the model again by appending the function response as a new message, and let the model summarize the results back to the user. Nevertheless, for our purposes, this step is not needed.\n\nNote that in a real-case scenario it is strongly recommended to build in user confirmation flows before taking actions.\n4.1 Define our Function and JSON schema\nSince we want the model to output a dictionary of labels and recognized entities:\n\n{\n    \"gpe\": [\"Germany\", \"Europe\"],\n    \"date\": [\"1440\"],\n    \"person\": [\"Johannes Gutenberg\"],\n    \"product\": [\"movable-type printing press\"],\n    \"event\": [\"Renaissance\"],\n    \"quantity\": [\"3,600 pages\"],\n    \"time\": [\"workday\"]\n}\nwe need to define the corresponding JSON schema to be passed to the tools parameter:\n4.2 Chat Completion\nNow, we invoke the model. It's important to note that we direct the API to use a specific function by setting the tool_choice parameter to {\"type\": \"function\", \"function\" : {\"name\": \"enrich_entities\"}}.\n5. Let's Enrich a Text with Wikipedia links\n5.1 Run OpenAI Task\n5.2 Function Response\n5.3 Token Usage\nTo estimate the inference costs, we can parse the response's \"usage\" field. Detailed token costs per model are available in the OpenAI Pricing Guide:"])</script><script>self.__next_f.push([1,"82:T22ee,"])</script><script>self.__next_f.push([1,"\nDocumentation puts useful information inside other people’s heads. Follow these tips to write better documentation.\n\nMake docs easy to skim\n\nFew readers read linearly from top to bottom. They’ll jump around, trying to assess which bit solves their problem, if any. To reduce their search time and increase their odds of success, make docs easy to skim.\n\nSplit content into sections with titles. Section titles act as signposts, telling readers whether to focus in or move on.\n\nPrefer titles with informative sentences over abstract nouns. For example, if you use a title like “Results”, a reader will need to hop into the following text to learn what the results actually are. In contrast, if you use the title “Streaming reduced time to first token by 50%”, it gives the reader the information immediately, without the burden of an extra hop.\n\nInclude a table of contents. Tables of contents help readers find information faster, akin to how hash maps have faster lookups than linked lists. Tables of contents also have a second, oft overlooked benefit: they give readers clues about the doc, which helps them understand if it’s worth reading.\n\nKeep paragraphs short. Shorter paragraphs are easier to skim. If you have an essential point, consider putting it in its own one-sentence paragraph to reduce the odds it’s missed. Long paragraphs can bury information.\n\nBegin paragraphs and sections with short topic sentences that give a standalone preview. When people skim, they look disproportionately at the first word, first line, and first sentence of a section. Write these sentences in a way that don’t depend on prior text. For example, consider the first sentence “Building on top of this, let’s now talk about a faster way.” This sentence will be meaningless to someone who hasn’t read the prior paragraph. Instead, write it in a way that can understood standalone: e.g., “Vector databases can speed up embeddings search.”\n\nPut topic words at the beginning of topic sentences. Readers skim most efficiently when they only need to read a word or two to know what a paragraph is about. Therefore, when writing topic sentences, prefer putting the topic at the beginning of the sentence rather than the end. For example, imagine you’re writing a paragraph on vector databases in the middle of a long article on embeddings search. Instead of writing “Embeddings search can be sped up by vector databases” prefer “Vector databases speed up embeddings search.” The second sentence is better for skimming, because it puts the paragraph topic at the beginning of the paragraph.\n\nPut the takeaways up front. Put the most important information at the tops of documents and sections. Don’t write a Socratic big build up. Don’t introduce your procedure before your results.\n\nUse bullets and tables. Bulleted lists and tables make docs easier to skim. Use them frequently.\n\nBold important text. Don’t be afraid to bold important text to help readers find it.\n\nWrite well\n\nBadly written text is taxing to read. Minimize the tax on readers by writing well.\n\nKeep sentences simple. Split long sentences into two. Cut adverbs. Cut unnecessary words and phrases. Use the imperative mood, if applicable. Do what writing books tell you.\n\nWrite sentences that can be parsed unambiguously. For example, consider the sentence “Title sections with sentences.” When a reader reads the word “Title”, their brain doesn’t yet know whether “Title” is going to be a noun or verb or adjective. It takes a bit of brainpower to keep track as they parse the rest of the sentence, and can cause a hitch if their brain mispredicted the meaning. Prefer sentences that can be parsed more easily (e.g., “Write section titles as sentences”) even if longer. Similarly, avoid noun phrases like “Bicycle clearance exercise notice” which can take extra effort to parse.\n\nAvoid left-branching sentences. Linguistic trees show how words relate to each other in sentences. Left-branching trees require readers to hold more things in memory than right-branching sentences, akin to breadth-first search vs depth-first search. An example of a left-branching sentence is “You need flour, eggs, milk, butter and a dash of salt to make pancakes.” In this sentence you don’t find out what ‘you need’ connects to until you reach the end of the sentence. An easier-to-read right-branching version is “To make pancakes, you need flour, eggs, milk, butter, and a dash of salt.” Watch out for sentences in which the reader must hold onto a word for a while, and see if you can rephrase them.\n\nAvoid demonstrative pronouns (e.g., “this”), especially across sentences. For example, instead of saying “Building on our discussion of the previous topic, now let’s discuss function calling” try “Building on message formatting, now let’s discuss function calling.” The second sentence is easier to understand because it doesn’t burden the reader with recalling the previous topic. Look for opportunities to cut demonstrative pronouns altogether: e.g., “Now let’s discuss function calling.”\n\nBe consistent. Human brains are amazing pattern matchers. Inconsistencies will annoy or distract readers. If we use Title Case everywhere, use Title Case. If we use terminal commas everywhere, use terminal commas. If all of the Cookbook notebooks are named with underscores and sentence case, use underscores and sentence case. Don’t do anything that will cause a reader to go ‘huh, that’s weird.’ Help them focus on the content, not its inconsistencies.\n\nDon’t tell readers what they think or what to do. Avoid sentences like “Now you probably want to understand how to call a function” or “Next, you’ll need to learn to call a function.” Both examples presume a reader’s state of mind, which may annoy them or burn our credibility. Use phrases that avoid presuming the reader’s state. E.g., “To call a function, …”\n\nBe broadly helpful\n\nPeople come to documentation with varying levels of knowledge, language proficiency, and patience. Even if we target experienced developers, we should try to write docs helpful to everyone.\n\nWrite simply. Explain things more simply than you think you need to. Many readers might not speak English as a first language. Many readers might be really confused about technical terminology and have little excess brainpower to spend on parsing English sentences. Write simply. (But don’t oversimplify.)\n\nAvoid abbreviations. Write things out. The cost to experts is low and the benefit to beginners is high. Instead of IF, write instruction following. Instead of RAG, write retrieval-augmented generation (or my preferred term: the search-ask procedure).\n\nOffer solutions to potential problems. Even if 95% of our readers know how to install a Python package or save environment variables, it can still be worth proactively explaining it. Including explanations is not costly to experts—they can skim right past them. But excluding explanations is costly to beginners—they might get stuck or even abandon us. Remember that even an expert JavaScript engineer or C++ engineer might be a beginner at Python. Err on explaining too much, rather than too little.\n\nPrefer terminology that is specific and accurate. Jargon is bad. Optimize the docs for people new to the field, instead of ourselves. For example, instead of writing “prompt”, write “input.” Or instead of writing “context limit” write “max token limit.” The latter terms are more self-evident, and are probably better than the jargon developed in base model days.\n\nKeep code examples general and exportable. In code demonstrations, try to minimize dependencies. Don’t make users install extra libraries. Don’t make them have to refer back and forth between different pages or sections. Try to make examples simple and self-contained.\n\nPrioritize topics by value. Documentation that covers common problems—e.g., how to count tokens—is magnitudes more valuable than documentation that covers rare problems—e.g., how to optimize an emoji database. Prioritize accordingly.\n\nDon’t teach bad habits. If API keys should not be stored in code, never share an example that stores an API key in code.\n\nIntroduce topics with a broad opening. For example, if explaining how to program a good recommender, consider opening by briefly mentioning that recommendations are widespread across the web, from YouTube videos to Amazon items to Wikipedia. Grounding a narrow topic with a broad opening can help people feel more secure before jumping into uncertain territory. And if the text is well-written, those who already know it may still enjoy it.\n\nBreak these rules when you have a good reason\n\nUltimately, do what you think is best. Documentation is an exercise in empathy. Put yourself in the reader’s position, and do what you think will help them the most.\n"])</script><script>self.__next_f.push([1,"83:T11e7,"])</script><script>self.__next_f.push([1,"\nMuch of the internet is powered by RESTful APIs. Giving GPT the ability to call them opens up a world of possibilities. This notebook demonstrates how GPTs can be used to intelligently call APIs. It leverages OpenAPI specifications and chained function calls.\n\nThe OpenAPI Specification (OAS) is a universally accepted standard for describing the details of RESTful APIs in a format that machines can read and interpret. It enables both humans and computers to understand the capabilities of a service, and it can be leveraged to show GPT how to call APIs.\n\nThis notebook is divided into two main sections:\n\nHow to convert a sample OpenAPI specification into a list of function definitions for the chat completions API.\nHow to use the chat completions API to intelligently invoke these functions based on user instructions.\n\nWe recommend familiariazing yourself with function-calling before proceding.\n\nHow to convert an OpenAPI specification into function definitions\n\nThe example OpenAPI spec we use here was created using gpt-4. We will transform this sample spec into a set of function definitions that can be supplied to the chat completion API. The model, based on the provided user instructions, generates a JSON object containing the necessary arguments to call these functions.\n\nBefore we proceed, let's inspect this generated spec. OpenAPI specs include details about the API's endpoints, the operations they support, the parameters they accept, the requests they can handle, and the responses they return. The spec is defined in JSON format.\n\nThe endpoints in the spec include operations for:\n\nListing all events\nCreating a new event\nRetrieving an event by ID\nDeleting an event by ID\nUpdating an event name by ID\n\nEach operation in the spec has an operationId, which we will use as the function name when we parse the spec into function specifications. The spec also includes schemas that define the data types and structures of the parameters for each operation.\n\nYou can see the schema here:\n\nNow that we have a good understanding of the OpenAPI spec, we can proceed to parse it into function specifications.\n\nWe can write a simple openapi_to_functions function to generate a list of definitions, where each function is represented as a dictionary containing the following keys:\n\nname: This corresponds to the operation identifier of the API endpoint as defined in the OpenAPI specification.\ndescription: This is a brief description or summary of the function, providing an overview of what the function does.\nparameters: This is a schema that defines the expected input parameters for the function. It provides information about the type of each parameter, whether it is required or optional, and other related details.\n\nFor each of the endpoints defined in the schema, we need to do the following:\n\nResolve JSON references: In an OpenAPI specification, it's common to use JSON references (also known as $ref) to avoid duplication. These references point to definitions that are used in multiple places. For example, if multiple API endpoints return the same object structure, that structure can be defined once and then referenced wherever it's needed. We need to resolve and replace these references with the content they point to.\n\nExtract a name for the functions: We will simply use the operationId as the function name. Alternatively, we could use the endpoint path and operation as the function name.\n\nExtract a description and parameters: We will iterate through the description, summary, requestBody and parameters fields to populate the function's description and parameters.\n\nHere's the implementation:\n\nHow to call these functions with GPT\n\nNow that we have these function definitions, we can leverage GPT to call them intelligently based on user inputs.\n\nIt's important to note that the chat completions API does not execute the function; instead, it generates the JSON that you can use to call the function in your own code.\n\nFor more information on function-calling, refer to our dedicated function-calling guide.\n\nConclusion\n\nWe have demonstrated how to convert OpenAPI specs into function specifications that can be given to GPT for it to intelligently call them, and shown how these can be chained together to perform complex operations.\n\nPossible extensions of this system could include handling more complex user instructions that require conditional logic or looping, integrating with real APIs to perform actual operations, and improving error handling and validation to ensure the instructions are feasible and the function calls are successful."])</script><script>self.__next_f.push([1,"84:T1c75,"])</script><script>self.__next_f.push([1,"\nThis notebook covers how to fine-tune to increase function calling accuracy and reliability. You can find more information on function calling here, and on fine tuning here\n\nFor context, from the function calling notebook above:\ntools is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n\nFunction calling is a very powerful tool when it functions as intended. However, we have seen that as the number of functions increases, and the complexity of the task at hand increases, function calling becomes less accurate (e.g.: more hallucinated invocations, and incorrect invocations).\n\nBefore fine tuning for function calling, it's best to begin with:\n\nImprovements to the function definitions. Make them more clear, and more distinct from one another.\nExperiment with prompt engineering: often a more detailed prompt can help the model call the correct function.\n\nIf the steps above fail to improve function calling to a satisfactory level, then you can try fine tuning for function calling.\n\nOverview\n\nThis notebook contains three sections\n\nAssessing baseline function calling performance:** Evaluating an out-of-the-box gpt-3.5-turbo model on our given function (let's assume that for latency + cost reasons we cannot use gpt-4o for a drone copilot)\nGenerating synthetic data:** Using gpt-4o to create 'golden' set of prompts and function invocations to use as training data\nFine-tuning**: Running the fine tuning job, and evaluating the fine-tuned model\n\nNote: This notebook provides an example of how to create synthetic training data for fine tuning for function calling given just a list of functions. While real-world production test evals are preferable, this method produces strong results and can be used in conjunction with real-world training data.\n\nGetting baseline function calling performance\n\nUtilities\n\nLet's define utility functions for making calls to the Chat Completions API, one to get the completion and one to get the function call.\n\nBaseline testing\n\nLet's build an intelligent drone co-pilot. We want to be able to give the co-pilot commands, and have it either call the function\nfor that command, or deny that request if the command is unfeasible.\nWe can first define a system prompt for the copilot.\n\nNow let's define functions for all of the actions the copilot can take.\n\nFor starters, let's see how function calling performs with some straight forward feasible prompts, and then couple of obviously impossible requests which call the 'reject_request' function.\n\nNice! The model performs quite well with these requests. Now let's try some more difficult requests: requests that are almost feasible and are drone-related, but that the drone cannot actually do, and the pilot should reject.\n\nNow we run into some problems.\nThe model here should reject all of these requests, as they are impossible/conflicting/ambiguous given the functions, however instead the model calls functions that are somewhat related to the request, but incorrect. For example, the model sets follow_me_mode when asked to initiate following on social media.\n\n\nIn this simple case, more prompt engineering may resolve some of these issues, but for the purpose of this example we will demonstrate how fine tuning can be used to improve performance. Additionally, while this case is relatively straightforward, as the number of and complexity of the functions increases, fine tuning becomes more and more impactful.\n\nAgain, our goal here is to improve performance and use less tokens, so fine-tuning allows us to:\n\nOmit function and parameter descriptions: remove the description field from function and parameters\nOmit parameters: remove the entire properties field from the parameters object\nOmit function entirely: remove the entire function object from the functions array\n\nGenerating synthetic data\n\nHelper functions\n\nWe want to generate every invocation of every function, so that we have\nfull coverage of all potential invocations to create synthetic data for. Then, we will use gpt-4o to come up with prompts that would call each invocation, and we will use that prompt - function invocation pair as training data.\n\nGenerating every invocation for a function with fixed enums is more simple, but for a function such as\ncontrol_gimbal we need to set the tilt and pan integer values, so to generate those synthetic invocations we will first set a placeholder, and then later use gpt-4o to come up with reasonable values.\n\nThe functions below take in all the functions from the function list, and look\nat all the potential invocations of those functions given each function's parameters.\nThe functions also account for required parameters, so that all the invocations\nare actually feasible.\n\nLet's generate every invocation for every function first\n\nPrompts:\n\nIn the below snippet, we generate the invocation of each function except for the reject_request function.\n\nTo perform effective fine-tuning we need correctly labeled data. We could manually come up with examples and label the data,\\\nor we can generate synthetic data with the help of gpt-4o\n\nEmpirically, gpt-4o needs a bit more help to get good realistic examples of prompts that would generate the reject_request function, so we'll do that next...\n\nNow that we have all the invocations, let's use gpt-4o to generate prompts that would result in those invocations\n\nNow let's format the training examples properly. For more documentation on the proper training data formatting for fine tuning for function calling, see here: https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples\n\nNow, back to the rejection function. Let's generate some prompts that are nearly possible, but should result in the reject_request function being called. To do so, we queried gpt-4o asking for requests that are related to, but not quite possible with, the given list of functions.\n\nNow combine all the training examples together\n\nFine tuning\n\nFinally, we can kick off the fine-tuning job\n\nIn addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a job, or cancel a job.\n\nAfter a fine-tuning job has finished, you can also see metrics around how the training process went by querying a fine-tuning job, extracting a file ID from the result_files, and then retrieving that files content. Each results CSV file has the following columns: step, train_loss, train_accuracy, valid_loss, and valid_mean_token_accuracy. While metrics can he helpful, evaluating samples from the fine-tuned model provides the most relevant sense of model quality.\n\nEvaluations\n\nGreat! We trained a fine-tuned model for function calling. Let's see how it does on our evaluation set for prompts that the drone assistant\nshould automatically reject.\n\nGreat! While the original model only rejected 60%, the fine tuned model rejected 100% requests and used less tokens to do so.\n\nConclusion\n\nCongratulations! You are now ready to fine tune your model for function calling. We can't wait to see what you build."])</script><script>self.__next_f.push([1,"85:T41e,"])</script><script>self.__next_f.push([1,"\nThis notebook demonstrates how to use GPT's visual capabilities with a video. GPT-4o doesn't take videos as input directly, but we can use vision and the 128K context window to describe the static frames of a whole video at once. We'll walk through two examples:\n\nUsing GPT-4o to get a description of a video\nGenerating a voiceover for a video with GPT-o and the TTS API\n\n1. Using GPT's visual capabilities to get a description of a video\n\nFirst, we use OpenCV to extract frames from a nature video containing bisons and wolves:\n\nDisplay frames to make sure we've read them in correctly:\n\nOnce we have the video frames, we craft our prompt and send a request to GPT (Note that we don't need to send every frame for GPT to understand what's going on):\n\n2. Generating a voiceover for a video with GPT-4 and the TTS API\n\nLet's create a voiceover for this video in the style of David Attenborough. Using the same video frames we prompt GPT to give us a short script:\n\nNow we can pass the script to the TTS API where it will generate an mp3 of the voiceover:"])</script><script>self.__next_f.push([1,"86:T2d67,"])</script><script>self.__next_f.push([1,"\nDALL·E-3 is the latest version of our DALL-E text-to-image generation models. As the current state of the art in text-to-image generation, DALL·E is capable of generating high-quality images across a wide variety of domains. If you're interested in more technical details of how DALL·E-3 was built, you can read more about in our research paper. I'll be going over some of the new features and capabilities of DALL·E-3 in this article, as well as some examples of what new products you can build with the API.\n\nAs a reminder, the Image generation API hasn't changed and maintains the same endpoints and formatting as with DALL·E-2. If you're looking for a guide on how to use the Image API, see the Cookbook article on the subject.\n\nThe only API endpoint available for use with DALL·E-3 right now is Generations (/v1/images/generations). We don’t support variations or inpainting yet, though the Edits and Variations endpoints are available for use with DALL·E-2.\n\nGenerations\n\nThe generation API endpoint creates an image based on a text prompt. There’s a couple new parameters that we've added to enhance what you can create with our models. Here’s a quick overview of the options:\n\nNew parameters:\n\nmodel** (‘dall-e-2’ or ‘dall-e-3’): This is the model you’re generating with. Be careful to set it to ‘dall-e-3’ as it defaults to ‘dall-e-2’ if empty.\nstyle** (‘natural’ or ‘vivid’): The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. Defaults to ‘vivid’.\nquality** (‘standard’ or ‘hd’): The quality of the image that will be generated. ‘hd’ creates images with finer details and greater consistency across the image. Defaults to ‘standard’.\n\nOther parameters:\n\nprompt** (str): A text description of the desired image(s). The maximum length is 1000 characters. Required field.\nn** (int): The number of images to generate. Must be between 1 and 10. Defaults to 1. For dall-e-3, only n=1 is supported.\nsize** (...): The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for DALL·E-2 models. Must be one of 1024x1024, 1792x1024, or 1024x1792 for DALL·E-3 models.\nresponse_format** ('url' or 'b64_json'): The format in which the generated images are returned. Must be one of \"url\" or \"b64_json\". Defaults to \"url\".\nuser** (str): A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse. Learn more.\n\nNew Features\n\nOur launch of DALL·E-3 comes with lots of new features and capabilities to help you generate the images you want. Here’s a quick overview of what’s new:\n\nPrompt Rewriting\n\nA new feature in the latest DALL·E-3 API is prompt rewriting, where we use GPT-4 to optimize all of your prompts before they’re passed to DALL-E. In our research, we’ve seen that using very detailed prompts give significantly better results. You can read more about our captioning, prompting, and safety mitigations in the DALL·E-3 research paper.\n\nKeep in mind that this feature isn’t able to be disabled at the moment, though you can achieve a high level of fidelity by simply giving instructions to the relabeler in your prompt, as I'll show below with examples.\n\nPrompt Rewriting\n\nStandard vs HD Quality\n\nDALL·E-3 introduces a new 'quality' parameter that allows you to adjust the level of detail and organization in all of your generations. The 'standard' quality generations are the DALL·E-3 you're familiar with, with 'hd' generations bringing a new level of attention to detail and adherence to your prompt. Keep in mind that setting your generation quality to ‘hd’ does increase the cost per image, as well as often increasing the time it takes to generate by ~10 seconds or so.\n\nFor example, here we have two different icons in 'hd' and 'standard' quality. Often the choice between either quality is up to taste, but 'hd' often wins when the task requires more ability to capture details and textures or better composition of a scene.\n\nIcons\n\nHere's another example, this time with a prompt of 'An infinite, uniform grid of tessellated cubes.', which DALL·E conveniently rewrites as \"An infinite, uniform grid of tessellated cubes painted carefully in an isometric perspective. The cubes are meticulously arranged in such a way that they seem to stretch endlessly into the distance. Each cube is identical to the next, with light reflecting consistently across all surfaces, underscoring their uniformity. This is a digitally rendered image.\":\n\nCubes\n\nNew Sizes\n\nDALL·E-3 accepts three different image sizes: 1024px by 1024px, 1792px by 1024px, and 1024px by 1792px. Beyond giving more flexibility in terms of aspect ratio, these sizes can have significant effects on the style and context of your generated image. For example, vertical images might work better when you’re looking for an image that looks like it was taken by a cellphone camera, or horizontal images may work better for landscape paintings or digital designs.\n\nTo demonstrate this difference, here’s multiple variations on the same input prompt with a different aspect ratio. In this case, my prompt was: “Professional photoshoot of a Chemex brewer in the process of brewing coffee.” (For reference, this is a photo of a real Chemex brewer).\n\nHere is the generation in square form (in both HD and standard qualities):\n\nsquare_coffee\n\nYou can see how these images are framed closely to the item and seem to be taken in a more closed space with various surrounding items nearby.\n\nHere are the results on the same prompts with a wider aspect ratio:\n\nwide_coffee\n\nCompared to the previous generations, these come in the form of close-ups. The background is blurred, with greater focus on the item itself, more like professionally organized photoshoots rather than quick snaps.\n\nLastly, we have the vertical aspect ratio:\n\ntall_coffee\n\nThese feel more akin to cellphone images, with a more candid appearance. There’s more action involved: the slowly dripping coffee or the active pour from the pot.\n\nNew Styles\n\nDALL·E-3 introduces two new styles: natural and vivid. The natural style is more similar to the DALL·E-2 style in its 'blander' realism, while the vivid style is a new style that leans towards generating hyper-real and cinematic images. For reference, all DALL·E generations in ChatGPT are generated in the 'vivid' style.\n\nThe natural style is specifically useful in cases where DALL·E-3 over-exaggerates or confuses a subject that's supposed to be more simple, subdued, or realistic. I've often used it for logo generation, stock photos, or other cases where I'm trying to match a real-world object.\n\nHere's an example of the same prompt as above in the vivid style. The vivid is far more cinematic (and looks great), but might pop too much if you're not looking for that.\n\nvivid_coffee\n\nThere's many cases in which I prefer the natural style, such as this example of a painting in the style of Thomas Cole's 'Desolation':\n\nthomas_cole\n\nExamples and Prompts\n\nTo help you get started building with DALL·E-3, I've come up with a few examples of products you could build with the API, as well as collected some styles and capabilities that seem to be unique to DALL·E-3 at the moment. I've also listed some subjects that I'm struggling to prompt DALL·E-3 to generate in case you want to try your hand at it.\n\nIcon Generation\n\nHave you ever struggled to find the perfect icon for your website or app? It would be awesome to see a custom icon generator app that lets you pick the style, size, and subject of your icon, and then generates a custom SVG from the DALL·E generation. Here's some examples of helpful website icons I generated with DALL·E-3:\n\nicon_set\n\nIn this case, I used Potrace to convert the images to SVGs, which you can download here. This is what I used to convert the images:\n\npotrace -s cat.jpg -o cat.svg\n\nYou might need to boost the brightness and contrast of the image before converting it to an SVG. I used the following command to do so:\n\nconvert cat.jpg -brightness-contrast 50x50 cat.jpg\n\nLogo Generation\n\nDALL·E-3 is great at jumpstarting the logo creation process for your company or product. By prompting DALL·E to create 'Vector logo design of a Greek statue, minimalistic, with a white background' I achieved the following:\n\nlogo_greece\n\nHere's another logo I created, this time for an Arabian coffee shop:\n\nlogo_arabia\n\nIn the case of iterating on an existing logo, I took OpenAI's logo, asked GPT-4V to describe it, and then asked DALL·E to generate variations on the logo:\n\niteration\n\nCustom Tattoos\n\nDALL·E-3 is great at generating line art, which might be useful for generating custom tattoos. Here's some line art I generated with DALL·E-3:\n\ntattoos\n\nDie-Cut Stickers \u0026 T-Shirts\n\nWhat if you could generate custom die-cut stickers and t-shirts with DALL·E-3, integrating with a print-on-demand service like Printful or Stickermule? You could have a custom sticker or t-shirt in minutes, with no design experience required. Here's some examples of stickers I generated with DALL·E-3:\n\nstickers\n\nMinecraft Skins\n\nWith some difficulty, I managed to prompt DALL·E-3 to generate Minecraft skins. I'm sure with some clever prompting you could get DALL·E-3 to reliably generate incredible Minecraft skins. It might be hard to use the words 'Minecraft' since DALL·E might think you are trying to generate content from the game itself, instead, you can communicate the idea differently: \"Flat player skin texture of a ninja skin, compatible with Minecraftskins.com or Planet Minecraft.\"\n\nHere's what I managed to create. They might need some work, but I think they're a good start:\n\nminecraft\n\nAnd much more...\n\nHere's some ideas I've had that I haven't had time to try yet:\n\nCustom emojis or Twitch emotes?\nVector illustrations?\nPersonalized Bitmoji-style avatars?\nAlbum art?\nCustom greeting cards?\nPoster/flyer 'pair-programming' with DALL·E?\n\nShowcase\n\nWe're really just starting to figure out what DALL·E-3 is capable of. Here's some of the best styles, generations, and prompts I've seen so far. I've been unable to locate the original authors of some of these images, so if you know who created them, please let me know!\n\ncollage\n\nSources:\n\n@scharan79 on Reddit\n@TalentedJuli on Reddit\n@Wild-Culture-5068 on Reddit\n@popsicle_pope on Reddit\n@gopatrik on Twitter\n@ARTiV3RSE on Twitter\n@willdepue on Twitter\nVarious OpenAI employees\n\nChallenges\n\nDALL·E-3 is still very new and there's still a lot of things it struggles with (or maybe I just haven't figured out how to prompt it correctly yet). Here's some challenges which you might want to try your hand at:\n\nWeb Design\n\nDALL·E really struggles at generating real looking websites, apps, etc. and often generates what looks like a portfolio page of a web designer. Here's the best I've gotten so far:\n\nwebsites\n\nSeamless Textures\n\nIt feels like DALL·E-3 is so close to being able to generate seamless textures. Often they come out great, just slightly cutoff or with a few artifacts. See examples below:\n\nseamless\n\nFonts\n\nUsing DALL·E to generate custom fonts or iterate on letter designs could be really cool, but I haven't been able to get it to work yet. Here's the best I've gotten so far:\n\nfonts\n\nMore Resources\n\nThanks for reading! If you're looking for more resources on DALL·E-3, here are some related links:\n\nDALL·E-3 Blog Post\nDALL·E-3 Research Paper\nImage API Documentation\nImage API Cookbook\n"])</script><script>self.__next_f.push([1,"87:T1185,"])</script><script>self.__next_f.push([1,"\nTLDR: Developers can now specify seed parameter in the Chat Completion request to receive (mostly) consistent outputs. To help you keep track of these changes, we expose the system_fingerprint field. If this value is different, you may see different outputs due to changes we've made on our systems. Please note that this feature is in beta and only currently supported for gpt-4-1106-preview and gpt-3.5-turbo-1106.\n\nContext\n\nReproducibility has always been a big request from user communities when using our APIs. For instance, when granted the capability of getting reproducible numerical result, users can unlock quite a bit of use cases that’s sensitive to numerical changes.\n\nModel level features for consistent outputs\n\nThe Chat Completions and Completions APIs are non-deterministic by default (which means model outputs may differ from request to request), but now offer some control towards deterministic outputs using a few model level controls.\n\nThis can unlock consistent completions which enables full control on the model behaviors for anything built on top of the APIs, and quite useful for reproducing results and testing so you know get peace of mind from knowing exactly what you’d get.\n\nImplementing consistent outputs\n\nTo receive mostly deterministic outputs across API calls:\n\nSet the seed parameter to any integer of your choice, but use the same value across requests. For example, 12345.\nSet all other parameters (prompt, temperature, top_p, etc.) to the same values across requests.\nIn the response, check the system_fingerprint field. The system fingerprint is an identifier for the current combination of model weights, infrastructure, and other configuration options used by OpenAI servers to generate the completion. It changes whenever you change request parameters, or OpenAI updates numerical configuration of the infrastructure serving our models (which may happen a few times a year).\n\nIf the seed, request parameters, and system_fingerprint all match across your requests, then model outputs will mostly be identical. There is a small chance that responses differ even when request parameters and system_fingerprint match, due to the inherent non-determinism of our models.\n\nModel level controls for consistent outputs - seed and system_fingerprint\n\nseed\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nsystem_fingerprint\n\nThis fingerprint represents the backend configuration that the model runs with. It can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.This is the indicator on whether users should expect \"almost always the same result\".\n\nExample: Generating a short excerpt with a fixed seed\n\nIn this example, we will demonstrate how to generate a short excerpt using a fixed seed. This can be particularly useful in scenarios where you need to generate consistent results for testing, debugging, or for applications that require consistent outputs.\nPython SDK\nNote\nSwitch to latest version of the SDK (1.3.3 at time of writing).\nFirst, let's try generating few different versions of a short excerpt about \"a journey to Mars\" without the seed parameter. This is the default behavior:\nNow, let's try to tun the same code with a constant seed of 123 and temperature of 0 and compare the responses and system_fingerprint.\nAs we can observe, the seed parameter allows us to generate much more consistent results.\nConclusion\n\nWe demonstrated how to use a fixed integer seed to generate consistent outputs from our model. This is particularly useful in scenarios where reproducibility is important. However, it's important to note that while the seed ensures consistency, it does not guarantee the quality of the output. Note that when you want to use reproducible outputs, you need to set the seed to the same integer across Chat Completions calls. You should also match any other parameters like temperature, max_tokens etc. Further extension of reproducible outputs could be to use consistent seed when benchmarking/evaluating the performance of different prompts or models, to ensure that each version is evaluated under the same conditions, making the comparisons fair and the results reliable."])</script><script>self.__next_f.push([1,"88:T28e2,"])</script><script>self.__next_f.push([1,"The new Assistants API is a stateful evolution of our Chat Completions API meant to simplify the creation of assistant-like experiences, and enable developer access to powerful tools like Code Interpreter and Retrieval.\nAssistants API Diagram\nChat Completions API vs Assistants API\n\nThe primitives of the Chat Completions API are Messages, on which you perform a Completion with a Model (gpt-3.5-turbo, gpt-4, etc). It is lightweight and powerful, but inherently stateless, which means you have to manage conversation state, tool definitions, retrieval documents, and code execution manually.\n\nThe primitives of the Assistants API are\n\nAssistants, which encapsulate a base model, instructions, tools, and (context) documents,\nThreads, which represent the state of a conversation, and\nRuns, which power the execution of an Assistant on a Thread, including textual responses and multi-step tool use.\n\nWe'll take a look at how these can be used to create powerful, stateful experiences.\n\nSetup\n\nPython SDK\nNote\nWe've updated our Python SDK to add support for the Assistants API, so you'll need to update it to the latest version (1.2.3 at time of writing).\n\nAnd make sure it's up to date by running:\n\nPretty Printing Helper\n\nComplete Example with Assistants API\n\nAssistants\n\nThe easiest way to get started with the Assistants API is through the Assistants Playground.\n\nAssistants Playground\n\nLet's begin by creating an assistant! We'll create a Math Tutor just like in our docs.\n\nCreating New Assistant\n\nYou can view Assistants you've created in the Assistants Dashboard.\n\nAssistants Dashboard\n\nYou can also create Assistants directly through the Assistants API, like so:\n\nRegardless of whether you create your Assistant through the Dashboard or with the API, you'll want to keep track of the Assistant ID. This is how you'll refer to your Assistant throughout Threads and Runs.\n\nNext, we'll create a new Thread and add a Message to it. This will hold the state of our conversation, so we don't have re-send the entire message history each time.\n\nThreads\n\nCreate a new thread:\n\nThen add the Message to the thread:\nNote\nEven though you're no longer sending the entire history each time, you will still be charged for the tokens of the entire conversation history with each Run.\n\nRuns\n\nNotice how the Thread we created is not associated with the Assistant we created earlier! Threads exist independently from Assistants, which may be different from what you'd expect if you've used ChatGPT (where a thread is tied to a model/GPT).\n\nTo get a completion from an Assistant for a given Thread, we must create a Run. Creating a Run will indicate to an Assistant it should look at the messages in the Thread and take action: either by adding a single response, or using tools.\nNote\nRuns are a key difference between the Assistants API and Chat Completions API. While in Chat Completions the model will only ever respond with a single message, in the Assistants API a Run may result in an Assistant using one or multiple tools, and potentially adding multiple messages to the Thread.\n\nTo get our Assistant to respond to the user, let's create the Run. As mentioned earlier, you must specify both the Assistant and the Thread.\n\nUnlike creating a completion in the Chat Completions API, creating a Run is an asynchronous operation. It will return immediately with the Run's metadata, which includes a status that will initially be set to queued. The status will be updated as the Assistant performs operations (like using tools and adding messages).\n\nTo know when the Assistant has completed processing, we can poll the Run in a loop. (Support for streaming is coming soon!) While here we are only checking for a queued or in_progress status, in practice a Run may undergo a variety of status changes which you can choose to surface to the user. (These are called Steps, and will be covered later.)\n\nMessages\n\nNow that the Run has completed, we can list the Messages in the Thread to see what got added by the Assistant.\n\nAs you can see, Messages are ordered in reverse-chronological order – this was done so the most recent results are always on the first page (since results can be paginated). Do keep a look out for this, since this is the opposite order to messages in the Chat Completions API.\n\nLet's ask our Assistant to explain the result a bit further!\n\nThis may feel like a lot of steps to get a response back, especially for this simple example. However, you'll soon see how we can add very powerful functionality to our Assistant without changing much code at all!\n\nExample\n\nLet's take a look at how we could potentially put all of this together. Below is all the code you need to use an Assistant you've created.\n\nSince we've already created our Math Assistant, I've saved its ID in MATH_ASSISTANT_ID. I then defined two functions:\n\nsubmit_message: create a Message on a Thread, then start (and return) a new Run\nget_response: returns the list of Messages in a Thread\n\nI've also defined a create_thread_and_run function that I can re-use (which is actually almost identical to the client.beta.threads.create_and_run compound function in our API ;) ). Finally, we can submit our mock user requests each to a new Thread.\n\nNotice how all of these API calls are asynchronous operations; this means we actually get async behavior in our code without the use of async libraries! (e.g. asyncio)\n\nOnce all Runs are going, we can wait on each and get the responses.\n\nEt voilà!\n\nYou may have noticed that this code is not actually specific to our math Assistant at all... this code will work for any new Assistant you create simply by changing the Assistant ID! That is the power of the Assistants API.\n\nTools\n\nA key feature of the Assistants API is the ability to equip our Assistants with Tools, like Code Interpreter, Retrieval, and custom Functions. Let's take a look at each.\n\nCode Interpreter\n\nLet's equip our Math Tutor with the Code Interpreter tool, which we can do from the Dashboard...\n\nEnabling code interpreter\n\n...or the API, using the Assistant ID.\n\nNow, let's ask the Assistant to use its new tool.\n\nAnd that's it! The Assistant used Code Interpreter in the background, and gave us a final response.\n\nFor some use cases this may be enough – however, if we want more details on what precisely an Assistant is doing we can take a look at a Run's Steps.\n\nSteps\n\nA Run is composed of one or more Steps. Like a Run, each Step has a status that you can query. This is useful for surfacing the progress of a Step to a user (e.g. a spinner while the Assistant is writing code or performing retrieval).\n\nLet's take a look at each Step's step_details.\n\nWe can see the step_details for two Steps:\n\ntool_calls (plural, since it could be more than one in a single Step)\nmessage_creation\n\nThe first Step is a tool_calls, specifically using the code_interpreter which contains:\n\ninput, which was the Python code generated before the tool was called, and\noutput, which was the result of running the Code Interpreter.\n\nThe second Step is a message_creation, which contains the message that was added to the Thread to communicate the results to the user.\n\nRetrieval\n\nAnother powerful tool in the Assistants API is Retrieval: the ability to upload files that the Assistant will use as a knowledge base when answering questions. This can also be enabled from the Dashboard or the API, where we can upload files we want to be used.\n\nEnabling retrieval\nNote\nThere are more intricacies in Retrieval, like Annotations, which may be covered in another cookbook.\n\nFunctions\n\nAs a final powerful tool for your Assistant, you can specify custom Functions (much like the Function Calling in the Chat Completions API). During a Run, the Assistant can then indicate it wants to call one or more functions you specified. You are then responsible for calling the Function, and providing the output back to the Assistant.\n\nLet's take a look at an example by defining a display_quiz() Function for our Math Tutor.\n\nThis function will take a title and an array of questions, display the quiz, and get input from the user for each:\n\ntitle\nquestions\n  question_text\n  question_type: [MULTIPLE_CHOICE, FREE_RESPONSE]\n  choices: [\"choice 1\", \"choice 2\", ...]\n\nUnfortunately I don't know how to get user input within a Python Notebook, so I'll be mocking out responses with get_mock_response.... This is where you'd get the user's actual input.\n\nHere's what a sample quiz would look like:\n\nNow, let's define the interface of this function in JSON format, so our Assistant can call it:\n\nOnce again, let's update our Assistant either through the Dashboard or the API.\n\nEnabling custom function\nNote\nPasting the function JSON into the Dashboard was a bit finicky due to indentation, etc. I just asked ChatGPT to format my function the same as one of the examples on the Dashboard :).\n\nAnd now, we ask for a quiz.\n\nNow, however, when we check the Run's status we see requires_action! Let's take a closer.\n\nThe required_action field indicates a Tool is waiting for us to run it and submit its output back to the Assistant. Specifically, the display_quiz function! Let's start by parsing the name and arguments.\nNote\nWhile in this case we know there is only one Tool call, in practice the Assistant may choose to call multiple tools.\n\nNow let's actually call our display_quiz function with the arguments provided by the Assistant:\n\nGreat! (Remember these responses are the one's we mocked earlier. In reality, we'd be getting input from the back from this function call.)\n\nNow that we have our responses, let's submit them back to the Assistant. We'll need the tool_call ID, found in the tool_call we parsed out earlier. We'll also need to encode our listof responses into a str.\n\nWe can now wait for the Run to complete once again, and check our Thread!\n\nWoohoo 🎉\n\nConclusion\n\nWe covered a lot of ground in this notebook, give yourself a high-five! Hopefully you should now have a strong foundation to build powerful, stateful experiences with tools like Code Interpreter, Retrieval, and Functions!\n\nThere's a few sections we didn't cover for the sake of brevity, so here's a few resources to explore further:\n\nAnnotations: parsing file citations\nFiles: Thread scoped vs Assistant scoped\nParallel Function Calls: calling multiple tools in a single Step\nMulti-Assistant Thread Runs: single Thread with Messages from multiple Assistants\nStreaming: coming soon!\n\nNow go off and build something amazing!"])</script><script>self.__next_f.push([1,"89:Tfd7,"])</script><script>self.__next_f.push([1,"This notebook demonstrates how to  build a semantic search application  using OpenAI and MongoDB Atlas vector search\nStep 1: Setup the environment\n\nThere are 2 pre-requisites for this:\n\nMongoDB Atlas cluster: To create a forever free MongoDB Atlas cluster, first, you need to create a MongoDB Atlas account if you don't already have one. Visit the MongoDB Atlas website and click on “Register.” Visit the MongoDB Atlas dashboard and set up your cluster. In order to take advantage of the $vectorSearch operator in an aggregation pipeline, you need to run MongoDB Atlas 6.0.11 or higher. This tutorial can be built using a free cluster. When you’re setting up your deployment, you’ll be prompted to set up a database user and rules for your network connection. Please ensure you save your username and password somewhere safe and have the correct IP address rules in place so your cluster can connect properly. If you need more help getting started, check out our tutorial on MongoDB Atlas.\n\nOpenAI API key   To create your OpenAI key, you'll need to create an account. Once you have that, visit the OpenAI platform. Click on your profile icon in the top right of the screen to get the dropdown menu and select “View API keys”.\n\nNote: After executing the step above you will be prompted to enter the credentials.\nFor this tutorial, we will be using the\nMongoDB sample dataset. Load the sample dataset using the Atlas UI. We'll be using the “sample_mflix” database, which contains a “movies” collection where each document contains fields like title, plot, genres, cast, directors, etc.\n\nStep 2: Setup embeddings generation function\nStep 3: Create and store embeddings\n\nEach document in the sample dataset sample_mflix.movies corresponds to a movie; we will execute an operation to create a vector embedding for the data in the \"plot\" field and store it in the database. Creating vector embeddings using OpenAI embeddings endpoint is necessary for performing a similarity search based on intent.\nAfter executing the above, the documents in \"movies\" collection will contain an additional field of \"embedding\", as defined by the EMBEDDDING_FIELD_NAME variable, apart from already existing fields like title, plot, genres, cast, directors, etc.\nNote: We are restricting this to just 500 documents in the interest of time. If you want to do this over the entire dataset of 23,000+ documents in our sample_mflix database, it will take a little while. Alternatively, you can use the sample_mflix.embedded_movies collection which includes a pre-populated plot_embedding field that contains embeddings created using OpenAI's text-embedding-3-small embedding model that you can use with the Atlas Search vector search feature.\n\nStep 4: Create a vector search index\n\nWe will create Atlas Vector Search Index on this collection which will allow us to perform the Approximate KNN search, which powers the semantic search.\nWe will cover 2 ways to create this index - Atlas UI and using MongoDB python driver.\n\n(Optional) Documentation: Create a Vector Search Index\nNow head over to Atlas UI and create an Atlas Vector Search index using the steps descibed here.  The 'dimensions' field with value 1536, corresponds to openAI text-embedding-ada002.\n\nUse the definition given below in the JSON editor on the Atlas UI.\n\n{\n  \"mappings\": {\n    \"dynamic\": true,\n    \"fields\": {\n      \"embedding\": {\n        \"dimensions\": 1536,\n        \"similarity\": \"dotProduct\",\n        \"type\": \"knnVector\"\n      }\n    }\n  }\n}\n(Optional) Alternatively, we can use pymongo driver to create these vector search indexes programatically\nThe python command given in the cell below will create the index (this only works for the most recent version of the Python Driver for MongoDB and MongoDB server version 7.0+ Atlas cluster).\nStep 5: Query your data\n\nThe results for the query here finds movies which have semantically similar plots to the text captured in the query string, rather than being based on the keyword search.\n\n(Optional) Documentation: Run Vector Search Queries"])</script><script>self.__next_f.push([1,"8a:T1f53,"])</script><script>self.__next_f.push([1,"In this notebook we will look into building an RAG pipeline and evaluating it with LlamaIndex. It has following 3 sections.\n\nUnderstanding Retrieval Augmented Generation (RAG).\nBuilding RAG with LlamaIndex.\nEvaluating RAG with LlamaIndex.\nRetrieval Augmented Generation (RAG)\n\nLLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n\nIn RAG, your data is loaded and and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n\nEven if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\nRAG Overview\nStages within RAG\n\nThere are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n\nLoading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n\nIndexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n\nStoring: Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n\nQuerying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n\nEvaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\nBuild RAG system.\n\nNow that we have understood the significance of RAG system, let's build a simple RAG pipeline.\nSet Your OpenAI API Key\nLet's use Paul Graham Essay text for building RAG pipeline.\nDownload Data\nLoad Data and Build Index.\nBuild a QueryEngine and start querying.\nCheck response.\nBy default it retrieves two similar nodes/ chunks. You can modify that in vector_index.as_query_engine(similarity_top_k=k).\n\nLet's check the text in each of these retrieved nodes.\nWe have built a RAG pipeline and now need to evaluate its performance. We can assess our RAG system/query engine using LlamaIndex's core evaluation modules. Let's examine how to leverage these tools to quantify the quality of our retrieval-augmented generation system.\nEvaluation\n\nEvaluation should serve as the primary metric for assessing your RAG application. It determines whether the pipeline will produce accurate responses based on the data sources and a range of queries.\n\nWhile it's beneficial to examine individual queries and responses at the start, this approach may become impractical as the volume of edge cases and failures increases. Instead, it may be more effective to establish a suite of summary metrics or automated evaluations. These tools can provide insights into overall system performance and indicate specific areas that may require closer scrutiny.\n\nIn a RAG system, evaluation focuses on two critical aspects:\n\nRetrieval Evaluation:** This assesses the accuracy and relevance of the information retrieved by the system.\nResponse Evaluation:** This measures the quality and appropriateness of the responses generated by the system based on the retrieved information.\nQuestion-Context Pair Generation:\n\nFor the evaluation of a RAG system, it's essential to have queries that can fetch the correct context and subsequently generate an appropriate response. LlamaIndex offers a generate_question_context_pairs module specifically for crafting questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluation. For more details on Question Generation, please refer to the documentation.\nRetrieval Evaluation:\n\nWe are now prepared to conduct our retrieval evaluations. We will execute our RetrieverEvaluator using the evaluation dataset we have generated.\n\nWe first create the Retriever and then define two functions: get_eval_results, which operates our retriever on the dataset, and display_results, which presents the outcomes of the evaluation.\nLet's create the retriever.\nDefine RetrieverEvaluator. We use Hit Rate and MRR metrics to evaluate our Retriever.\n\nHit Rate:\n\nHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n\nMean Reciprocal Rank (MRR):\n\nFor each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n\nLet's check these metrics to check the performance of out retriever.\nLet's define a function to display the Retrieval evaluation results in table format.\nObservation:\n\nThe Retriever with OpenAI Embedding  demonstrates a performance with a hit rate of 0.7586, while the MRR, at 0.6206, suggests there's room for improvement in ensuring the most relevant results appear at the top. The observation that MRR is less than the hit rate indicates that the top-ranking results aren't always the most relevant. Enhancing MRR could involve the use of rerankers, which refine the order of retrieved documents. For a deeper understanding of how rerankers can optimize retrieval metrics, refer to the detailed discussion in our blog post.\nResponse Evaluation:\n\nFaithfulnessEvaluator: Measures if the response from a query engine matches any source nodes which is useful for measuring if the response is hallucinated.\nRelevancy Evaluator: Measures if the response + source nodes match the query.\nFaithfulness Evaluator\nLet's start with FaithfulnessEvaluator.\nWe will use gpt-3.5-turbo for generating response for a given query and gpt-4 for evaluation.\n\nLet's create service_context seperately for gpt-3.5-turbo and gpt-4.\nCreate a QueryEngine with gpt-3.5-turbo service_context to generate response for the query.\nCreate a  FaithfulnessEvaluator.\n\n\n\nLet's evaluate on one question.\nGenerate response first and use faithfull evaluator.\nRelevancy Evaluator\nRelevancyEvaluator is useful to measure if the response and source nodes (retrieved context) match the query. Useful to see if response actually answers the query.\nInstantiate RelevancyEvaluator for relevancy evaluation with gpt-4\nLet's do relevancy evaluation for one of the query.\nBatch Evaluator:\n\nNow that we have done FaithFulness and Relevancy Evaluation independently. LlamaIndex has BatchEvalRunner to compute multiple evaluations in batch wise manner.\nObservation:\n\nFaithfulness score of 1.0 signifies that the generated answers contain no hallucinations and are entirely based on retrieved context.\n\nRelevancy score of 1.0 suggests that the answers generated are consistently aligned with the retrieved context and the queries.\nConclusion\nIn this notebook, we have explored how to build and evaluate a RAG pipeline using LlamaIndex, with a specific focus on evaluating the retrieval system and generated responses within the pipeline.\n\nLlamaIndex offers a variety of other evaluation modules as well, which you can explore further here"])</script><script>self.__next_f.push([1,"8b:T18d5,"])</script><script>self.__next_f.push([1,"This notebook shows how to use LLMs in combination with Neo4j, a graph database, to perform Retrieval Augmented Generation (RAG).\n\nWhy use RAG?\n\nIf you want to use LLMs to generate answers based on your own content or knowledge base, instead of providing large context when prompting the model, you can fetch the relevant information in a database and use this information to generate a response.\n\nThis allows you to:\nReduce hallucinations\nProvide relevant, up to date information to your users\nLeverage your own content/knowledge base\n\nWhy use a graph database?\n\nIf you have data where relationships between data points are important and you might want to leverage that, then it might be worth considering graph databases instead of traditional relational databases.\n\nGraph databases are good to address the following:\nNavigating deep hierarchies\nFinding hidden connections between items\nDiscovering relationships between items\n\nUse cases\n\nGraph databases are particularly relevant for recommendation systems, network relationships or analysing correlation between data points.\n\nExample use cases for RAG with graph databases include:\nRecommendation chatbot\nAI-augmented CRM\nTool to analyse customer behavior with natural language\n\nDepending on your use case, you can assess whether using a graph database makes sense.\n\nIn this notebook, we will build a product recommendation chatbot, with a graph database that contains Amazon products data.\n\nSetup\n\nWe will start by installing and importing the relevant libraries.\n\nMake sure you have your OpenAI account set up and you have your OpenAI API key handy.\nDataset\n\nWe will use a dataset that was created from a relational database and converted to a json format, creating relationships between entities with the completions API.\n\nWe will then load this data into the graph db to be able to query it.\nLoading dataset\nConnecting to db\nImporting data\nQuerying the database\nCreating vector indexes\n\nIn order to efficiently search our database for terms closely related to user queries, we need to use embeddings. To do this, we will create vector indexes on each type of property.\n\nWe will be using the OpenAIEmbeddings Langchain utility. It's important to note that Langchain adds a pre-processing step, so the embeddings will slightly differ from those generated directly with the OpenAI embeddings API.\nQuerying the database directly\nUsing GraphCypherQAChain, we can generate queries against the database using Natural Language.\nExtracting entities from the prompt\n\nHowever, there is little added value here compared to just writing the Cypher queries ourselves, and it is prone to error.\n\nIndeed, asking an LLM to generate a Cypher query directly might result in the wrong parameters being used, whether it's the entity type or the relationship type, as is the case above.\n\nWe will instead use LLMs to decide what to search for, and then generate the corresponding Cypher queries using templates.\n\nFor this purpose, we will instruct our model to find relevant entities in the user prompt that can be used to query our database.\nGenerating queries\n\nNow that we know what to look for, we can generate the corresponding Cypher queries to query our database.\n\nHowever, the entities extracted might not be an exact match with the data we have, so we will use the GDS cosine similarity function to return products that have relationships with entities similar to what the user is asking.\nFinding similar items\n\nWe can then leverage the graph db to find similar products based on common characteristics.\n\nThis is where the use of a graph db really comes into play.\n\nFor example, we can look for products that are the same category and have another characteristic in common, or find products that have relationships to the same entities.\n\nThis criteria is arbitrary and completely depends on what is the most relevant in relation to your use case.\nFinal result\n\nNow that we have all the pieces working, we will stitch everything together.\n\nWe can also add a fallback option to do a product name/title similarity search if we can't find relevant entities in the user prompt.\n\nWe will explore 2 options, one with a Langchain agent for a conversational experience, and one that is more deterministic based on code only.\n\nDepending on your use case, you might choose one or the other option and tailor it to your needs.\nBuilding a Langchain agent\n\nWe will create a Langchain agent to handle conversations and probing the user for more context.\n\nWe need to define exactly how the agent should behave, and give it access to our query and similarity search tools.\nBuilding a code-only experience\n\nAs our experiments show, using an agent for this type of task might not be the best option.\n\nIndeed, the agent seems to retrieve results from the tools, but comes up with made-up responses.\n\nFor this specific use case, if the conversational aspect is less relevant, we can actually create a function that will call our previously-defined tasks and provide an answer.\nConclusion\n\nUser experience\n\nWhen the primary objective is to extract specific information from our database, Large Language Models (LLMs) can significantly enhance our querying capabilities.\n\nHowever, it's crucial to base much of this process on robust code logic to ensure a foolproof user experience.\n\nFor crafting a genuinely conversational chatbot, further exploration in prompt engineering is necessary, possibly incorporating few-shot examples. This approach helps mitigate the risk of generating inaccurate or misleading information and ensures more precise responses.\n\nUltimately, the design choice depends on the desired user experience. For instance, if the aim is to create a visual recommendation system, the importance of a conversational interface is less relevant.\n\nWorking with a knowledge graph\n\nRetrieving content from a knowledge graph adds complexity but can be useful if you want to leverage connections between items.\n\nThe querying part of this notebook would work on a relational database as well, the knowledge graph comes in handy when we want to couple the results with similar items that the graph is surfacing.\n\nConsidering the added complexity, make sure using a knowledge graph is the best option for your use case.\nIf it is the case, feel free to refine what this cookbook presents to match your needs and perform even better!"])</script><script>self.__next_f.push([1,"8c:T4f6,"])</script><script>self.__next_f.push([1,"\nSupabase is an open-source Firebase alternative built on top of Postgres, a production-grade SQL database.\n\nSupabase Vector is a vector toolkit built on pgvector, a Postgres extension that allows you to store your embeddings inside the same database that holds the rest of your application data. When combined with pgvector's indexing algorithms, vector search remains fast at large scales.\n\nSupabase adds an ecosystem of services and tools on top of Postgres that makes app development as quick as possible, including:\n\nAuto-generated REST APIs\nAuto-generated GraphQL APIs\nRealtime APIs\nAuthentication\nFile storage\nEdge functions\n\nWe can use these services alongside pgvector to store and query embeddings within Postgres.\n\nOpenAI Cookbook Examples\n\nBelow are guides and resources that walk you through how to use OpenAI embedding models with Supabase Vector.\n\n| Guide                                    | Description                                                |\n| ---------------------------------------- | ---------------------------------------------------------- |\n| Semantic search | Store, index, and query embeddings at scale using pgvector |\n\nAdditional resources\n\nVector columns\nVector indexes\nRAG with permissions\nGoing to production\nDeciding on compute\n"])</script><script>self.__next_f.push([1,"8d:T2b21,"])</script><script>self.__next_f.push([1,"\nThe purpose of this guide is to demonstrate how to store OpenAI embeddings in Supabase Vector (Postgres + pgvector) for the purposes of semantic search.\n\nSupabase is an open-source Firebase alternative built on top of Postgres, a production-grade SQL database. Since Supabase Vector is built on pgvector, you can store your embeddings within the same database that holds the rest of your application data. When combined with pgvector's indexing algorithms, vector search remains fast at large scales.\n\nSupabase adds an ecosystem of services and tools to make app development as quick as possible (such as an auto-generated REST API). We'll use these services to store and query embeddings within Postgres.\n\nThis guide covers:\n\nSetting up your database\nCreating a SQL table that can store vector data\nGenerating OpenAI embeddings using OpenAI's JavaScript client\nStoring the embeddings in your SQL table using the Supabase JavaScript client\nPerforming semantic search over the embeddings using a Postgres function and the Supabase JavaScript client\n\nSetup database\n\nFirst head over to https://database.new to provision your Supabase database. This will create a Postgres database on the Supabase cloud platform. Alternatively, you can follow the local development options if you prefer to run your database locally using Docker.\n\nIn the studio, jump to the SQL editor and execute the following SQL to enable pgvector:\n\n-- Enable the pgvector extension\ncreate extension if not exists vector;\nIn a production application, the best practice is to use database migrations so that all SQL operations are managed within source control. To keep things simple in this guide, we'll execute queries directly in the SQL Editor. If you are building a production app, feel free to move these into a database migration.\n\nCreate a vector table\n\nNext we'll create a table to store documents and embeddings. In the SQL Editor, run:\n\ncreate table documents (\n  id bigint primary key generated always as identity,\n  content text not null,\n  embedding vector (1536) not null\n);\n\nSince Supabase is built on Postgres, we're just using regular SQL here. You can modify this table however you like to better fit your application. If you have existing database tables, you can simply add a new vector column to the appropriate table.\n\nThe important piece to understand is the vector data type, which is a new data type that became available when we enabled the pgvector extension earlier. The size of the vector (1536 here) represents the number of dimensions in the embedding. Since we're using OpenAI's text-embedding-3-small model in this example, we set the vector size to 1536.\n\nLet's go ahead and create a vector index on this table so that future queries remain performant as the table grows:\n\ncreate index on documents using hnsw (embedding vector_ip_ops);\n\nThis index uses the HNSW algorithm to index vectors stored in the embedding column, and specifically when using the inner product operator (``). We'll explain more about this operator later when we implement our match function.\n\nLet's also follow security best practices by enabling row level security on the table:\n\nalter table documents enable row level security;\n\nThis will prevent unauthorized access to this table through the auto-generated REST API (more on this shortly).\n\nGenerate OpenAI embeddings\n\nThis guide uses JavaScript to generate embeddings, but you can easily modify it to use any language supported by OpenAI.\n\nIf you are using JavaScript, feel free to use whichever server-side JavaScript runtime that you prefer (Node.js, Deno, Supabase Edge Functions).\n\nIf you're using Node.js, first install openai as a dependency:\n\nnpm install openai\n\nthen import it:\n\nimport OpenAI from \"openai\";\n\nIf you're using Deno or Supabase Edge Functions, you can import openai directly from a URL:\n\nimport OpenAI from \"https://esm.sh/openai@4\";\nIn this example we import from https://esm.sh which is a CDN that automatically fetches the respective NPM module for you and serves it over HTTP.\n\nNext we'll generate an OpenAI embedding using text-embedding-3-small:\n\nconst openai = new OpenAI();\n\nconst input = \"The cat chases the mouse\";\n\nconst result = await openai.embeddings.create({\n  input,\n  model: \"text-embedding-3-small\",\n});\n\nconst [{ embedding }] = result.data;\n\nRemember that you will need an OpenAI API key to interact with the OpenAI API. You can pass this as an environment variable called OPENAI_API_KEY, or manually set it when you instantiate your OpenAI client:\n\nconst openai = new OpenAI({\n  apiKey: \"\",\n});\n\nRemember:** Never hard-code API keys in your code. Best practice is to either store it in a .env file and load it using a library like dotenv or load it from an external key management system._\n\nStore embeddings in database\n\nSupabase comes with an auto-generated REST API that dynamically builds REST endpoints for each of your tables. This means you don't need to establish a direct Postgres connection to your database - instead you can interact with it simply using by the REST API. This is especially useful in serverless environments that run short-lived processes where re-establishing a database connection every time can be expensive.\n\nSupabase comes with a number of client libraries to simplify interaction with the REST API. In this guide we'll use the JavaScript client library, but feel free to adjust this to your preferred language.\n\nIf you're using Node.js, install @supabase/supabase-js as a dependency:\n\nnpm install @supabase/supabase-js\n\nthen import it:\n\nimport { createClient } from \"@supabase/supabase-js\";\n\nIf you're using Deno or Supabase Edge Functions, you can import @supabase/supabase-js directly from a URL:\n\nimport { createClient } from \"https://esm.sh/@supabase/supabase-js@2\";\n\nNext we'll instantiate our Supabase client and configure it so that it points to your Supabase project. In this guide we'll store a reference to your Supabase URL and key in a .env file, but feel free to modify this based on how your application handles configuration.\n\nIf you are using Node.js or Deno, add your Supabase URL and service role key to a .env file. If you are using the cloud platform, you can find these from your Supabase dashboard settings page. If you're running Supabase locally, you can find these by running npx supabase status in a terminal.\n\n.env\n\nSUPABASE_URL=\nSUPABASE_SERVICE_ROLE_KEY=\n\nIf you are using Supabase Edge Functions, these environment variables are automatically injected into your function for you so you can skip the above step.\n\nNext we'll pull these environment variables into our app.\n\nIn Node.js, install the dotenv dependency:\n\nnpm install dotenv\n\nAnd retrieve the environment variables from process.env:\n\nimport { config } from \"dotenv\";\n\n// Load .env file\nconfig();\n\nconst supabaseUrl = process.env[\"SUPABASE_URL\"];\nconst supabaseServiceRoleKey = process.env[\"SUPABASE_SERVICE_ROLE_KEY\"];\n\nIn Deno, load the .env file using the dotenv standard library:\n\nimport { load } from \"https://deno.land/std@0.208.0/dotenv/mod.ts\";\n\n// Load .env file\nconst env = await load();\n\nconst supabaseUrl = env[\"SUPABASE_URL\"];\nconst supabaseServiceRoleKey = env[\"SUPABASE_SERVICE_ROLE_KEY\"];\n\nIn Supabase Edge Functions, simply load the injected environment variables directly:\n\nconst supabaseUrl = Deno.env.get(\"SUPABASE_URL\");\nconst supabaseServiceRoleKey = Deno.env.get(\"SUPABASE_SERVICE_ROLE_KEY\");\n\nNext let's instantiate our supabase client:\n\nconst supabase = createClient(supabaseUrl, supabaseServiceRoleKey, {\n  auth: { persistSession: false },\n});\n\nFrom here we use the supabase client to insert our text and embedding (generated earlier) into the database:\n\nconst { error } = await supabase.from(\"documents\").insert({\n  content: input,\n  embedding,\n});\nIn production, best practice would be to check the response error to see if there were any problems inserting the data and handle it accordingly.\n\nSemantic search\n\nFinally let's perform semantic search over the embeddings in our database. At this point we'll assume your documents table has been filled with multiple records that we can search over.\n\nLet's create a match function in Postgres that performs the semantic search query. Execute the following in the SQL Editor:\n\ncreate function match_documents (\n  query_embedding vector (1536),\n  match_threshold float,\n)\nreturns setof documents\nlanguage plpgsql\nas $$\nbegin\n  return query\n  select *\n  from documents\n  where documents.embedding  query_embedding  query_embedding;\nend;\n$$;\n\nThis function accepts a query_embedding which represents the embedding generated from the search query text (more on this shortly). It also accepts a match_threshold which specifies how similar the document embeddings have to be in order for query_embedding to count as a match.\n\nInside the function we implement the query which does two things:\n\nFilters the documents to only include those who's embeddings match within the above match_threshold. Since the ` operator performs the negative inner product (versus positive inner product), we negate the similarity threshold before comparing. This means a match_threshold` of 1 is most similar, and -1 is most dissimilar.\nOrders the documents by negative inner product (``) ascending. This allows us to retrieve documents that match closest first.\nSince OpenAI embeddings are normalized, we opted to use inner product (`) because it is slightly more performant than other operators like cosine distance (`). It is important to note though this only works because the embeddings are normalized - if they weren't, cosine distance should be used.\n\nNow we can call this function from our application using the supabase.rpc() method:\n\nconst query = \"What does the cat chase?\";\n\n// First create an embedding on the query itself\nconst result = await openai.embeddings.create({\n  input: query,\n  model: \"text-embedding-3-small\",\n});\n\nconst [{ embedding }] = result.data;\n\n// Then use this embedding to search for matches\nconst { data: documents, error: matchError } = await supabase\n  .rpc(\"match_documents\", {\n    query_embedding: embedding,\n    match_threshold: 0.8,\n  })\n  .select(\"content\")\n  .limit(5);\n\nIn this example, we set a match threshold to 0.8. Adjust this threshold based on what works best with your data.\n\nNote that since match_documents returns a set of documents, we can treat this rpc() like a regular table query. Specifically this means we can chain additional commands to this query, like select() and limit(). Here we select just the columns we care about from the documents table (content), and we limit the number of documents returned (max 5 in this example).\n\nAt this point you have a list of documents that matched the query based on semantic relationship, ordered by most similar first.\n\nNext steps\n\nYou can use this example as the foundation for other semantic search techniques, like retrieval augmented generation (RAG).\n\nFor more information on OpenAI embeddings, read the Embedding docs.\n\nFor more information on Supabase Vector, read the AI \u0026 Vector docs.\n"])</script><script>self.__next_f.push([1,"8e:T230d,"])</script><script>self.__next_f.push([1,"\nIn this notebook we share examples of how to implement guardrails for your LLM applications. A guardrail is a generic term for detective controls that aim to steer your application. Greater steerability is a common requirement given the inherent randomness of LLMs, and so creating effective guardrails has become one of the most common areas of performance optimization when pushing an LLM from prototype to production.\n\nGuardrails are incredibly diverse and can be deployed to virtually any context you can imagine something going wrong with LLMs. This notebook aims to give simple examples that can be extended to meet your unique use case, as well as outlining the trade-offs to consider when deciding whether to implement a guardrail, and how to do it.\n\nThis notebook will focus on:\nInput guardrails that flag inappropriate content before it gets to your LLM\nOutput guardrails that validate what your LLM has produced before it gets to the customer\n\nNote: This notebook tackles guardrails as a generic term for detective controls around an LLM - for the official libraries that provide distributions of pre-built guardrails frameworks, please check out the following:\nNeMo Guardrails\nGuardrails AI\n\n1. Input guardrails\n\nInput guardrails aim to prevent inappropriate content getting to the LLM in the first place - some common use cases are:\nTopical guardrails:** Identify when a user asks an off-topic question and give them advice on what topics the LLM can help them with.\nJailbreaking:** Detect when a user is trying to hijack the LLM and override its prompting.\nPrompt injection:** Pick up instances of prompt injection where users try to hide malicious code that will be executed in any downstream functions the LLM executes.\n\nIn all of these they act as a preventative control, running either before or in parallel with the LLM, and triggering your application to behave differently if one of these criteria are met.\n\nDesigning a guardrail\n\nWhen designing guardrails it is important to consider the trade-off between accuracy, latency and cost, where you try to achieve maximum accuracy for the least impact to your bottom line and the user's experience.\n\nWe'll begin with a simple topical guardrail which aims to detect off-topic questions and prevent the LLM from answering if triggered. This guardrail consists of a simple prompt and uses gpt-4o-mini, maximising latency/cost holding a good enough accuracy, but if we wanted to optimize further we could consider:\nAccuracy:** You could consider fine-tuning gpt-4o-mini or few-shot examples to increase the accuracy. RAG can also be effective if you have a corpus of information that can help determine whether a piece of content is allowed or not.\nLatency/Cost:** You could try fine-tuning smaller models, such as babbage-002 or open-source offerings like Llama, which can perform quite well when given enough training examples. When using open-source offerings you can also tune the machines you are using for inference to maximize either cost or latency reduction.\n\nThis simple guardrail aims to ensure the LLM only answers to a predefined set of topics, and responds to out-of-bounds queries with a canned message.\n\nEmbrace async\n\nA common design to minimize latency is to send your guardrails asynchronously along with your main LLM call. If your guardrails get triggered you send back their response, otherwise send back the LLM response.\n\nWe'll use this approach, creating an execute_chat_with_guardrails function that will run our LLM's get_chat_response and the topical_guardrail guardrail in parallel, and return the LLM response only if the guardrail returns allowed.\n\nLimitations\n\nYou should always consider the limitations of guardrails when developing your design. A few of the key ones to be aware of are:\nWhen using LLMs as a guardrail, be aware that they have the same vulnerabilities as your base LLM call itself. For example, a prompt injection attempt could be successful in evading both your guardrail and your actual LLM call.\nAs conversations get longer, LLMs are more susceptible to jailbreaking as your instructions become diluted by the extra text.\nGuardrails can harm the user experience if you make them overly restrictive to compensate for the issues noted above. This manifests as over-refusals, where your guardrails reject innocuous user requests because there are similarities with prompt injection or jailbreaking attempts.\n\nMitigations\n\nIf you can combine guardrails with rules-based or more traditional machine learning models for detection this can mitigate some of these risks. We've also seen customers have guardrails that only ever consider the latest message, to alleviate the risks of the model being confused by a long conversation.\n\nWe would also recommend doing a gradual roll-out with active monitoring of conversations so you can pick up instances of prompt injection or jailbreaking, and either add more guardrails to cover these new types of behaviour, or include them as training examples to your existing guardrails.\nLooks like our guardrail worked - the first question was allowed through, but the second was blocked for being off-topic. Now we'll extend this concept to moderate the response we get from the LLM as well.\n2. Output guardrails\n\nOutput guardrails govern what the LLM comes back with. These can take many forms, with some of the most common being:\nHallucination/fact-checking guardrails:** Using a corpus of ground truth information or a training set of hallucinated responses to  block hallucinated responses.\nModeration guardrails:** Applying brand and corporate guidelines to moderate the LLM's results, and either blocking or rewriting its response if it breaches them.\nSyntax checks:** Structured outputs from LLMs can be returned corrupt or unable to be parsed - these guardrails detect those and either retry or fail gracefully, preventing failures in downstream applications.\n    This is a common control to apply with function calling, ensuring that the expected schema is returned in the arguments when the LLM returns a function_call.\n\nModeration guardrail\n\nHere we implement a moderation guardrail that uses a version of the G-Eval evaluation method to score the presence of unwanted content in the LLM's response. This method is demonstrated in more detail in of our other notebooks.\n\nTo accomplish this we will make an extensible framework for moderating content that takes in a domain and applies criteria to a piece of content using a set of steps:\nWe set a domain name, which describes the type of content we're going to moderate.\nWe provide criteria, which outline clearly what the content should and should not contain.\nStep-by-step instructions are provided for the LLM to grade the content.\nThe LLM returns a discrete score from 1-5.\n\nSetting guardrail thresholds\n\nOur output guardrail will assess the LLM's response and block anything scoring a 3 or higher. Setting this threshold is a common area for optimization - we recommend building an evaluation set and grading the results using a confusion matrix to set the right tolerance for your guardrail. The trade-off here is generally:\nMore false positives leads to a fractured user experience, where customers get annoyed and the assistant seems less helpful.\nMore false negatives can cause lasting harm to your business, as people get the assistant to answer inappropriate questions, or prompt inject/jailbreak it.\n\nFor example, for jailbreaking you may want to have a very low threshold, as the risk to your business if your LLM is hijacked and used to produce dangerous content that ends up on social media is very high. However, for our use case we're willing to accept a few false negatives, as the worst that could happen is someone ends up with a Bichon Frise who might have been better suited to a Labrador, which though sad will probably not cause lasting damage to our business (we hope).\nConclusion\n\nGuardrails are a vibrant and evolving topic in LLMs, and we hope this notebook has given you an effective introduction to the core concepts around guardrails. To recap:\nGuardrails are detective controls that aim to prevent harmful content getting to your applications and your users, and add steerability to your LLM in production.\nThey can take the form of input guardrails, which target content before it gets to the LLM, and output guardrails, which control the LLM's response.\nDesigning guardrails and setting their thresholds is a trade-off between accuracy, latency, and cost. Your decision should be based on clear evaluations of the performance of your guardrails, and an understanding of what the cost of a false negative and false positive are for your business.\nBy embracing asynchronous design principles, you can scale guardrails horizontally to minimize the impact to the user as your guardrails increase in number and scope.\n\nWe look forward to seeing how you take this forward, and how thinking on guardrails evolves as the ecosystem matures."])</script><script>self.__next_f.push([1,"8f:T324a,"])</script><script>self.__next_f.push([1,"\nWelcome to the Clothing Matchmaker App Jupyter Notebook! This project demonstrates the power of the GPT-4o mini model in analyzing images of clothing items and extracting key features such as color, style, and type. The core of our app relies on this advanced image analysis model developed by OpenAI, which enables us to accurately identify the characteristics of the input clothing item.\n\nGPT-4o mini is a small model that combines natural language processing with image recognition, allowing it to understand and generate responses based on both text and visual inputs with low latency.\n\nBuilding on the capabilities of the GPT-4o mini model, we employ a custom matching algorithm and the RAG technique to search our knowledge base for items that complement the identified features. This algorithm takes into account factors like color compatibility and style coherence to provide users with suitable recommendations. Through this notebook, we aim to showcase the practical application of these technologies in creating a clothing recommendation system.\n\nUsing the combination of GPT-4o mini + RAG (Retrieval-Augmented Generation) offers several advantages:\n\nContextual Understanding: GPT-4o mini can analyze input images and understand the context, such as the objects, scenes, and activities depicted. This allows for more accurate and relevant suggestions or information across various domains, whether it's interior design, cooking, or education.\nRich Knowledge Base: RAG combines the generative capabilities of GPT-4 with a retrieval component that accesses a large corpus of information across different fields. This means the system can provide suggestions or insights based on a wide range of knowledge, from historical facts to scientific concepts.\nCustomization: The approach allows for easy customization to cater to specific user needs or preferences in various applications. Whether it's tailoring suggestions to a user's taste in art or providing educational content based on a student's learning level, the system can be adapted to deliver personalized experiences.\n\nOverall, the GPT-4o mini + RAG approach offers a fast, powerful, and flexible solution for various fashion-related applications, leveraging the strengths of both generative and retrieval-based AI techniques.\n\nEnvironment Setup\nFirst we will install the necessary dependencies, then import the libraries and write some utility functions that we will use later on.\nCreating the Embeddings\nWe will now set up the knowledge base by choosing a database and generating embeddings for it. I am using the sample_styles.csv file for this in the data folder. This is a sample of a bigger dataset that contains ~44K items. This step can also be replaced by using an out-of-the-box vector database. For example, you can follow one of these cookbooks to set up your vector database.\nNow we will generate embeddings for the entire dataset. We can parallelize the execution of these embeddings to ensure that the script scales up for larger datasets. With this logic, the time to create embeddings for the full 44K entry dataset decreases from 4h to 2-3min.\nTwo options for creating the embeddings:\nThe next line will create the embeddings for the sample clothes dataset. This will take around 0.02s to process and another ~30s to write the results to a local .csv file. The process is using our text_embedding_3_large model which is priced at $0.00013/1K tokens. Given that the dataset has around 1K entries, the following operation will cost approximately $0.001. If you decide to work with the entire dataset of 44K entries, this operation will take 2-3min to process and it will cost approximately $0.07.\n\nIf you would not like to proceed with creating your own embeddings, we will use a dataset of pre-computed embeddings. You can skip this cell and uncomment the code in the following cell to proceed with loading the pre-computed vectors. This operation takes ~1min to load all the data in memory.\nBuilding the Matching Algorithm\n\nIn this section, we'll develop a cosine similarity retrieval algorithm to find similar items in our dataframe. We'll utilize our custom cosine similarity function for this purpose. While the sklearn library offers a built-in cosine similarity function, recent updates to its SDK have led to compatibility issues, prompting us to implement our own standard cosine similarity calculation.\n\nIf you already have a vector database set up, you can skip this step. Most standard databases come with their own search functions, which simplify the subsequent steps outlined in this guide. However, we aim to demonstrate that the matching algorithm can be tailored to meet specific requirements, such as a particular threshold or a specified number of matches returned.\n\nThe find_similar_items function accepts four parameters:\nembedding: The embedding for which we want to find a match.\nembeddings: A list of embeddings to search through for the best matches.\nthreshold (optional): This parameter specifies the minimum similarity score for a match to be considered valid. A higher threshold results in closer (better) matches, while a lower threshold allows for more items to be returned, though they may not be as closely matched to the initial embedding.\ntop_k (optional): This parameter determines the number of items to return that exceed the given threshold. These will be the top-scoring matches for the provided embedding.\nAnalysis Module\n\nIn this module, we leverage gpt-4o-mini to analyze input images and extract important features like detailed descriptions, styles, and types. The analysis is performed through a straightforward API call, where we provide the URL of the image for analysis and request the model to identify relevant features.\n\nTo ensure the model returns accurate results, we use specific techniques in our prompt:\n\nOutput Format Specification: We instruct the model to return a JSON block with a predefined structure, consisting of:\n   items (str[]): A list of strings, each representing a concise title for an item of clothing, including style, color, and gender. These titles closely resemble the productDisplayName property in our original database.\n   category (str): The category that best represents the given item. The model selects from a list of all unique articleTypes present in the original styles dataframe.\n   gender (str): A label indicating the gender the item is intended for. The model chooses from the options [Men, Women, Boys, Girls, Unisex].\n\nClear and Concise Instructions:\n   We provide clear instructions on what the item titles should include and what the output format should be. The output should be in JSON format, but without the json tag that the model response normally contains.\n\nOne Shot Example:\n   To further clarify the expected output, we provide the model with an example input description and a corresponding example output. Although this may increase the number of tokens used (and thus the cost of the call), it helps to guide the model and results in better overall performance.\n\nBy following this structured approach, we aim to obtain precise and useful information from the gpt-4o-mini model for further analysis and integration into our database.\nTesting the Prompt with Sample Images\n\nTo evaluate the effectiveness of our prompt, let's load and test it with a selection of images from our dataset. We'll use images from the \"data/sample_clothes/sample_images\" folder, ensuring a variety of styles, genders, and types. Here are the chosen samples:\n\n2133.jpg: Men's shirt\n7143.jpg: Women's shirt\n4226.jpg: Casual men's printed t-shirt\n\nBy testing the prompt with these diverse images, we can assess its ability to accurately analyze and extract relevant features from different types of clothing items and accessories.\nWe need a utility function to encode the .jpg images in base64\nNext, we process the output from the image analysis and use it to filter and display matching items from our dataset. Here's a breakdown of the code:\n\nExtracting Image Analysis Results: We extract the item descriptions, category, and gender from the image_analysis dictionary.\n\nFiltering the Dataset: We filter the styles_df DataFrame to include only items that match the gender from the image analysis (or are unisex) and exclude items of the same category as the analyzed image.\n\nFinding Matching Items: We use the find_matching_items_with_rag function to find items in the filtered dataset that match the descriptions extracted from the analyzed image.\n\nDisplaying Matching Items: We create an HTML string to display images of the matching items. We construct the image paths using the item IDs and append each image to the HTML string. Finally, we use display(HTML(html)) to render the images in the notebook.\n\nThis cell effectively demonstrates how to use the results of image analysis to filter a dataset and visually display items that match the analyzed image's characteristics.\nGuardrails\n\nIn the context of using Large Language Models (LLMs) like GPT-4o mini, \"guardrails\" refer to mechanisms or checks put in place to ensure that the model's output remains within desired parameters or boundaries. These guardrails are crucial for maintaining the quality and relevance of the model's responses, especially when dealing with complex or nuanced tasks.\n\nGuardrails are useful for several reasons:\n\nAccuracy: They help ensure that the model's output is accurate and relevant to the input provided.\nConsistency: They maintain consistency in the model's responses, especially when dealing with similar or related inputs.\nSafety: They prevent the model from generating harmful, offensive, or inappropriate content.\nContextual Relevance: They ensure that the model's output is contextually relevant to the specific task or domain it is being used for.\n\nIn our case, we are using GPT-4o mini to analyze fashion images and suggest items that would complement an original outfit. To implement guardrails, we can refine results: After obtaining initial suggestions from GPT-4o mini, we can send the original image and the suggested items back to the model. We can then ask GPT-4o mini to evaluate whether each suggested item would indeed be a good fit for the original outfit.\n\nThis gives the model the ability to self-correct and adjust its own output based on feedback or additional information. By implementing these guardrails and enabling self-correction, we can enhance the reliability and usefulness of the model's output in the context of fashion analysis and recommendation.\n\nTo facilitate this, we write a prompt that asks the LLM for a simple \"yes\" or \"no\" answer to the question of whether the suggested items match the original outfit or not. This binary response helps streamline the refinement process and ensures clear and actionable feedback from the model.\nFinally, let's determine which of the items identified above truly complement the outfit.\nWe can observe that the initial list of potential items has been further refined, resulting in a more curated selection that aligns well with the outfit. Additionally, the model provides explanations for why each item is considered a good match, offering valuable insights into the decision-making process.\nConclusion\n\nIn this Jupyter Notebook, we explored the application of GPT-4o mini and other machine learning techniques to the domain of fashion. We demonstrated how to analyze images of clothing items, extract relevant features, and use this information to find matching items that complement an original outfit. Through the implementation of guardrails and self-correction mechanisms, we refined the model's suggestions to ensure they are accurate and contextually relevant.\n\nThis approach has several practical uses in the real world, including:\n\nPersonalized Shopping Assistants: Retailers can use this technology to offer personalized outfit recommendations to customers, enhancing the shopping experience and increasing customer satisfaction.\nVirtual Wardrobe Applications: Users can upload images of their own clothing items to create a virtual wardrobe and receive suggestions for new items that match their existing pieces.\nFashion Design and Styling: Fashion designers and stylists can use this tool to experiment with different combinations and styles, streamlining the creative process.\n\nHowever, one of the considerations to keep in mind is cost. The use of LLMs and image analysis models can incur costs, especially if used extensively. It's important to consider the cost-effectiveness of implementing these technologies. gpt-4o-mini is priced at $0.01 per 1000 tokens. This adds up to $0.00255 for one 256px x 256px image.\n\nOverall, this notebook serves as a foundation for further exploration and development in the intersection of fashion and AI, opening doors to more personalized and intelligent fashion recommendation systems."])</script><script>self.__next_f.push([1,"90:Tdc1,"])</script><script>self.__next_f.push([1,"\nThis notebook shows how to leverage GPT-4V to turn rich PDF documents such as slide decks or exports from web pages into usable content for your RAG application.\n\nThis technique can be used if you have a lot of unstructured data containing valuable information that you want to be able to retrieve as part of your RAG pipeline.\n\nFor example, you could build a Knowledge Assistant that could answer user queries about your company or product based on information contained in PDF documents.\n\nThe example documents used in this notebook are located at data/example_pdfs. They are related to OpenAI's APIs and various techniques that can be used as part of LLM projects.\nData preparation\n\nIn this section, we will process our input data to prepare it for retrieval.\n\nWe will do this in 2 ways:\n\nExtracting text with pdfminer\nConverting the PDF pages to images to analyze them with GPT-4V\n\nYou can skip the 1st method if you want to only use the content inferred from the image analysis.\nSetup\n\nWe need to install a few libraries to convert the PDF to images and extract the text (optional).\n\nNote: You need to install poppler on your machine for the pdf2image library to work. You can follow the instructions to install it here.\nFile processing\nTesting with an example\nImage analysis with GPT-4V\n\nAfter converting a PDF file to multiple images, we'll use GPT-4V to analyze the content based on the images.\nTesting with an example\nProcessing all documents\nWe will list all files in the example folder and process them by\nExtracting the text\nConverting the docs to images\nAnalyzing pages with GPT-4V\n\nNote: This takes about ~2 mins to run. Feel free to skip and load directly the result file (see below).\nEmbedding content\nBefore embedding the content, we will chunk it logically by page.\nFor real-world scenarios, you could explore more advanced ways to chunk the content:\nCutting it into smaller pieces\nAdding data - such as the slide title, deck title and/or the doc description - at the beginning of each piece of content. That way, each independent chunk can be in context\n\nFor the sake of brevity, we will use a very simple chunking strategy and rely on separators to split the text by page.\nRetrieval-augmented generation\n\nThe last step of the process is to generate outputs in response to input queries, after retrieving content as context to reply.\nWrapping up\n\nIn this notebook, we have learned how to develop a basic RAG pipeline based on PDF documents. This includes:\n\nHow to parse pdf documents, taking slide decks and an export from an HTML page as examples, using a python library as well as GPT-4V to interpret the visuals\nHow to process the extracted content, clean it and chunk it into several pieces\nHow to embed the processed content using OpenAI embeddings\nHow to retrieve content that is relevant to an input query\nHow to use GPT-4-turbo to generate an answer using the retrieved content as context\n\nIf you want to explore further, consider these optimisations:\n\nPlaying around with the prompts provided as examples\nChunking the content further and adding metadata as context to each chunk\nAdding rule-based filtering on the retrieval results or re-ranking results to surface to most relevant content\n\nYou can apply the techniques covered in this notebook to multiple use cases, such as assistants that can access your proprietary data, customer service or FAQ bots that can read from your internal policies, or anything that requires leveraging rich documents that would be better understood as images."])</script><script>self.__next_f.push([1,"91:T1070,"])</script><script>self.__next_f.push([1,"\nThis notebook explores how to leverage the vision capabilities of the GPT-4* models (for example gpt-4o, gpt-4o-mini or gpt-4-turbo) to tag \u0026 caption images.\n\nWe can leverage the multimodal capabilities of these models to provide input images along with additional context on what they represent, and prompt the model to output tags or image descriptions. The image descriptions can then be further refined with a language model (in this notebook, we'll use gpt-4o-mini) to generate captions.\n\nGenerating text content from images can be useful for multiple use cases, especially use cases involving search.\nWe will illustrate a search use case in this notebook by using generated keywords and product captions to search for products - both from a text input and an image input.\n\nAs an example, we will use a dataset of Amazon furniture items, tag them with relevant keywords and generate short, descriptive captions.\nSetup\nTag images\n\nIn this section, we'll use GPT-4o mini to generate relevant tags for our products.\n\nWe'll use a simple zero-shot approach to extract keywords, and deduplicate those keywords using embeddings to avoid having multiple keywords that are too similar.\n\nWe will use a combination of an image and the product title to avoid extracting keywords for other items that are depicted in the image - sometimes there are multiple items used in the scene and we want to focus on just the one we want to tag.\nExtract keywords\nTesting with a few examples\nLooking up existing keywords\n\nUsing embeddings to avoid duplicates (synonyms) and/or match pre-defined keywords\nTesting with example keywords\nGenerate captions\n\nIn this section, we'll use GPT-4o mini to generate an image description and then use a few-shot examples approach with GPT-4-turbo to generate captions from the images.\n\nIf few-shot examples are not enough for your use case, consider fine-tuning a model to get the generated captions to match the style \u0026 tone you are targeting.\nDescribing images with GPT-4o mini\nTesting on a few examples\nTurning descriptions into captions\nUsing a few-shot examples approach to turn a long description into a short image caption\nTesting on a few examples\nImage search\n\nIn this section, we will use generated keywords and captions to search items that match a given input, either text or image.\n\nWe will leverage our embeddings model to generate embeddings for the keywords and captions and compare them to either input text or the generated caption from an input image.\nPreparing the dataset\nProcessing all 312 lines of the dataset will take a while.\nTo test out the idea, we will only run it on the first 50 lines: this takes ~20 mins.\nFeel free to skip this step and load the already processed dataset (see below).\nEmbedding captions and keywords\nWe can now use the generated captions and keywords to match relevant content to an input text query or caption.\nTo do this, we will embed a combination of keywords + captions.\nNote: creating the embeddings will take ~3 mins to run. Feel free to load the pre-processed dataset (see below).\nSearch from input text\n\nWe can compare the input text from a user directly to the embeddings we just created.\nSearch from image\n\nIf the input is an image, we can find similar images by first turning images into captions, and embedding those captions to compare them to the already created embeddings.\nWrapping up\n\n\nIn this notebook, we explored how to leverage the multimodal capabilities of gpt-4o-mini to tag and caption images. By providing images along with contextual information to the model, we were able to generate tags and descriptions that can be further refined to create captions. This process has practical applications in various scenarios, particularly in enhancing search functionalities.\n\nThe search use case illustrated can be directly applied to applications such as recommendation systems, but the techniques covered in this notebook can be extended beyond items search and used in multiple use cases, for example RAG applications leveraging unstructured image data.\n\nAs a next step, you could explore using a combination of rule-based filtering with keywords and embeddings search with captions to retrieve more relevant results."])</script><script>self.__next_f.push([1,"92:T28d0,"])</script><script>self.__next_f.push([1,"\nNote: This guide is designed to complement our Guardrails Cookbook by providing a more focused look at moderation techniques. While there is some overlap in content and structure, this cookbook delves deeper into the nuances of tailoring moderation criteria to specific needs, offering a more granular level of control. If you're interested in a broader overview of content safety measures, including guardrails and moderation, we recommend starting with the Guardrails Cookbook. Together, these resources offer a comprehensive understanding of how to effectively manage and moderate content within your applications.\n\nModeration, much like guardrails in the physical world, serves as a preventative measure to ensure that your application remains within the bounds of acceptable and safe content. Moderation techniques are incredibly versatile and can be applied to a wide array of scenarios where LLMs might encounter issues. This notebook is designed to offer straightforward examples that can be adapted to suit your specific needs, while also discussing the considerations and trade-offs involved in deciding whether to implement moderation and how to go about it. This notebook will use our Moderation API, a tool you can use to check whether text is potentially harmful.\n\nThis notebook will concentrate on:\n\nInput Moderation:** Identifying and flagging inappropriate or harmful content before it is processed by your LLM.\nOutput Moderation:** Reviewing and validating the content generated by your LLM before it reaches the end user.\nCustom Moderation:** Tailoring moderation criteria and rules to suit the specific needs and context of your application, ensuring a personalized and effective content control mechanism.\n1. Input moderation\nInput Moderation focuses on preventing harmful or inappropriate content from reaching the LLM, with common applications including:\nContent Filtering:** Prevent the spread of harmful content such as hate speech, harassment, explicit material, and misinformation on social media, forums, and content creation platforms.\nCommunity Standards Enforcement:** Ensure that user interactions, such as comments, forum posts, and chat messages, adhere to the community guidelines and standards of online platforms, including educational environments, gaming communities, or dating apps.\nSpam and Fraud Prevention:** Filter out spam, fraudulent content, and misleading information in online forums, comment sections, e-commerce platforms, and customer reviews.\n\nThese measures act as preventive controls, operating before or alongside the LLM to alter your application's behavior if specific criteria are met.\nEmbrace async\nA common design to minimize latency is to send your moderations asynchronously along with your main LLM call. If your moderation gets triggered you send back a placeholder response, otherwise send back the LLM response. This pattern can also be found in our Guardrails Cookbook. It's important to note that while the async mode is effective in minimizing latency, it can also lead to unnecessary costs. Specifically, you could avoid completion costs if the content is flagged before processing. Therefore, it's crucial to balance the benefits of reduced latency with the potential for increased expenses when using async mode.\n\nWe'll use this approach, creating an execute_chat_with_moderation function that will run our LLM's get_chat_response and the check_expression moderation function in parallel, and return the LLM response only if the moderation returns False (not triggered).\n\nWorkflow:\nWe will create a workflow that incorporates the Moderation API to check user input for any potentially unsafe content before it is sent to the Language Model (LLM). This ensures that only appropriate content is processed further in the application.\n\nReceive an input from the user\nUse the Moderation API to analyze the input for any problematic content.\nConditional Processing:\nIf the input is flagged by the moderation check, handle it accordingly (e.g., reject the input, ask the user to rephrase, etc.).\nIf the input is not flagged, pass it to the LLM for further processing.\n\nWe will demonstrate this workflow with two example prompts.\nLooks like our moderation worked - the first question was allowed through, but the second was blocked for inapropriate content. Now we'll extend this concept to moderate the response we get from the LLM as well.\n2. Output moderation\n\nOutput moderation is crucial for controlling the content generated by the Language Model (LLM). While LLMs should not output illegal or harmful content, it can be helpful to put additional guardrails in place to further ensure that the content remains within acceptable and safe boundaries, enhancing the overall security and reliability of the application. Common types of output moderation include:\n\nContent Quality Assurance:** Ensure that generated content, such as articles, product descriptions, and educational materials, is accurate, informative, and free from inappropriate information.\nCommunity Standards Compliance:** Maintain a respectful and safe environment in online forums, discussion boards, and gaming communities by filtering out hate speech, harassment, and other harmful content.\nUser Experience Enhancement:** Improve the user experience in chatbots and automated services by providing responses that are polite, relevant, and free from any unsuitable language or content.\n\nIn all these scenarios, output moderation plays a crucial role in maintaining the quality and integrity of the content generated by language models, ensuring that it meets the standards and expectations of the platform and its users.\nSetting moderation thresholds\nOpenAI has selected thresholds for moderation categories that balance precision and recall for our use cases, but your use case or tolerance for moderation may be different. Setting this threshold is a common area for optimization - we recommend building an evaluation set and grading the results using a confusion matrix to set the right tolerance for your moderation. The trade-off here is generally:\n\nMore false positives leads to a fractured user experience, where customers get annoyed and the assistant seems less helpful.\nMore false negatives can cause lasting harm to your business, as people get the assistant to answer inappropriate questions, or provide inappropriate responses.\n\nFor example, on a platform dedicated to creative writing, the moderation threshold for certain sensitive topics might be set higher to allow for greater creative freedom while still providing a safety net to catch content that is clearly beyond the bounds of acceptable expression. The trade-off is that some content that might be considered inappropriate in other contexts is allowed, but this is deemed acceptable given the platform's purpose and audience expectations.\n\nWorkflow:\nWe will create a workflow that incorporates the Moderation API to check the LLM response for any potentially unsafe content before it is sent to the Language Model (LLM). This ensures that only appropriate content is displayed to the user.\n\nReceive an input from the user\nSend prompt to LLM and generate a response\nUse the Moderation API to analyze the LLM's response for any problematic content.\nConditional Processing:\nIf the response is flagged by the moderation check, handle it accordingly (e.g., reject the response, show a placeholder message, etc.).\nIf the response is not flagged, display it to the user.\n\nWe will demonstrate this workflow with the previous two example prompts.\nAs observed, the first two examples behaved as expected. However, the \"interesting example\" did not perform as anticipated. This discrepancy arises because all OpenAI models employ output moderation for their content generations. Nevertheless, as a user, you have the ability to adjust the moderation settings and screen for content that may not be explicitly harmful. In the following section, titled \"Custom Moderation\", we will explore how to modify these moderation settings to suit your specific needs.\n3. Custom moderation\n\nNote: This section does not use the Moderation API.\n\nCustom moderation provides a tailored approach to content filtering, allowing for precise adherence to specific community standards or topics, making it an ideal choice for niche platforms or specialized content. Unlike the general categories covered by the moderation API, custom moderation offers a higher degree of control and requires more effort to set up and maintain. It shares the common goal of ensuring content safety with guardrails, focusing on setting criteria to filter and manage content, whether it's user input or output from language models. Custom moderation can be an alternative or an extension to the moderation API, that uses our completions endpoint. For those interested in exploring guardrails further, a cookbook with examples and guidance is available here.\n\nWorkflow\nWe will implement a small example to showcase custom moderation. We will use an extra call to gpt-4o mini to assess messages and decide if they should be moderated or not.\nConclusion\n\nIn conclusion, this notebook has explored the essential role of moderation in applications powered by language models (LLMs). We've delved into both input and output moderation strategies, highlighting their significance in maintaining a safe and respectful environment for user interactions. Through practical examples, we've demonstrated the use of OpenAI's Moderation API to preemptively filter user inputs and to scrutinize LLM-generated responses for appropriateness. The implementation of these moderation techniques is crucial for upholding the integrity of your application and ensuring a positive experience for your users.\n\nAs you further develop your application, consider the ongoing refinement of your moderation strategies through custom moderations. This may involve tailoring moderation criteria to your specific use case or integrating a combination of machine learning models and rule-based systems for a more nuanced analysis of content. Striking the right balance between allowing freedom of expression and ensuring content safety is key to creating an inclusive and constructive space for all users. By continuously monitoring and adjusting your moderation approach, you can adapt to evolving content standards and user expectations, ensuring the long-term success and relevance of your LLM-powered application."])</script><script>self.__next_f.push([1,"93:T928,"])</script><script>self.__next_f.push([1,"The objective of this notebook is to demonstrate how to summarize large documents with a controllable level of detail.\n\nIf you give a GPT model the task of summarizing a long document (e.g. 10k or more tokens), you'll tend to get back a relatively short summary that isn't proportional to the length of the document. For instance, a summary of a 20k token document will not be twice as long as a summary of a 10k token document. One way we can fix this is to split our document up into pieces, and produce a summary piecewise. After many queries to a GPT model, the full summary can be reconstructed. By controlling the number of text chunks and their sizes, we can ultimately control the level of detail in the output.\nWe'll define a simple utility to wrap calls to the OpenAI API.\nNext we'll define some utilities to chunk a large document into smaller pieces.\nNow we can define a utility to summarize text with a controllable level of detail (note the detail parameter).\n\nThe function first determines the number of chunks by interpolating between a minimum and a maximum chunk count based on a controllable detail parameter. It then splits the text into chunks and summarizes each chunk.\nNow we can use this utility to produce summaries with varying levels of detail. By increasing detail from 0 to 1 we get progressively longer summaries of the underlying document. A higher value for the detail parameter results in a more detailed summary because the utility first splits the document into a greater number of chunks. Each chunk is then summarized, and the final summary is a concatenation of all the chunk summaries.\nThe original document is nearly 15k tokens long. Notice how large the gap is between the length of summary_with_detail_0 and summary_with_detail_1. It's nearly 25 times longer!\nLet's inspect the summaries to see how the level of detail changes when the detail parameter is increased from 0 to 1.\nNote that this utility also allows passing additional instructions.\nFinally, note that the utility allows for recursive summarization, where each summary is based on the previous summaries, adding more context to the summarization process. This can be enabled by setting the summarize_recursively parameter to True. This is more computationally expensive, but can increase consistency and coherence of the combined summary."])</script><script>self.__next_f.push([1,"94:T1079,"])</script><script>self.__next_f.push([1,"\nThe new GPT-4 Turbo model, available as gpt-4-turbo-2024-04-09 as of April 2024, now enables function calling with vision capabilities, better reasoning and a knowledge cutoff date of Dec 2023. Using images with function calling will unlock multimodal use cases and the ability to use reasoning, allowing you to go beyond OCR and image descriptions.\n\nWe will go through two examples to demonstrate the use of function calling with GPT-4 Turbo with Vision:\n\nSimulating a customer service assistant for delivery exception support\nAnalyzing an organizational chart to extract employee information\nInstallation and Setup\n1. Simulating a customer service assistant for delivery exception support\nWe will simulate a customer service assistant for a delivery service that is equipped to analyze images of packages. The assistant will perform the following actions based on the image analysis:\nIf a package appears damaged in the image, automatically process a refund according to policy.\nIf the package looks wet, initiate a replacement.\nIf the package appears normal and not damaged, escalate to an agent.\nLet's look at the sample images of packages that the customer service assistant will analyze to determine the appropriate action. We will encode the images as base64 strings for processing by the model.\nWe have successfully encoded the sample images as base64 strings and displayed them. The customer service assistant will analyze these images to determine the appropriate action based on the package condition.\n\nLet's now define the functions/tools for order processing, such as escalating an order to an agent, refunding an order, and replacing an order. We will create placeholder functions to simulate the processing of these actions based on the identified tools. We will be using Pydantic models to define the structure of the data for order actions.\n\nSimulating user messages and processing the package images\n\nWe will simulate user messages containing the package images and process the images using the GPT-4 Turbo with Vision model. The model will identify the appropriate tool call based on the image analysis and the predefined actions for damaged, wet, or normal packages. We will then process the identified action based on the order ID and display the results.\n2.  Analyzing an organizational chart to extract employee information\n\nFor the second example, we will analyze an organizational chart image to extract employee information, such as employee names, roles, managers, and manager roles. We will use GPT-4 Turbo with Vision to process the organizational chart image and extract structured data about the employees in the organization. Indeed, function calling lets us go beyond OCR to actually deduce and translate hierarchical relationships within the chart.\n\nWe will start with a sample organizational chart in PDF format that we want to analyze and convert the first page of the PDF to a JPEG image for analysis.\nThe organizational chart image has been successfully extracted from the PDF file and displayed. Let's now define a function to analyze the organizational chart image using the new GPT4 Turbo with Vision. The function will extract information about the employees, their roles, and their managers from the image. We will use function/tool calling to specify the input parameters for the organizational structure, such as the employee name, role, and manager's name and role. We will use Pydantic models to define the structure of the data.\n\nNow, we will define a function to parse the response from GPT-4 Turbo with Vision and extract the employee data. We will tabulate the extracted data for easy visualization. Please note that the accuracy of the extracted data may vary based on the complexity and clarity of the input image.\nThe extracted data from the organizational chart has been successfully parsed and displayed in a DataFrame. This approach allows us to leverage GPT-4 Turbo with Vision capabilities to extract structured information from images, such as organizational charts and diagrams, and process the data for further analysis. By using function calling, we can extend the functionality of multimodal models to perform specific tasks or call external functions."])</script><script>self.__next_f.push([1,"95:T2a18,"])</script><script>self.__next_f.push([1,"\nSynthetic data generation using large language models (LLMs) offers a powerful solution to a commonly faced problem: the availability of high-quality, diverse, and privacy-compliant data. This could be used in a number of scenarios such as training a data science  machine learning model (SVMs, decision trees, KNN's), finetuning a different GPT model on the data, as a solution to the coldstart problem, helping build compelling demos/apps with realistic data, scenario testing etc.\n\nThere are a number of key drivers which may see you wanting to leverage synthetic data.\nHuman data may have privacy restrictions and/or identifiable data within it which we do not want to be used.\nSynthetic data can be much more structured and therefore easier to manipulate than real data.\nIn domains where data is sparse or data of certain categories is sparse we may want to augment the data.\nWhen dealing with imbalanced datasets or datasets which lack diversity, we may want to create data to improve the richness of our datasets.\n\nUnlike traditional data augmentation or manual data creation methods, using LLMs allows for the generation of rich, nuanced, and contextually relevant datasets that can significantly enhance it's usefulness to enterprises and developers.\n\nWe split this tutorial into 2 parts. In this cookbook, we will have the following agenda:\nCSV with a structured prompt\nCSV with a Python program\nMultitable CSV with a python program\nSimply creating textual data\nDealing with imbalanced or non-diverse textual data\nwhile in part 2, we will look at prompting strategies for getting better textual data.\n\nThe last two in particular are useful for creating synthetic data to finetune another GPT model. For example using higher quality data produced by gpt-4o to finetune the cheaper and quicker gpt-3.5-turbo for improved performance while reducing costs.\n\nGetting setup\n1. CSV with a structure prompt\nHere we create data in the simplest way. You can quickly generate data by addressing 3 key points: telling it the format of the data (CSV), the schema, and useful information regarding how columns relate (the LLM will be able to deduce this from the column names but a helping hand will improve performance).\n2. CSV with a Python program\nThe issue with generating data directly is we are limited in the amount of data we can generate because of the context. Instead what we can do is ask the LLM to generate a python program to generate the synthetic data. This allows us to scale to much more data while also providing us a view into how the data was generated by inspecting the python program.\n\nThis would then let us edit the python program as we desire while giving us a good basis to start from.\n\nWe need to make sure to parse the output of this appropriately as often there may be surrounding text to the python code. We can also explicitly ask it to state all assumptions it made about the data it's generating, however in this circumstance it told us that automatically.\n3. Multitable CSV with a python program\nFor more complex relationships however we need to make sure to specify a few more characteristics.\n\nTo create multiple different datasets which relate to each other (for example housing, location, house type), as before we would need to specify the format, schema and useful information. However, the useful information required to get good performance is higher now. It's case-specific but a good amount of things to describe would be how the datasets relate to each other, addressing the size of the datasets in relation to one another, making sure foreign and primary keys are made appropriately and ideally using previously generated datasets to populate new ones so the actual data values match where necessary.\n4. Simply creating textual data\nHere we take a first look at creating textual data. This can be used to finetune another GPT model for example. In this case we imagine ourselves a retailer trying to streamline the process of creating descriptions for items they are selling. We again need to specify the format of the data, in particular in this case we want one which is easy to parse as an output.\nThe example we consider below is one in which we want to create input output training pairs for GPT model to finetune on. We will have the products' name and the category it belongs to as input and the output will be a description.\n\nSpecifying the structure of the output explicitly and giving commands to not deviate from this help enforce the output structure. You can run this in a loop and append the data to generate more synthetic data. Again, as before we will need to parse the data well so that our code further downstream does not break.\nNote: the above output is truncated. And now we can parse it as below to get a list of products, categories and their descriptions. For example, let's take a look at the products it's generated.\n\n5. Dealing with imbalanced or non-diverse textual data\nSome of the most important aspects of generating high-quality synthetic data are accuracy (does the data make sense), consistency (are two separate data points for the same input roughly the same) and diversity (making sure our data distribution matches as much of the distribution that exists in production).\n\n\nTo increase the diversity of our data, we start first by clustering the data. This will provide us information about which clusters are underrepresented (imbalanced dataset) or which data is not addressed at all (widening the data distribution). Then, we will either suggest new clusters (using self-reflection type call from GPT) or ask the next iteration of our synthetic generation calls to explicitly target the underrepresented clusters.\n\nWe can then recursively run this generation and analysis of cluster loop to automate generating diverse synthetic data.\nFor demonstrative purposes, we explicitly prompt the LLM to generate information about 4 different topical areas: vehicle, clothing, toiletries, food. We will then cluster the data and see if it managed to find these 4 topic areas.\nNote: The above output is truncated. In the example above, we would explicitly include the topic area as part of the response per example as it helps condition the proceeding output and tends to give better performance. We can also give it an actual example of what the output should look like so it gets the right idea of style of output but also to help enforce structure.\nWe will now cluster the data to analyze it. We will use K-means clustering to segregate the data. An important parameter of K-means to set is K, the number of clusters.\n\nWe know that there should be 4 cluster (4 topics) since we specified this in prompt: vehicle, electronics, clothing, food. However in general for our data, we do not know the number of clusters that exist. Therefore we will use the elbow method to find the optimal number of clusters.\n\nIn the elbow method, we iterate through a range of different K's, each time storing the inertia. The inertia measures the sum of the squared distances between each point in a cluster and the centroid of that cluster thus telling us how well-separated and dense each cluster is. If we plot K against the inertia, we are able to see how the inertia drops and where the drop in inertia is least rapid (often making an elbow shape) we can set our optimal number of clusters. You can read into more depth about the elbow method here).\nFirst let's store our data into a pandas dataframe for ease of analysis\n\n\nNext let us embed our data as the embeddings is what we will cluster since they should be close to each other in vector space if they are similar.\nNow we perform the elbow method.\nThis will output a chart for us in which we have to visually tell where the optimal cluster point is. We can see below that we see a gradual decrease of inertia rather than a sharp elbow but the point of steepest decrease appears to occur around 3, 4 or 5 clusters which lines up with our expectations given our prompt.\nelbow_chart\nFor demonstration purposes we will pick 5 as the optimal cluster number to show it doesn't matter exactly where we pick it as long as we are approximately right. There are numerous correct ways to categorize data. We also store which cluster each data point belongs to.\nWe will analyze the cluster data now. There are two separate things we will look to address. 1. imbalanced data, 2. Expanding the data distribution.\nFirst for imbalanced data we count the number of examples in each cluster. Then we select a few examples from each cluster at random and ask the LLM what topics these map to.\nWe can see the topics found here:\nEco-friendly Transportation, Luxury and Leisure Items, Personal Care Products, Electronic Toothbrushes and Clothing and Apparel\nmatch well enough but not exactly to our initial prompt of:\nvehicle, clothing, toiletries, food.\n\nAs we chose 5 clusters, it split up toiletries into Skincare and Personal Care which doesn't affect us too much further downstream.\nWe now have the clusters and their counts so we could prompt the LLM to generate more examples within the topics we want. However for this example we won't take that further as they are well-split and you would just follow the procedure above for prompting the model to generate data while passing in the underrepresented topics.\nNext, we will try and deal with increasing the diversity of our data distribution.\n\nFirst we start in a similar way by finding a few examples from each cluster at random and ask the LLM what topics these map to. In addition to this in the same LLM call, we will ask it to generate more topics to increase the diversity of our data. We do this in one call to save time/cost.\nWe can see here again that we explicitly prompt the output structure it should follow. I also tell it the purpose of generating topics (to promote diversity) so the model has full context.\nWe then parse the data into a list of cluster-mapping jsons and a list of topics\nAnd finally we can use this information to further prompt a model to keep generating synthetic data. We do this by passing all the topics in the list of jsons to the prompt below.\nYou can run this in a loop to append to your previous data and in this way you can keep generating more textual synthetic data to train another GPT model while making sure that we cater to imbalanced datasets and generating a diversity of data.\nYou have now completed part 1 of the synthetic data generation tutorial where we have gone through:\nCSV with a structured prompt\nCSV with a Python program\nMultitable CSV with a python program\nSimply creating textual data\nDealing with imbalanced or non-diverse textual data\n\nIn part 2 you will find find out techniques for better prompting an LLM to enhance textual synthetic data generation."])</script><script>self.__next_f.push([1,"96:T14d6,"])</script><script>self.__next_f.push([1,"\nMultimodal RAG integrates additional modalities into traditional text-based RAG, enhancing LLMs' question-answering by providing extra context and grounding textual data for improved understanding.\n\nAdopting the approach from the clothing matchmaker cookbook, we directly embed images for similarity search, bypassing the lossy process of text captioning, to boost retrieval accuracy.\n\nUsing CLIP-based embeddings further allows fine-tuning with specific data or updating with unseen images.\n\nThis technique is showcased through searching an enterprise knowledge base with user-provided tech images to deliver pertinent information.\nInstallations\nFirst let's install the relevant packages.\nThen let's import all the needed packages.\n\nNow let's load the CLIP model.\n\nWe will now:\nCreate the image embedding database\nSet up a query to the vision model\nPerform the semantic search\nPass a user query to the image\n\nCreate image embedding database\nNext we will create our image embeddings knowledge base from a directory of images. This will be the knowledge base of technology that we search through to provide information to the user for an image they upload.\n\nWe pass in the directory in which we store our images (as JPEGs) and loop through each to create our embeddings.\n\nWe also have a description.json. This has an entry for every single image in our knowledge base. It has two keys: 'image_path' and 'description'. It maps each image to a useful description of this image to aid in answering the user question.\nFirst let's write a function to get all the image paths in a given directory. We will then get all the jpeg's from a directory called 'image_database'\nNext we will write a function to get the image embeddings from the CLIP model given a series of paths.\n\nWe first preprocess the image using the preprocess function we got earlier. This performs a few things to ensure the input to the CLIP model is of the right format and dimensionality including resizing, normalization, colour channel adjustment etc.\n\nWe then stack these preprocessed images together so we can pass them into the model at once rather than in a loop. And finally return the model output which is an array of embeddings.\nWe can now create our vector database.\nAnd also ingest our json for image-description mapping and create a list of jsons. We also create a helper function to search through this list for a given image we want, so we can obtain the description of that image\nLet us display an example image, this will be the user uploaded image. This is a piece of tech that was unveiled at the 2024 CES. It is the DELTA Pro Ultra Whole House Battery Generator.\nDelta Pro\nQuerying the vision model\nNow let's have a look at what GPT-4 Vision (which wouldn't have seen this technology before) will label it as.\n\n\nFirst we will need to write a function to encode our image in base64 as this is the format we will pass into the vision model. Then we will create a generic image_query function to allow us to query the LLM with an image input.\nAs we can see, it tries its best from the information it's been trained on but it makes a mistake due to it not having seen anything similar in its training data. This is because it is an ambiguous image making it difficult to extrapolate and deduce.\nPerforming semantic search\nNow let's perform similarity search to find the two most similar images in our knowledge base. We do this by getting the embeddings of a user inputted image_path, retrieving the indexes and distances of the similar iamges in our database. Distance will be our proxy metric for similarity and a smaller distance means more similar. We then sort based on distance in descending order.\nWe require the indices as we will use this to serach through our image_directory and selecting the image at the location of the index to feed into the vision model for RAG.\nAnd let's see what it brought back (we display these in order of similarity):\nDelta Pro2\n\nDelta Pro3\nWe can see here it brought back two images which contain the DELTA Pro Ultra Whole House Battery Generator. In one of the images it also has some background which could be distracting but manages to find the right image.\nUser querying the most similar image\nNow for our most similar image, we want to pass it and the description of it to gpt-v with a user query so they can inquire about the technology that they may have bought. This is where the power of the vision model comes in, where you can ask general queries for which the model hasn't been explicitly trained on to the model and it responds with high accuracy.\nIn our example below, we will inquire as to the capacity of the item in question.\nAnd we see it is able to answer the question. This was only possible by matching images directly and from there gathering the relevant description as context.\nConclusion\nIn this notebook, we have gone through how to use the CLIP model, an example of creating an image embedding database using the CLIP model, performing semantic search and finally providing a user query to answer the question.\nThe applications of this pattern of usage spread across many different application domains and this is easily improved to further enhance the technique. For example you may finetune CLIP, you may improve the retrieval process just like in RAG and you can prompt engineer GPT-V."])</script><script>self.__next_f.push([1,"97:Tf07,"])</script><script>self.__next_f.push([1,"\nThe new Batch API allows to create async batch jobs for a lower price and with higher rate limits.\n\nBatches will be completed within 24h, but may be processed sooner depending on global usage.\n\nIdeal use cases for the Batch API include:\n\nTagging, captioning, or enriching content on a marketplace or blog\nCategorizing and suggesting answers for support tickets\nPerforming sentiment analysis on large datasets of customer feedback\nGenerating summaries or translations for collections of documents or articles\n\nand much more!\n\nThis cookbook will walk you through how to use the Batch API with a couple of practical examples.\n\nWe will start with an example to categorize movies using gpt-4o-mini, and then cover how we can use the vision capabilities of this model to caption images.\n\nPlease note that multiple models are available through the Batch API, and that you can use the same parameters in your Batch API calls as with the Chat Completions endpoint.\nSetup\nFirst example: Categorizing movies\n\nIn this example, we will use gpt-4o-mini to extract movie categories from a description of the movie. We will also extract a 1-sentence summary from this description.\n\nWe will use JSON mode to extract categories as an array of strings and the 1-sentence summary in a structured format.\n\nFor each movie, we want to get a result that looks like this:\n\n{\n    categories: ['category1', 'category2', 'category3'],\n    summary: '1-sentence summary'\n}\nLoading data\n\nWe will use the IMDB top 1000 movies dataset for this example.\nProcessing step\n\nHere, we will prepare our requests by first trying them out with the Chat Completions endpoint.\n\nOnce we're happy with the results, we can move on to creating the batch file.\nCreating the batch file\n\nThe batch file, in the jsonl format, should contain one line (json object) per request.\nEach request is defined as such:\n\n{\n    \"custom_id\": ,\n    \"method\": \"POST\",\n    \"url\": \"/v1/chat/completions\",\n    \"body\": {\n        \"model\": ,\n        \"messages\": ,\n        // other parameters\n    }\n}\n\nNote: the request ID should be unique per batch. This is what you can use to match results to the initial input files, as requests will not be returned in the same order.\nUploading the file\nCreating the batch job\nChecking batch status\n\nNote: this can take up to 24h, but it will usually be completed faster.\n\nYou can continue checking until the status is 'completed'.\nRetrieving results\nReading results\nReminder: the results are not in the same order as in the input file.\nMake sure to check the custom_id to match the results against the input requests\nSecond example: Captioning images\n\nIn this example, we will use gpt-4-turbo to caption images of furniture items.\n\nWe will use the vision capabilities of the model to analyze the images and generate the captions.\nLoading data\n\nWe will use the Amazon furniture dataset for this example.\nProcessing step\n\nAgain, we will first prepare our requests with the Chat Completions endpoint, and create the batch file afterwards.\nCreating the batch job\n\nAs with the first example, we will create an array of json tasks to generate a jsonl file and use it to create the batch job.\nGetting results\n\nAs with the first example, we can retrieve results once the batch job is done.\n\nReminder: the results are not in the same order as in the input file.\nMake sure to check the custom_id to match the results against the input requests\nWrapping up\n\nIn this cookbook, we have seen two examples of how to use the new Batch API, but keep in mind that the Batch API works the same way as the Chat Completions endpoint, supporting the same parameters and most of the recent models (gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo...).\n\nBy using this API, you can significantly reduce costs, so we recommend switching every workload that can happen async to a batch job with this new API."])</script><script>self.__next_f.push([1,"98:T6e6,"])</script><script>self.__next_f.push([1,"\nThe ChatCompletion endpoint now includes the ability to specify whether a tool must be called every time, by adding tool_choice='required' as a parameter.\n\nThis adds an element of determinism to how you build your wrapping application, as you can count on a tool being provided with every call. We'll demonstrate here how this can be useful for a contained flow like customer service, where having the ability to define specific exit points gives more control.\n\nThe notebook concludes with a multi-turn evaluation, where we spin up a customer GPT to imitate our customer and test the LLM customer service agent we've set up.\nConfig definition\n\nWe will define tools and instructions which our LLM customer service agent will use. It will source the right instructions for the problem the customer is facing, and use those to answer the customer's query.\n\nAs this is a demo example, we'll ask the model to make up values where it doesn't have external systems to source info.\nExample\n\nTo test this we will run an example for a customer who has experienced fraud, and see how the model handles it.\n\nPlay the role of the user and provide plausible next steps to keep the conversation going.\nEvaluation\n\nNow we'll do a simple evaluation where a GPT will pretend to be our customer. The two will go back and forth until a resolution is reached.\n\nWe'll reuse the functions above, adding an execute_conversation function where the customer GPT will continue answering.\nConclusion\n\nYou can now control your LLM's behaviour explicitly by making tool use mandatory, as well as spin up GPT testers to challenge your LLM and to act as automated test cases.\n\nWe hope this has given you an appreciation for a great use case for tool use, and look forward to seeing what you build!"])</script><script>self.__next_f.push([1,"99:T1635,"])</script><script>self.__next_f.push([1,"\nGPT-4o (\"o\" for \"omni\") and GPT-4o mini are natively multimodal models designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats. GPT-4o mini is the lightweight version of GPT-4o.\n\nBackground\n\nBefore GPT-4o, users could interact with ChatGPT using Voice Mode, which operated with three separate models. GPT-4o integrates these capabilities into a single model that's trained across text, vision, and audio. This unified approach ensures that all inputs — whether text, visual, or auditory — are processed cohesively by the same neural network.\n\nGPT-4o mini is the next iteration of this omni model family, available in a smaller and cheaper version. This model offers higher accuracy than GPT-3.5 Turbo while being just as fast and supporting multimodal inputs and outputs.\n\nCurrent API Capabilities\n\nCurrently, the API supports {text, image} inputs only, with {text} outputs, the same modalities as gpt-4-turbo.\n\nAdditional modalities, including audio, will be introduced soon. This guide will help you get started with using GPT-4o mini for text, image, and video understanding.\nGetting Started\nInstall OpenAI SDK for Python\n\nConfigure the OpenAI client and submit a test request\nTo setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage.\n\nYou can get an API key by following these steps:\nCreate a new project\nGenerate an API key in your project\n(RECOMMENDED, BUT NOT REQUIRED) Setup your API key for all projects as an env var\n\nOnce we have this setup, let's start with a simple {text} input to the model for our first request. We'll use both system and user messages for our first request, and we'll receive a response from the assistant role.\nImage Processing\nGPT-4o mini can directly process images and take intelligent actions based on the image. We can provide images in two formats:\nBase64 Encoded\nURL\n\nLet's first view the image we'll use, then try sending this image as both Base64 and as a URL link to the API\nBase64 Image Processing\nURL Image Processing\nVideo Processing\nWhile it's not possible to directly send a video to the API, GPT-4o can understand videos if you sample frames and then provide them as images.\n\nSince GPT-4o mini in the API does not yet support audio-in (as of July 2024), we'll use a combination of GPT-4o mini and Whisper to process both the audio and visual for a provided video, and showcase two usecases:\nSummarization\nQuestion and Answering\n\nSetup for Video Processing\nWe'll use two python packages for video processing - opencv-python and moviepy.\n\nThese require ffmpeg, so make sure to install this beforehand. Depending on your OS, you may need to run brew install ffmpeg or sudo apt install ffmpeg\nProcess the video into two components: frames and audio\nExample 1: Summarization\nNow that we have both the video frames and the audio, let's run a few different tests to generate a video summary to compare the results of using the models with different modalities. We should expect to see that the summary generated with context from both visual and audio inputs will be the most accurate, as the model is able to use the entire context from the video.\n\nVisual Summary\nAudio Summary\nVisual + Audio Summary\n\nVisual Summary\nThe visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker.\nThe results are as expected - the model is able to capture the high level aspects of the video visuals, but misses the details provided in the speech.\n\nAudio Summary\nThe audio summary is generated by sending the model the audio transcript. With just the audio, the model is likely to bias towards the audio content, and will miss the context provided by the presentations and visuals.\n\n{audio} input for GPT-4o isn't currently available but will be coming soon! For now, we use our existing whisper-1 model to process the audio\nThe audio summary is biased towards the content discussed during the speech, but comes out with much less structure than the video summary.\n\nAudio + Visual Summary\nThe Audio + Visual summary is generated by sending the model both the visual and the audio from the video at once. When sending both of these, the model is expected to better summarize since it can perceive the entire video at once.\nAfter combining both the video and audio, we're able to get a much more detailed and comprehensive summary for the event which uses information from both the visual and audio elements from the video.\n\nExample 2: Question and Answering\nFor the Q\u0026A, we'll use the same concept as before to ask questions of our processed video while running the same 3 tests to demonstrate the benefit of combining input modalities:\nVisual Q\u0026A\nAudio Q\u0026A\nVisual + Audio Q\u0026A\nComparing the three answers, the most accurate answer is generated by using both the audio and visual from the video. Sam Altman did not discuss the raising windows or radio on during the Keynote, but referenced an improved capability for the model to execute multiple functions in a single request while the examples were shown behind him.\n\nConclusion\n\nIntegrating many input modalities such as audio, visual, and textual, significantly enhances the performance of the model on a diverse range of tasks. This multimodal approach allows for more comprehensive understanding and interaction, mirroring more closely how humans perceive and process information.\n\nCurrently, GPT-4o and GPT-4o mini in the API support text and image inputs, with audio capabilities coming soon."])</script><script>self.__next_f.push([1,"9a:T2508,"])</script><script>self.__next_f.push([1,"This notebook provides step by step instuctions on using Azure AI Search (f.k.a Azure Cognitive Search) as a vector database with OpenAI embeddings, then creating an Azure Function on top to plug into a Custom GPT in ChatGPT.\n\nThis can be a solution for customers looking to set up RAG infrastructure contained within Azure, and exposing it as an endpoint to integrate that with other platforms such as ChatGPT.\n\nAzure AI Search is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\n\nAzure Functions is a serverless compute service that runs event-driven code, automatically managing infrastructure, scaling, and integrating with other Azure services.\nPrerequisites:\nFor the purposes of this exercise you must have the following:\nAzure user with permission to create Azure AI Search Service and Azure Function Apps\nAzure subscription ID and a resource group.\nOpenAI Key\nArchitecture\nBelow is a diagram of the architecture of this solution, which we'll walk through step-by-step.\n\nazure-rag-architecture.png\nNote: This architecture pattern of vector data store + serverless functions can be extrapolated to other vector data stores. For example, if you would want to use something like Postgres within Azure, you'd change the Configure Azure AI Search Settings step to set-up the requirements for Postgres, you'd modify the Create Azure AI Vector Search to create the database and table in Postgres instead, and you'd update the function_app.py code in this repository to query Postgres instead of Azure AI Search. The data preparation and creation of the Azure Function would stay consistent.\n\nTable of Contents:\nSetup of Environment\n    Setup environment by installing and importing the required libraries and configuring our Azure settings. Includes:\n     Install and Import Required Libraries\n     Configure OpenAI Settings\n     Configure Azure AI Search Settings\n\n\nPrepare Data Prepare the data for uploading by embedding the documents, as well as capturing additional metadata. We will use a subset of OpenAI's docs as example data for this.\n\nCreate Azure AI Vector Search Create an Azure AI Vector Search and upload the data we've prepared. Includes:\n     Create Index: Steps to create an index in Azure AI Search.\n     Upload Data: Instructions to upload data to Azure AI Search.\n     Test Search: Steps to test the search functionality.\n\nCreate Azure Function Create an Azure Function to interact with the Azure AI Vector Search. Includes:\n     Create Storage Account: Steps to create a storage account for the Azure Function.\n     Create Function App: Instructions to create a function app in Azure.\n\nInput in a Custom GPT in ChatGPT Integrate the Azure Function with a Custom GPT in ChatGPT. Includes:\n     Create OpenAPI Spec: Steps to create an OpenAPI specification for the Azure Function.\n     Create GPT Instructions: Instructions to create GPT-specific instructions for the integration.\n\nSet up environment\nWe'll set up our environment by importing the required libraries and configuring our Azure settings.\nInstall and import required libraries\nWe categorize these libraries into standard Python libraries, third-party libraries, and Azure-related libraries for readability.\nConfigure OpenAI settings\n\nBefore going through this section, make sure you have your OpenAI API key.\n\nConfigure Azure AI Search Settings\nYou can locate your Azure AI Search service details in the Azure Portal or programmatically via the Search Management SDK.\n\nPrerequisites:\nSubscription ID from Azure\nResource Group name from Azure\nRegion in Azure\nCreate and Configure Azure AI Search Service\nBelow we'll generate a unique name for the search service, set up the service properties, and create the search service.\nGet the Search Service API Key\nNow that we have the search service up and running, we need the Search Service API Key, which we'll use to initiate the index creation, and later to execute the search.\nPrepare data\nWe're going to embed and store a few pages of the OpenAI docs in the oai_docs folder. We'll first embed each, add it to a CSV, and then use that CSV to upload to the index.\n\nIn order to handle longer text files beyond the context of 8191 tokens, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).\n\nWe will take a function from Python's own cookbook that breaks up a sequence into chunks.\nNow we define a function that encodes a string into tokens and then breaks it up into chunks. We'll use tiktoken, a fast open-source tokenizer by OpenAI.\n\nTo read more about counting tokens with Tiktoken, check out this cookbook.\n\nFinally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The average flag can be set to True to return the weighted average of the chunk embeddings, or False to simply return the unmodified list of chunk embeddings.\nNote: there are other, more sophisticated techniques you can take here, including:\n- using GPT-4o to capture images/chart descriptions for embedding.\n- keeping text overlap between the chunks to minimize cutting off important context.\n- chunking based on paragraphs or sections.\n- adding more descriptive metadata about each article.\nNext, we can define a helper function that will capture additional metadata about the documents. This is useful to use as a metadata filter for search queries, and capturing richer data for search.\n\nIn this example, I'll choose from a list of categories to use later on in a metadata filter.\nNow, we can define some helper functions to process the .txt files in the oai_docs folder within the data folder. You can use this with your own data as well and supports both .txt and .pdf files.\nWe'll now use this helper function to process our OpenAI documentation. Feel free to update this to use your own data by changing the folder in process_files below.\n\nNote that this will process the documents in chosen folder concurrently, so this should take  Note that this Azure Function does not have any authentication. However, you can set authentication on it following docs here\nCreate storage account\nWe can create a new storage account using the code below, but feel free to skip that block and modify the subsequent steps to use an existing storage account. This may take up to 30 seconds.\nCreate Function App\nThis Function App is where the python code will execute once it is triggered via a GPT Action. To read more about Function Apps, see the docs here.\nTo deploy Function Apps, we'll need to use the Azure CLI and Azure Functions Core Tools.\nThe below will attempt to install it and run it based on your platform type in your virtual environment, but if that does not work, read the Azure documentation to figure out how to install Azure Function Core Tools and Azure CLI. After doing that, run the below subprocess.run commands in your terminal after navigating to this folder.\nFirst we'll make sure we have the relevant tools in the environment in order to run the Azure commands necessary. This may take a few minutes to install.\nNow, we need to create a local.settings.json file with our key environment variables for Azure\nCheck the local.settings.json file and make sure that the environment variables match what you expect.\n\nNow, give your app a name below, and you are ready to create your Function App and then publish your function.\nOnce we've created the Function App, we now want to add the configuration variables to the function app to use in the function. Specifically, we need the OPENAI_API_KEY, the SEARCH_SERVICE_API_KEY, and the EMBEDDINGS_MODEL as these are all used in the function_app.py code.\nWe are now ready to publish your function code function_app.py to the Azure Function. This may take up to 10 minutes to deploy. Once this is finished, we now have an API endpoint using an Azure Function on top of Azure AI Search.\nInput in a Custom GPT in ChatGPT\nNow that we have an Azure Function that queries this Vector Search Index, let's put it as a GPT Action!\n\nSee documentation here on GPTs and here on GPT Actions. Use the below as the instructions for the GPT and as the OpenAPI spec for the GPT Action.\n\nCreate OpenAPI Spec\nBelow is a sample OpenAPI spec. When we run the block below, a functional spec should be copied to the clipboard to paste in the GPT Action.\n\nNote that this does not have any authentication by default, but you can set up Azure Functions with OAuth by following the pattern in this cookbook in the Authentication section or looking at the documentation here.\nCreate GPT Instructions\n\nFeel free to modify instructions as you see fit. Check out our docs here for some tips on prompt engineering.\nWe now have a GPT that queries a vector database!\nRecap\nWe've now successfully integrated Azure AI Search with GPT Actions in ChatGPT by doing the following:\nembedded them using OpenAI's embeddings, while adding some additional metadata using gpt-4o.\nuploaded that data to Azure AI Search.\ncreated an endpoint to query it using Azure Functions.\nincorporated it into a Custom GPT.\n\nOur GPT can now retrieve information to help answer user queries, making it much more accurate and customized to our data. Here's the GPT in action:\nazure-rag-quickstart-gpt.png"])</script><script>self.__next_f.push([1,"9b:T74c,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to a Weather.gov weather forecast. This Action takes a user’s question about a location, converts the lat-long into a weather forecast office (WFO), x, and y coordinates, then converts those 3 values into a weather forecast.\n\nNote: When setting up the GPT Action, for authentication, leave it with \"None\". This is a public API and does not require any Authentication\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to forecast the weather\n\nExample Use Cases:\nUsers can plan out their day based on weather patterns\nUsers can quickly visualize (including graphs) what the weather is forecasted to look like\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://www.weather.gov/\nApplication API Documentation: https://www.weather.gov/documentation/services-web-api\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nFAQ \u0026 Troubleshooting\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"9c:Tfab,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Google BigQuery, Google Cloud's Analytical Data Warehouse. This Action takes a user’s question, scans the relevant tables to gather the data schema, then writes a SQL query to answer the user’s question.\n\nNote: these instructions return back a functioning SQL statement, rather than the result itself. Currently middleware is required to return back a CSV file – we’ll be posting instructions on an example of that soon\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to BigQuery's DWH.\n\nExample Use Cases:\nData scientists can connect to tables and run data analyses using ChatGPT's Data Analysis\nCitizen data users can ask basic questions of their transactional data\nUsers gain more visibility into their data \u0026 potential anomalies\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://cloud.google.com/bigquery\nApplication API Documentation: https://cloud.google.com/bigquery/docs/reference/rest\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nSet up a GCP project\nSet up a BQ dataset in that GCP project\nEnsure that the user authenticating into BigQuery via ChatGPT has access to that BQ dataset\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\nGo to the Google Cloud Console\nNavigate to API \u0026 Services \u003e Credentials\nCreate new OAuth credentials (or use an existing one)\nLocate your OAuth Client ID \u0026 Client Secret and store both values securely (see screenshot below)\ngptactions_BigQuery_auth.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID**: use Client ID from steps above\nClient Secret**: use Client Secret from steps above\nAuthorization URL**: https://accounts.google.com/o/oauth2/auth\nToken URL**: https://oauth2.googleapis.com/token\nScope**: https://www.googleapis.com/auth/bigquery\nToken**: Default (POST)\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\nIn the “Authorized redirect URIs” (see screenshot above), add your callback URL\n\nFAQ \u0026 Troubleshooting\nCallback URL Error: If you get a callback URL error in ChatGPT, pay close attention to the screenshot above. You need to add the callback URL directly into GCP for the action to authenticate correctly\nSchema calls the wrong project or dataset: If ChatGPT calls the wrong project or dataset, consider updating your instructions to make it more explicit either (a) which project / dataset should be called or (b) to require the user provide those exact details before it runs the query\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"9d:T1784,"])</script><script>self.__next_f.push([1,"\nA lot of enterprise data is unstructured and locked up in difficult-to-use formats, e.g. PDFs, PPT, PNG, that are not optimized for use with LLMs or databases. As a result this type of data tends to be underutilized for analysis and product development, despite it being so valuable. The traditional way of extracting information from unstructured or non-ideal formats has been to use OCR, but OCR struggles with complex layouts and can have limited multilingual support. Moreover, manually applying transforms to data can be cumbersome and timeconsuming.\n\nThe multi-modal capabilities of GPT-4o enable new ways to extract and transform data because of GPT-4o's ability to adapt to different types of documents and to use reasoning for interpreting the content of documents. Here are some reasons why you would choose GPT-4o for your extraction and transformation workflows over traditional methods.\n\n| Extraction                                               | Transformation                                              |\n|---------------------------------------------------------------|------------------------------------------------------------------|\n| Adaptable: Handles complex document layouts better, reducing errors | Schema Adaptability: Easily transforms data to fit specific schemas for database ingestion |\n| Multilingual Support: Seamlessly processes documents in multiple languages | Dynamic Data Mapping: Adapts to different data structures and formats, providing flexible transformation rules |\n| Contextual Understanding: Extracts meaningful relationships and context, not just text | Enhanced Insight Generation: Applies reasoning to create more insightful transformations, enriching the dataset with derived metrics, metadata and relationships |\n| Multimodality: Processes various document elements, including images and tables |  |\n\nThis cookbook has three parts:\nHow to extract data from multilingual PDFs\nHow to transform data according to a schema for loading into a database\nHow to load transformed data into a database for downstream analysis\n\nWe're going to mimic a simple ELT workflow where data is first extracted from PDFs into JSON using GPT-4o, stored in an unstructured format somewhere like a data lake, transformed to fit a schema using GPT-4o, and then finally ingested into a relational database for querying. It's worth noting that you can do all of this with the BatchAPI if you're interested in lowering the cost of this workflow.\n\nThe data we'll be using is a set of publicly available 2019 hotel invoices from Germany available on Jens Walter's GitHub, (thank you Jens!). Though hotel invoices generally contain similar information (reservation details, charges, taxes etc.), you'll notice that the invoices present itemized information in different ways and are multilingual containing both German and English. Fortunately GPT-4o can adapt to a variety of different document styles without us having to specify formats and it can seamlessly handle a variety of languages, even in the same document.\nHere is what one of the invoices looks like:\n\nPart 1: Extracting data from PDFs using GPT-4o's vision capabilities\nGPT-4o doesn't natively handle PDFs so before we extract any data we'll first need to convert each page into an image and then encode the images as base64.\nWe can then pass each base64 encoded image in a GPT-4o LLM call, specifying a high level of detail and JSON as the response format. We're not concerned about enforcing a schema at this step, we just want all of the data to be extracted regardless of type.\nBecause invoice data can span multiple pages in a PDF, we're going to produce JSON objects for each page in the invoice and then append them together. The final invoice extraction will be a single JSON file.\nEach invoice JSON will have different keys depending on what data the original invoice contained, so at this point you can store the unschematized JSON files in a data lake that can handle unstructured data. For simplicity though, we're going to store the files in a folder. Here is what one of the extracted JSON files looks like, you'll notice that even though we didn't specify a schema, GPT-4o was able to understand German and group similar information together. Moreover, if there was a blank field in the invoice GPT-4o transcribed that as \"null\".\nPart 2: Transforming data according to a schema\n\nYou've extracted data from PDFs and have likely loaded the unstructured extractions as JSON objects in a data lake. The next step in our ELT workflow is to use GPT-4o to transform the extractions according to our desired schema. This will enable us to ingest any resulting tables into a database. We've decided upon the following schema that broadly covers most of the information we would have seen across the different invoices. This schema will be used to process each raw JSON extraction into our desired schematized JSON and can specify particular formats such as \"date\": \"YYYY-MM-DD\". We're also going to translate the data into English at this step.\n\nPart 3: Loading transformed data into a database\n\nNow that we've schematized all of our data, we can segment it into tables for ingesting into a relational database. In particular, we're going to create four tables: Hotels, Invoices, Charges and Taxes. All of the invoices pertained to one guest, so we won't create a guest table.\nNow let's check that we've correctly ingested the data by running a sample SQL query to determine the most expensive hotel stay and the same of the hotel!\nYou can even automate the generation of SQL queries at this step by using function calling, check out our cookbook on function calling with model generated arguments to learn how to do that.\nTo recap in this cookbook we showed you how to use GPT-4o for extracting and transforming data that would otherwise be inaccessible for data analysis. If you don't need these workflows to happen in real-time, you can take advantage of OpenAI's BatchAPI to run jobs asynchronously at a much lower cost!"])</script><script>self.__next_f.push([1,"9e:T1305,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Outlook, Microsoft's web service for emailing and calendar events. This action assumes a user’s context and allows them to send and retrieve emails and calendar events from Outlook.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Outlook\n\nExample Use Cases:\nA user can look up all of their meetings for the day and have ChatGPT summarize the day\nA user can email a ChatGPT output to someone directly\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://portal.azure.com/\nApplication API Documentation: https://learn.microsoft.com/en-us/graph/api/overview?view=graph-rest-1.0\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nEnsure you have the access and permissions to Set up an App Registration in Azure\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with Outlook. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nAzure Steps\nApp Registration: The first step is to register a new App registration in the Azure Portal which will be used to integrate OAuth between our application and Azure Active Directory/Entra ID. Simply provide the application with a relevant name, leaving the Redirect URI blank for now as we will return to this, and save.\ngptactions_outlook_registerapplication.png\nCertificate \u0026 Secrets: We next need to generate a client secret to provide secure communication between the GPT and Azure. Within the App registration, navigate to Certificate \u0026 secrets in the sidebar\ngptactions_outlook_secrets.png\nClick New client secret and create a new client secret with desired name and expiry date. Clicking save will provide us a Secret to use in our GPT creation. Make sure to save the Value field as it’ll only be visible at creation, and we will need it later!\ngptactions_outlook_secretvalue.png\nAPI Permissions: The next step is to provide the integration with the scope it needs to perform our specific required actions.\nWithin the App registration, navigate to Manage \u003e API permissions in the sidebar.\ngptactions_outlook_permissions.png\nClick Add a permission and Microsoft graph \u003e Delegated Permissions as options in the opened side menu. Use the search bar to add the following permissions:\nCalendars.ReadWrite\nMail.Read\nMail.Send\nUser.Read\n\ngptactions_outlook_permissionadd.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID: The value listed on the Azure Registered App’s Overview page under **Application (client) ID\nClient Secret: the secret **Value saved from step 2 of Azure Steps\nFor the following two inputs, replace  with the value listed on the Registered App’s Overview page under Directory (tenant) ID\nAuthorization URL**: https://login.microsoftonline.com//oauth2/v2.0/authorize\nToken URL**: https://login.microsoftonline.com//oauth2/v2.0/token\nScope**: https://graph.microsoft.com/User.Read https://graph.microsoft.com/Mail.Send https://graph.microsoft.com/Mail.Read https://graph.microsoft.com/Calendars.ReadWrite\nToken Exchange Method**: Default (POST Request)\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\ngptactions_outlook_callback.png\nIn the Azure app, navigate to the Manage \u003e Authentication tab, click Add a platform, select Web and add your callback URL under Redirect URI\n\ngptactions_outlook_redirectconfig.png\ngptactions_outlook_redirectinput.png\nFAQ \u0026 Troubleshooting\nCallback URL Error**: If you get a callback URL error in ChatGPT, double check the Callback URL value as it can occasionally change depending on any alterations made to the authentication\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"9f:T3cf1,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis solution enables a GPT action to answer a user’s question with the context of files the user can access in SharePoint or Office365, using Microsoft’s Graph API search capabilities and the ability to retrieve files. It uses Azure Functions to process the Graph API response and convert it to a human readable format or structure it in a way ChatGPT understands. This code is meant to be directional, and you should modify it to your requirements.\n\nThis solution uses the ability to retrieve files in Actions and use them as if you had uploaded them directly to a conversation. The Azure Function returns a base64 string that ChatGPT converts into a file. This solution can handle both structured and unstructured data, but does have size volume limitations (see docs here)\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to files in Sharpeoint\n\nExample Use Cases:\nA user needs to look up which files relate to a certain topic\nA user needs an answer to a critical question, buried deep in documents\nArchitecture / Example\n\n\nThis solution uses a Node.js Azure Function to, based on the logged in user:\n\nSearch for a relevant file that the user has access to, based on the user’s initial question. \n\nFor each file that is found, convert it to a base64 string.\n\nFormat the data in the structure ChatGPT is expecting here.\n\nReturn that to ChatGPT. The GPT then can use those files as if you had uploaded it to the conversation.\n\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://www.microsoft.com/en-us/microsoft-365/sharepoint/collaboration\nApplication API Documentation: https://learn.microsoft.com/en-us/previous-versions/office/developer/sharepoint-rest-reference/\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nAccess to a Sharepoint environment\nPostman (and knowledge of APIs and OAuth)\n\nMiddleware Information\nIf you follow the search concept files guide, the Microsoft Graph Search API returns references to files that fit the criteria, but not the file contents themselves. Therefore, middleware is required, rather than hitting the MSFT endpoints directly.\n\nWe need to restructure the response from that API so that it matches the expected structure in openaiFileResponse outlined here.\nAdditional Steps\nSet up Azure Function\nSet up an Azure Function using the steps in the Azure Function cookbook\nAdd in Function Code\nNow that you have an authenticated Azure Function, we can update the function to search SharePoint / O365\n\nGo to your test function and paste in the code from this file. Save the function.\nThis code is meant to be directional - while it should work out of the box, it is designed to be customized to your needs (see examples towards the end of this document).\n\nSet up the following env variables by going to the Configuration tab on the left under Settings. Note that this may be listed directly in Environment Variables depending on your Azure UI.\n\n    TENANT_ID: copied from previous section\n\n    CLIENT_ID: copied from previous section\n\nGo to the Console tab under the Development Tools\n\n    Install the following packages in console\n\n       npm install @microsoft/microsoft-graph-client\n\n       npm install axios\n\nOnce this is complete, try calling the function (POST call) from Postman again, putting the below into body (using a query and search term you think will generate responses).\n\n         {\n        \"searchTerm\": \"\"\n    }\n\nIf you get a response, you are ready to set this up with a Custom GPT! See the ChatGPT Section of the Azure Function page for more details on setting this up\n\nMore Detailed Walkthrough\n\nThe below walks through setup instructions and walkthrough unique to this solution. You can find the entire code here.\n\nCode Walkthrough\n\nThe below walks through the different parts of the function. Before you begin, ensure you have the required packages installed and environment variables set up (see the Installation Steps section).\n\nImplementing the Authentication \n\nBelow we have a few helper functions that we’ll use in the function.\n\nInitializing the Microsoft Graph Client\n\nCreate a function to initialize the Graph client with an access token. This will be used to search through Office 365 and SharePoint.\n\nconst { Client } = require('@microsoft/microsoft-graph-client');\n\nfunction initGraphClient(accessToken) {\n    return Client.init({\n        authProvider: (done) =\u003e {\n            done(null, accessToken);\n        }\n    });\n}\n\nObtaining an On-Behalf-Of (OBO) Token\n\nThis function uses an existing bearer token to request an OBO token from Microsoft's identity platform. This enables passing through the credentials to ensure the search only returns files the logged-in user can access.\n\nconst axios = require('axios');\nconst qs = require('querystring');\n\nasync function getOboToken(userAccessToken) {\n    const { TENANT_ID, CLIENT_ID, MICROSOFT_PROVIDER_AUTHENTICATION_SECRET } = process.env;\n    const params = {\n        client_id: CLIENT_ID,\n        client_secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET,\n        grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n        assertion: userAccessToken,\n        requested_token_use: 'on_behalf_of',\n        scope: 'https://graph.microsoft.com/.default'\n    };\n\n    const url = https\\://login.microsoftonline.com/${TENANT_ID}/oauth2/v2.0/token;\n    try {\n        const response = await axios.post(url, qs.stringify(params), {\n            headers: { 'Content-Type': 'application/x-www-form-urlencoded' }\n        });\n        return response.data.access\\_token;\n    } catch (error) {\n        console.error('Error obtaining OBO token:', error.response?.data || error.message);\n        throw error;\n    }\n}\n\nRetrieving Content from O365 / SharePoint Items\n\nThis function fetches the content of drive items, converts it to a base64 string, and restructures to match the openaiFileResponse format.\nconst getDriveItemContent = async (client, driveId, itemId, name) =\u003e {\n   try\n       const filePath = /drives/${driveId}/items/${itemId};\n       const downloadPath = filePath + /content\n       // this is where we get the contents and convert to base64\n       const fileStream = await client.api(downloadPath).getStream();\n       let chunks = [];\n           for await (let chunk of fileStream) {\n               chunks.push(chunk);\n           }\n       const base64String = Buffer.concat(chunks).toString('base64');\n       // this is where we get the other metadata to include in response\n       const file = await client.api(filePath).get();\n       const mime_type = file.file.mimeType;\n       const name = file.name;\n       return {\"name\":name, \"mime_type\":mime_type, \"content\":base64String}\n   } catch (error) {\n       console.error('Error fetching drive content:', error);\n       throw new Error(Failed to fetch content for ${name}: ${error.message});\n   }\n\nCreating the Azure Function to Handle Requests\n\nNow that we have all these helper functions, the Azure Function will orchestrate the flow, by authenticating the user, performing the search, and iterating through the search results to extract the text and retrieve the relevant parts of the text to the GPT.\n\nHandling HTTP Requests: The function starts by extracting the query and searchTerm from the HTTP request. It checks if the Authorization header is present and extracts the bearer token.\n\nAuthentication: Using the bearer token, it obtains an OBO token from Microsoft's identity platform using getOboToken defined above.\n\nInitializing the Graph Client: With the OBO token, it initializes the Microsoft Graph client using initGraphClient defined above.\n\nDocument Search: It constructs a search query and sends it to the Microsoft Graph API to find documents based on the searchTerm.\n\nDocument Processing: For each document returned by the search:\n\nIt retrieves the document content using getDriveItemContent.\n\nIt converts the document to base64 string and restructures it to match the openaiFileResponse structure.\n\nResponse: The function sends them back in the HTTP response.\nmodule.exports = async function (context, req) {\n   // const query = req.query.query || (req.body \u0026\u0026 req.body.query);\n   const searchTerm = req.query.searchTerm || (req.body \u0026\u0026 req.body.searchTerm);\n   if (!req.headers.authorization) {\n       context.res = {\n           status: 400,\n           body: 'Authorization header is missing'\n       };\n       return;\n   }\n   /// The below takes the token passed to the function, to use to get an OBO token.\n   const bearerToken = req.headers.authorization.split(' ')[1];\n   let accessToken;\n   try {\n       accessToken = await getOboToken(bearerToken);\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: Failed to obtain OBO token: ${error.message}\n       };\n       return;\n   }\n   // Initialize the Graph Client using the initGraphClient function defined above\n   let client = initGraphClient(accessToken);\n   // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n   const requestBody = {\n       requests: [\n           {\n               entityTypes: ['driveItem'],\n               query: {\n                   queryString: searchTerm\n               },\n               from: 0,\n               // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents.\n               size: 10\n           }\n       ]\n   };\n\n\n   try {\n       // This is where we are doing the search\n       const list = await client.api('/search/query').post(requestBody);\n       const processList = async () =\u003e {\n           // This will go through and for each search response, grab the contents of the file and summarize with gpt-3.5-turbo\n           const results = [];\n           await Promise.all(list.value[0].hitsContainers.map(async (container) =\u003e {\n               for (const hit of container.hits) {\n                   if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                       const { name, id } = hit.resource;\n                       // The below is where the file lives\n                       const driveId = hit.resource.parentReference.driveId;\n                       // we use the helper function we defined above to get the contents, convert to base64, and restructure it\n                       const contents = await getDriveItemContent(client, driveId, id, name);\n                       results.push(contents)\n               }\n           }));\n           return results;\n       };\n       let results;\n       if (list.value[0].hitsContainers[0].total == 0) {\n           // Return no results found to the API if the Microsoft Graph API returns no results\n           results = 'No results found';\n       } else {\n           // If the Microsoft Graph API does return results, then run processList to iterate through.\n           results = await processList();\n           // this is where we structure the response so ChatGPT knows they are files\n           results = {'openaiFileResponse': results}\n       }\n       context.res = {\n           status: 200,\n           body: results\n       };\n   } catch (error) {\n       context.res = {\n           status: 500,\n           body: Error performing search or processing results: ${error.message},\n       };\n   }\n};\nCustomizations\n\nBelow are some potential areas to customize. \n\nYou can customize the GPT prompt to search again a certain amount of times if nothing is found.\n\nYou can customize the code to only search through specific SharePoint sites or O365 Drives by customizing the search query. This will help focus the search and improve the retrieval. The function as setup now looks through all files the logged-in user can access.\n\nYou can update the code to only return certain types of files. For example, only return structured data / CSVs. \n\nYou can customize the amount of files it searches through within the call to Microsoft Graph. Note that you should only put a maximum of 10 files based on the documentation here. \n\nConsiderations\n\nNote that all the same limitations of Actions apply here, with regards to returning 100K characters or less and the 45 second timeout.\n\nMake sure you read the documentation here around returning files and file uploads, as those limitations apply here.\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nThis expects a response that matches the file retrieval structure in our doc here and passes in a searchTerm parameter to inform the search.\nMake sure to switch the function app name, function name and code based on link copied in screenshot above\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nSee above and on the Azure Function cookbook for more detailed instructions on authentication.\nFAQ \u0026 Troubleshooting\nWhy are you using the Microsoft Graph API in your code instead of the SharePoint API?\n\n  The SharePoint API is legacy - per the Microsoft documentation here, “For SharePoint Online, innovation using a REST API against SharePoint is driven via the Microsoft Graph REST API's.” The Graph API gives us more flexibility, and the SharePoint API still runs into the same file issues listed in the Why is this necessary instead of interacting with the Microsoft Graph API directly? section.\n\nWhat types of files does this support?\n\n  It follows the same guidelines as the documentation here about file uploads. \n\nWhy do I need to request an OBO token?\n\n  When you try to use the same token to authenticate to the Graph API as the one you use to authenticate into the Azure Function, you get an “invalid audience” token. This is because the audience for the token can only be user\\_impersonation.\n\n  To address this, the function requests a new token scoped to Files.Read.All within the app using the On Behalf Of flow. This will inherit the permissions of the logged in user, meaning this function will only search through files the logged-in user has access to. \n\n  We are purposefully requesting a new On Behalf Of token with each request, because Azure Function Apps are meant to be stateless. You could potentially integrate this with Azure Key Vault to store the secret and retrieve programmatically. \nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a0:T52e1,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis solution enables a GPT action to answer a user’s question with the context of files the user can access in SharePoint or Office365, using Microsoft’s Graph API search capabilities and the ability to retrieve files. It uses Azure Functions to process the Graph API response and convert it to a human readable format or structure it in a way ChatGPT understands. This code is meant to be directional, and you should modify it to your requirements.\n\nThis solution pre-processes the file within the Azure Function. The Azure Function returns text, instead of the base64 encoded file. Due to the pre-processing and the conversion to text, this solution is best used for large, unstructured documents, and for when you want to analyze more than the amount of files supported in the first solution (see documentation here).\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to files in Sharpeoint\n\nExample Use Cases:\nA user needs to look up which files relate to a certain topic\nA user needs an answer to a critical question, buried deep in documents\nArchitecture / Example\n\nThis solution uses a Node.js Azure Function to, based on the logged in user:\n\nSearch for a relevant file that the user has access to, based on the user’s initial question.\n\nFor each file that is found, convert it to a consistent readable format and retrieve all the text.\n\nUse GPT 4o mini (gpt-4o-mini) to extract the relevant text from the files based on the initial user’s question. Note the pricing of GPT 4o mini here - since we are dealing with small token chunks, the cost of this step is nominal.  \n\nReturns that data to ChatGPT. The GPT then uses that information to respond to the user's initial question.\n\nAs you can see from the below architecture diagram, the first three steps are the same as Solution 1. The main difference is that this solution converts the file to text instead of a base64 string, and then summarizes that text using GPT 4o mini.\n\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://www.microsoft.com/en-us/microsoft-365/sharepoint/collaboration\nApplication API Documentation: https://learn.microsoft.com/en-us/previous-versions/office/developer/sharepoint-rest-reference/\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nAccess to a Sharepoint environment\nPostman (and knowledge of APIs and OAuth)\nAn OpenAI API Key from platform.openai.com\nMiddleware Information\nIf you follow the search concept files guide, the Microsoft Graph Search API returns references to files that fit the criteria, but not the file contents themselves. Therefore, middleware is required, rather than hitting the MSFT endpoints directly.\n\nSteps:\n\nloop through the returned files and download the files using the Download File endpoint or Convert File endpoint\n\nconvert that Binary stream to human readable text using pdf-parse\n\nThen, we can optimize further by summarizing using gpt-4o-mini in the function to help with the 100,000 character limit we impose on Actions today. \nAdditional Steps\nSet up Azure Function\nSet up an Azure Function using the steps in the Azure Function cookbook\nAdd in Function Code\nNow that you have an authenticated Azure Function, we can update the function to search SharePoint / O365\n\nGo to your test function and paste in the code from this file. Save the function.\nThis code is meant to be directional - while it should work out of the box, it is designed to be customized to your needs (see examples towards the end of this document).\n\nSet up the following env variables by going to the Configuration tab on the left under Settings. Note that this may be listed directly in Environment Variables depending on your Azure UI.\n\n    TENANT_ID: copied from previous section\n\n    CLIENT_ID: copied from previous section\n\n    OPENAI_API_KEY: spin up an OpenAI API key on platform.openai.com.\n\nGo to the Console tab under the Development Tools\n\n    Install the following packages in console\n\n       npm install @microsoft/microsoft-graph-client\n\n       npm install axios\n\n       npm install pdf-parse\n\n       npm install openai\n\nOnce this is complete, try calling the function (POST call) from Postman again, putting the below into body (using a query and search term you think will generate responses).\n\n        {\n        \"query\": \"\",\n        \"searchTerm\": \"\"\n    }\n\nIf you get a response, you are ready to set this up with a Custom GPT!\n\nDetailed Walkthrough\nThe below walks through setup instructions and walkthrough unique to this solution of pre-processing the files and extracting summaries in the Azure Function. You can find the entire code here.\n\nCode Walkthrough\n\nImplementing the Authentication \n\nBelow we have a few helper functions that we’ll use in the function.\n\nInitializing the Microsoft Graph Client\nCreate a function to initialize the Graph client with an access token. This will be used to search through Office 365 and SharePoint.\n\nconst { Client } = require('@microsoft/microsoft-graph-client');\n\nfunction initGraphClient(accessToken) {\n    return Client.init({\n        authProvider: (done) =\u003e {\n            done(null, accessToken);\n        }\n    });\n}\n\nObtaining an On-Behalf-Of (OBO) Token\nThis function uses an existing bearer token to request an OBO token from Microsoft's identity platform. This enables passing through the credentials to ensure the search only returns files the logged-in user can access.\n\nconst axios = require('axios');\nconst qs = require('querystring');\n\nasync function getOboToken(userAccessToken) {\n    const { TENANT_ID, CLIENT_ID, MICROSOFT_PROVIDER_AUTHENTICATION_SECRET } = process.env;\n    const params = {\n        client_id: CLIENT_ID,\n        client_secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET,\n        grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n        assertion: userAccessToken,\n        requested_token_use: 'on_behalf_of',\n        scope: 'https://graph.microsoft.com/.default'\n    };\n\n    const url = https\\://login.microsoftonline.com/${TENANT_ID}/oauth2/v2.0/token;\n    try {\n        const response = await axios.post(url, qs.stringify(params), {\n            headers: { 'Content-Type': 'application/x-www-form-urlencoded' }\n        });\n        return response.data.access\\_token;\n    } catch (error) {\n        console.error('Error obtaining OBO token:', error.response?.data || error.message);\n        throw error;\n    }\n}\n\nRetrieving Content from O365 / SharePoint Items\n\nThis function fetches the content of drive items, handling different file types and converting files to PDF when necessary for text extraction. This uses the download endpoint for PDFs and the convert endpoint for other supported file types.\nconst getDriveItemContent = async (client, driveId, itemId, name) =\u003e {\n    try {\n        const fileType = path.extname(name).toLowerCase();\n        // the below files types are the ones that are able to be converted to PDF to extract the text. See https://learn.microsoft.com/en-us/graph/api/driveitem-get-content-format?view=graph-rest-1.0\u0026tabs=http\n        const allowedFileTypes = ['.pdf', '.doc', '.docx', '.odp', '.ods', '.odt', '.pot', '.potm', '.potx', '.pps', '.ppsx', '.ppsxm', '.ppt', '.pptm', '.pptx', '.rtf'];\n        // filePath changes based on file type, adding ?format=pdf to convert non-pdf types to pdf for text extraction, so all files in allowedFileTypes above are converted to pdf\n        const filePath = /drives/${driveId}/items/${itemId}/content + ((fileType === '.pdf' || fileType === '.txt' || fileType === '.csv') ? '' : '?format=pdf');\n        if (allowedFileTypes.includes(fileType)) {\n            response = await client.api(filePath).getStream();\n            // The below takes the chunks in response and combines\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            // the below extracts the text from the PDF.\n            const pdfContents = await pdfParse(buffer);\n            return pdfContents.text;\n        } else if (fileType === '.txt') {\n            // If the type is txt, it does not need to create a stream and instead just grabs the content\n            response = await client.api(filePath).get();\n            return response;\n        }  else if (fileType === '.csv') {\n            response = await client.api(filePath).getStream();\n            let chunks = [];\n            for await (let chunk of response) {\n                chunks.push(chunk);\n            }\n            let buffer = Buffer.concat(chunks);\n            let dataString = buffer.toString('utf-8');\n            return dataString\n\n    } else {\n        return 'Unsupported File Type';\n    }\n\n    } catch (error) {\n        console.error('Error fetching drive content:', error);\n        throw new Error(Failed to fetch content for ${name}: ${error.message});\n    }\n};\n\nIntegrating GPT 4o mini for Text Analysis\n\nThis function utilizes the OpenAI SDK to analyze text extracted from documents and find relevant information based on a user query. This helps to ensure only relevant text to the user’s question is returned to the GPT. \n\nconst getRelevantParts = async (text, query) =\u003e {\n    try {\n        // We use your OpenAI key to initialize the OpenAI client\n        const openAIKey = process.env[\"OPENAI_API_KEY\"];\n        const openai = new OpenAI({\n            apiKey: openAIKey,\n        });\n        const response = await openai.chat.completions.create({\n            // Using gpt-4o-mini due to speed to prevent timeouts. You can tweak this prompt as needed\n            model: \"gpt-4o-mini\",\n            messages: [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant that finds relevant content in text based on a query. You only return the relevant sentences, and you return a maximum of 10 sentences\"},\n                {\"role\": \"user\", \"content\": Based on this question: \"${query}\", get the relevant parts from the following text:\\n\\n${text}. If you cannot answer the question based on the text, respond with 'No information provided'}\n            ],\n            // using temperature of 0 since we want to just extract the relevant content\n            temperature: 0,\n            // using max_tokens of 1000, but you can customize this based on the number of documents you are searching.\n            max_tokens: 1000\n        });\n        return response.choices[0].message.content;\n    } catch (error) {\n        console.error('Error with OpenAI:', error);\n        return 'Error processing text with OpenAI' + error;\n    }\n};\n\nCreating the Azure Function to Handle Requests\n\nNow that we have all these helper functions, the Azure Function will orchestrate the flow, by authenticating the user, performing the search, and iterating through the search results to extract the text and retrieve the relevant parts of the text to the GPT.\n\nHandling HTTP Requests: The function starts by extracting the query and searchTerm from the HTTP request. It checks if the Authorization header is present and extracts the bearer token.\n\nAuthentication: Using the bearer token, it obtains an OBO token from Microsoft's identity platform using getOboToken defined above.\n\nInitializing the Graph Client: With the OBO token, it initializes the Microsoft Graph client using initGraphClient defined above.\n\nDocument Search: It constructs a search query and sends it to the Microsoft Graph API to find documents based on the searchTerm.\n\nDocument Processing: For each document returned by the search:\n\nIt retrieves the document content using getDriveItemContent.\n\nIf the file type is supported, it analyzes the content using getRelevantParts, which sends the text to OpenAI's model for extracting relevant information based on the query.\n\nIt collects the analysis results and includes metadata like the document name and URL.\n\nResponse: The function sorts the results by relevance and sends them back in the HTTP response.\n\nmodule.exports = async function (context, req) {\n    const query = req.query.query || (req.body \u0026\u0026 req.body.query);\n    const searchTerm = req.query.searchTerm || (req.body \u0026\u0026 req.body.searchTerm);\n    if (!req.headers.authorization) {\n        context.res = {\n            status: 400,\n            body: 'Authorization header is missing'\n        };\n        return;\n    }\n    /// The below takes the token passed to the function, to use to get an OBO token.\n    const bearerToken = req.headers.authorization.split(' ')[1];\n    let accessToken;\n    try {\n        accessToken = await getOboToken(bearerToken);\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: Failed to obtain OBO token: ${error.message}\n        };\n        return;\n    }\n    // Initialize the Graph Client using the initGraphClient function defined above\n    let client = initGraphClient(accessToken);\n    // this is the search body to be used in the Microsft Graph Search API: https://learn.microsoft.com/en-us/graph/search-concept-files\n    const requestBody = {\n        requests: [\n            {\n                entityTypes: ['driveItem'],\n                query: {\n                    queryString: searchTerm\n                },\n                from: 0,\n                // the below is set to summarize the top 10 search results from the Graph API, but can configure based on your documents.\n                size: 10\n            }\n        ]\n    };\n\n    try {\n        // Function to tokenize content (e.g., based on words).\n        const tokenizeContent = (content) =\u003e {\n            return content.split(/\\s+/);\n        };\n\n        // Function to break tokens into 10k token windows for gpt-4o-mini\n        const breakIntoTokenWindows = (tokens) =\u003e {\n            const tokenWindows = []\n            const maxWindowTokens = 10000; // 10k tokens\n            let startIndex = 0;\n\n            while (startIndex  {\n            // This will go through and for each search response, grab the contents of the file and summarize with gpt-4o-mini\n            const results = [];\n\n            await Promise.all(list.value[0].hitsContainers.map(async (container) =\u003e {\n                for (const hit of container.hits) {\n                    if (hit.resource[\"@odata.type\"] === \"#microsoft.graph.driveItem\") {\n                        const { name, id } = hit.resource;\n                        // We use the below to grab the URL of the file to include in the response\n                        const webUrl = hit.resource.webUrl.replace(/\\s/g, \"%20\");\n                        // The Microsoft Graph API ranks the reponses, so we use this to order it\n                        const rank = hit.rank;\n                        // The below is where the file lives\n                        const driveId = hit.resource.parentReference.driveId;\n                        const contents = await getDriveItemContent(client, driveId, id, name);\n                        if (contents !== 'Unsupported File Type') {\n                            // Tokenize content using function defined previously\n                            const tokens = tokenizeContent(contents);\n\n                            // Break tokens into 10k token windows\n                            const tokenWindows = breakIntoTokenWindows(tokens);\n\n                            // Process each token window and combine results\n                            const relevantPartsPromises = tokenWindows.map(window =\u003e getRelevantParts(window.join(' '), query));\n                            const relevantParts = await Promise.all(relevantPartsPromises);\n                            const combinedResults = relevantParts.join('\\n'); // Combine results\n\n                            results.push({ name, webUrl, rank, contents: combinedResults });\n                        }\n                        else {\n                            results.push({ name, webUrl, rank, contents: 'Unsupported File Type' });\n                        }\n                    }\n                }\n            }));\n\n            return results;\n        };\n        let results;\n        if (list.value[0].hitsContainers[0].total == 0) {\n            // Return no results found to the API if the Microsoft Graph API returns no results\n            results = 'No results found';\n        } else {\n            // If the Microsoft Graph API does return results, then run processList to iterate through.\n            results = await processList();\n            results.sort((a, b) =\u003e a.rank - b.rank);\n        }\n        context.res = {\n            status: 200,\n            body: results\n        };\n    } catch (error) {\n        context.res = {\n            status: 500,\n            body: Error performing search or processing results: ${error.message},\n        };\n    }\n};\n\nCustomizations\n\nBelow are some potential areas to customize. \n\nYou can customize the GPT prompt to search again a certain amount of times if nothing is found.\n\nYou can customize the code to only search through specific SharePoint sites or O365 Drives by customizing the search query. This will help focus the search and improve the retrieval. The function as setup now looks through all files the logged-in user can access.\n\nYou could use gpt-4o instead of gpt-4o-mini. This would slightly increase the cost and latency, but you may get higher quality summarizations.\n\nYou can customize the amount of files it searches through within the call to Microsoft Graph.\n\nConsiderations\n\nNote that all the same limitations of Actions apply here, with regards to returning 100K characters or less and the 45 second timeout.\n\n\nThis only works for text, not for images. With some additional code in the Azure Function, you could customize this by using GPT-4o to extract summarizations of images.\n\nThis does not work for structured data. We recommend Solution 1 if structured data is a major part of your use case.\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nThe below spec passes in the query parameter to inform the pre-processing and a searchTerm to find the right files in Microsoft Graph.\nMake sure to switch the function app name, function name and code based on link copied in screenshot above\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nSee above and on the Azure Function cookbook for more detailed instructions on authentication.\nFAQ \u0026 Troubleshooting\nWhy are you using the Microsoft Graph API in your code instead of the SharePoint API?\n\n  The SharePoint API is legacy - per the Microsoft documentation here, “For SharePoint Online, innovation using a REST API against SharePoint is driven via the Microsoft Graph REST API's.” The Graph API gives us more flexibility, and the SharePoint API still runs into the same file issues listed in the Why is this necessary instead of interacting with the Microsoft Graph API directly? section.\n\nWhat types of files does this support?\n    This supports all files listed in the documentation for the Convert File endpoint here. Specifically, it supports pdf, doc, docx, odp, ods, odt, pot, potm, potx, pps, ppsx, ppsxm, ppt, pptm, pptx, rtf.\n\n    When a search result returns XLS, XLSX, or CSV, this prompts the user to download the file and re-upload to ask questions using Advanced Data Analysis. As stated above, we recommend solution 1 if structured data is part of your use case.\n\nWhy do I need to request an OBO token?\n\n  When you try to use the same token to authenticate to the Graph API as the one you use to authenticate into the Azure Function, you get an “invalid audience” token. This is because the audience for the token can only be user\\_impersonation.\n\n  To address this, the function requests a new token scoped to Files.Read.All within the app using the On Behalf Of flow. This will inherit the permissions of the logged in user, meaning this function will only search through files the logged-in user has access to. \n\n  We are purposefully requesting a new On Behalf Of token with each request, because Azure Function Apps are meant to be stateless. You could potentially integrate this with Azure Key Vault to store the secret and retrieve programmatically. \nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a1:T26a6,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building middleware to connect a GPT Action to a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to build an Azure Function, MSFT's cloud-based function builder. This documentation helps a user set up an OAuth-protected Azure Function to connect to a GPT Action, and to a sample application.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Azure Function. This can in a few ways:\n\n100k character limit in GPT Actions: users can use the middleware to pre-process the text response from an API. For example, you can use OpenAI’s API in the middleware to summarize the text before sending it back to ChatGPT.\nTypically for actions, users are relying on the SaaS API to return text. You can convert the response for the vendor API into easily digestible text, and it can handle different data types such as structured and unstructured data.\nIt can return files instead of just text. This can be useful to surface CSV files for Data Analysis, or bring back an PDF file and ChatGPT will treat it like an upload.\n\nExample Use Cases:\nA user needs to look up files in Sharepoint, but needs a middleware app between ChatGPT and Sharepoint\nA user has built several steps in a row in an Azure function, and needs to be able to kick off that process using ChatGPT\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://learn.microsoft.com/en-us/azure/azure-functions/\nApplication API Documentation: https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference/\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nAzure Portal with access to create Azure Function Apps and Azure Entra App Registrations\nApplication Setup\nInstalling the app\nYou can read more about languages and deployment options for Azure Functions on the left hand side of the documentation here. \nOption 1: Use VSCode\n\nSee Microsoft’s documentation here for how to deploy using VSCode. If you have familiarity with this approach, feel free to use it. \n\nOption 2: Directly in Azure Portal\n\nSee the documentation here for how to deploy using the Azure portal. We’ll walk through an example here step by step.\n\nPart 1: Create Function\n\nCreate an Azure Function app. I used the following settings but you can use anything you are comfortable with. Note that not every language / operating system allows for editing the functions in the console directly - the combination I chose below does. For my walkthrough, I left everything as default and made the selections below. The below settings work out of the box for the SharePoint Node.js solutions here and here.\n\n   Basics\n\n      Do you want to deploy code or container image?:  Code\n\n      Runtime stack: Node.js\n\n      Operating system: Windows\n\n   Networking\n\n      Enable public access: on (need this on to connect to the GPT)\n\nAfter completing the above, you’ll land on the “Deployments” page. Once the deployment completes (which should only take a few minutes) click on “Go to Resource” to go back to the Function App\nYou may get an error the first time you attempt this, click create again and it will likely work.\nPart 2: Set up Auth\nOn the left-hand side menu of the Azure Function App, click on Authentication under the Settings menu. \n\n   Add identity provider\n\n   Select Microsoft as identity provider. \n\n   Workforce as tenant type\n\n   Create a new application. The instructions are fairly similar if you are using an existing application, but it is easier to create a new application as it will have the callback URLs and the API exposed automatically using “Easy Auth”. You can read more about that here.\n\n   Leave all the other settings on this page as the default, but feel free to change based on your internal guidelines.\n\n   On the permissions tab, click Add Permission and add Files.Read.All and Sites.ReadAll, then Add. This allows this application to read files which is important in order to use the Microsoft Graph Search API. If you are not using this for the SharePoint solution here and here you can skip this.\n\nOnce it is created, click on the enterprise application you just created (so, leave the Function App page and land on the Enterprise Application that you just spun up). We are now going to give it one more permission, to execute the Azure Function by impersonating the user logging into the application. See here for more details.\n\n   On the main page, click “View API Permissions”\n\n   Search for Microsoft Azure App Service in the APIs my organization uses and find user\\_impersonation \n\n   Add it, then you’ll need an Admin on Azure Portal to Grant Admin Consent.\n\n5) Within that enterprise application, Click on “Expose an API” on the left hand menu under Manage, then copy the scope that was created using the Copy to Clipboard button. The scope should look like “api://\\/user\\_impersonation”. Save this for later as SCOPE.\n\n6) Click on “Authentication” on the left hand menu under Manage\n\n   Under the Web section, you’ll notice one callback URI was added automatically. Add the Postman redirect URI () for testing.\n\n7) On the left-hand side, go to Overview. Copy the application (client) ID and and the directory (tenant) ID and save for later as CLIENT_ID and TENANT_ID.\nPart 3: Set up Test Function\nLeave the page by going home and then back to your Function App.\n\nClick on Create Function. For this example, I’m going to develop it in the portal, but you can also use VSCode or another IDE.\n\n   Choose HTTP trigger\n\n   For Authorization Level, you can choose any key type you want.\n\n      Note this may error out the first time, but it is likely the Function did create, do a refresh of the page to check.\n\nClick on the function you just created (You may need to click refresh to see it). Click on Get Function URL and save it to test in Postman. You will also use this when creating the OpenAPI spec later when you put it into the GPT. \n\nGo back to the function app and click on Configuration. Show the value for the MICROSOFT_PROVIDER_AUTHENTICATION_SECRET variable, copy it (click advanced edit to copy it), and save it for later.  \n\nAt this point, you should have a test function created, and you should have saved a client id, tenant id, secret, scope, and function URL. You are now ready to test out the authentication in Postman\nPart 4: Test Authentication in Postman\nTry to hit endpoint you created in Postman using those OAuth settings:\n\n    Grant Type: Authorization Code\n\n    Auth URL: https://login.microsoftonline.com/TENANT_ID/oauth2/v2.0/authorize\n\n    Auth Token URL: https://login.microsoftonline.com/TENANT_ID/oauth2/v2.0/token\n\n    Client ID: CLIENT_ID from step 7 above\n\n    Client secret: MICROSOFT_PROVIDER_AUTHENTICATION_SECRET from step 11 above\n\n    Scope: SCOPE from step 5 above\n\n    Client credentials: Send client credentials in body\n\nYou will need to click Get New Access Token, and then hit the endpoint you saved in step 10 above. If it was successful, you should get this response: ”This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.”\nPart 5: Set up your Application on an Azure Function\nThis should be done separately and is specific to your app. See the Sharepoint Cookbook) for an example of that.\nPart 6: Set up ChatGPT\nGenerate an OpenAPI spec for your endpoint. \n\nPaste that into the Actions section of a GPT, and choose OAuth as the authentication type. Fill out the OAuth settings the same way you did for Postman above. \n\nOnce you save the action, you will see a callback URI at the bottom of the GPT configuration. Copy that URL, then go back to your Function App in the Azure Portal.\n\nClick on Authentication under Settings, then click on your Entra application.\n\nOnce you are there, then click Authentication under the Manage section.\n\nAdd a new Redirect URI under the Web section of that page, and paste in the Callback URI you got from step 16, then click Save. \n\nTest out the GPT and it should work as expected.\nChatGPT Steps\nCustom GPT Instructions\nThis is application specific. See Sharepoint Cookbook) for an example\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nBelow is an example of what connecting to this Middlware might look like. You'll need to insert your application's \u0026 function's information in this section.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\nFollow steps 2 \u0026 4 above to setting up authentication\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID*: *see step 12 above\nClient Secret*: *ditto\nAuthorization URL*: *ditto\nToken URL*: *ditto\nScope*: *ditto\nToken*: *ditto\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nSee above for testing out this application\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a2:T13c5,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Canvas, a widely used LMS tool for course material, grading and general education purposes.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Canvas\n\nExample Use Cases:\nQuery content from LMS\nBuild quizzes\nCreate new modules\nUnderstand topics from assigned courses\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://canvas.instructure.com/login/canvas\nApplication API Documentation: https://canvas.instructure.com/doc/api/\n\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nEnsure you have the access and permissions in Canvas to create an approved integration with an Access Token for testing\nFor OAuth connection see here : https://canvas.instructure.com/doc/api/file.oauth.html\n\n\ngptactions_canvaslms_approvedintegrations.png\n\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nNote : this schema above does not contain all possible API endpoints, be sure to edit the schema to produce the appropriate actions from canvas API documentation\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application. Our sample integration below is utilizing a secret key, which is an easier method for testing. In production, it is more common to establish an OAuth connection to ensure permissions are respected from ChatGPT to Canvas, ensuring users the appropriate access.\nCanvas accounts : The first step is to ensure the canvas environment has users assigned to the appropriate role (student / teacher) with any access to appropriate courses\nFor testing purposes or service level account access create an approved integration access token as seen below\nUnder Account -\u003e Settings -\u003e Approved Integrations\n\n gptactions_canvaslms_settings.png\n\nCreate new Access Token\n\n gptactions_canvaslms_integrationslist.png\n\nProvide purpose and expiration\n\n gptactions_canvaslms_tokencreate.png\n\nCopy the new Token to use in the GPT Action (the token will not be accessible once you leave the page - ensure proper token management access is maintained to prevent unauthorzied access from another application.\n\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"API Key\". Enter in the information below. API Key provides general access to the Canvas environment based on the users permissions that created the API key, to follow permission models in Canvas use the OAuth method described below.\nAPI Key**: Use the token from above\nAuth Type**: Bearer\n\n gptactions_canvaslms_auth.png\nOAuth Connection\n\nIf you want to implement OAuth, follow the guide here : https://canvas.instructure.com/doc/api/file.oauth.html - specific instructions can vary depending on the Canvas implementation.\n\nParameters for GPT Configuration\nClient ID - obtained from site owner\nClient Secret - obtained from site owner\nAuthorization URL - : https:///login/oauth2/auth?client_id=XXX\u0026response_type=code\u0026redirect_uri=https://example.com/oauth_complete\u0026state=YYY\u0026scope=%20%20\nToken URL : https:// /login/oauth2/token\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to test the Action.\nYou will see a list of available actions from the API schema for Canvas, you can test the endpoints from the GPT builder to ensure Canvas can be reached. Once you verified connection make sure to save the GPT and share it witht the appropriate user base.\nNote: if you are using OAuth, you will need to set up the callback URL from ChatGPT in the Canvas app\nFAQ \u0026 Troubleshooting\nSchema calls the wrong project or dataset: If ChatGPT calls the wrong project or dataset, consider updating your instructions to make it more explicit either (a) which project / dataset should be called or (b) to require the user provide those exact details before it runs the query\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a3:T14b9,"])</script><script>self.__next_f.push([1,"This page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Salesforce, specifically, Salesforce Service Cloud. This schema detailed in this Action allows the user to pull case data and update cases directly from ChatGPT. The setup process to create Actions for other Salesforce Cloud solutions uses the same Connected App and authentication setup, but will require a different API schema.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Salesforce\n\nExample Use Cases:\nReduce average response time to customers\nReduce time to troubleshoot cases or issues\nEnsure more consistent brand voice in reponse to customers when combined with knowledge and instructions in the GPT\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nCreate Lightning Apps in Salesforce\nOAuth Tokens and Scopes\nSalesforce API Docs\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nEnsure you have permissions to create an App in Salesforce\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nContext: Your purpose is to pull information from Service Cloud, and push updates to cases. A user is going to ask you a question and ask you to make updates.\n\nInstructions:\nWhen a user asks you to help them solve a case in Service Cloud, ask for the case number and pull the details for the case into the conversation using the getCaseDetailsFromNumber action.\nIf the user asks you to update the case details, use the action updateCaseStatus.\n\nExample:\nUser: Help me solve case 00001104 in Service Cloud.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\nNavigate to Salesforce Setup\ngptactions_salesforce.png\nSearch for “App Manager”\n\ngptactions_salesforce.png\nClick “New Connected App”\nEnter a Connected App Name\nEnter contact email (your email)\nCheck the box to enable OAuth settings\nInsert a callback URL (use a placeholder like https://chat.openai.com/aip//oauth/callback for now, you’ll update this later when you create the Action in ChatGPT)\n\ngptactions_salesforce.png\nSelect “Selected OAuth Scopes” and grant the appropriate permissions. Scope these based on your internal security policies.\n\ngptactions_salesforce.png\nEnsure the following boxes are checked:\nEnable Client Credentials Flow\nEnable Authorization Code and Credentials FLow\nEnable Token Exchange Flow\nEnsure the following box is unchecked:\nRequire Proof Key for Code Exchange (PKCE) Extension for Supported Authorization Flows\n\ngptactions_salesforce.png\nSave your New Connected App\nUnder “Consumer Key and Secret” click “Manage Consumer Details”. Verify your access using the code emailed to your account, and then copy the key and secret.\nSalesforce Consumer Key = ChatGPT Client ID\nSalesforce Consumer Secret = ChatGPT Client Secret\n\ngptactions_salesforce.png\nReturn to App page\nClick “Manage”\nClick “Edit Policies”\nUnder OAuth Policies, check the “Enable Token Exchange Flow” box\n\ngptactions_salesforce.png\nClick save!\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID**: use Client ID from steps above\nClient Secret**: use Client Secret from steps above\nAuthorization URL**: https://[inserturlhere].my.salesforce.com/services/oauth2/authorize\nToken URL**: https://[inserturlhere].my.salesforce.com/services/oauth2/token\nScope**: full\nToken**: Default (POST)\nPost-Action Steps\n\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\nNavigate back to your Connected App in Salesforce, and add your callback URL.\n\nFAQ \u0026 Troubleshooting\nCallback URL Error:  If you get a callback URL error in ChatGPT, pay close attention to the screenshot above. You need to add the callback URL directly into Salesforce for the action to authenticate correctly\nInternal Server Error: Ensure all the correct boxes are checked and/or unchecked in the OAuth settings for your connected app.\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a4:Tfe4,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis GPT Action provides an overview of how to connect to Google Gmail, Google’s Private \u0026 Secure Email for Personal or Business. This Action is connected to the Google Gmail APIs that can read, send, list, and draft emails in the authorized account.\nValue + Example Business Use Cases\nValue: The Gmail GPT will serve as a powerful tool to streamline communication processes, improve customer engagement, and optimize resource allocation.\n\nExample Use Cases:\nManage internal communications by summarizing lengthy emails and drafting responses based on previous email threads.\nSupport agents can provide customers with instant responses adhering to a company’s communication guidelines, tone, and style.\nReference other GPTs , such as a data analsys GPT, and then ask for a draft/send of the consolidated analysis through email communication.\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://mail.google.com/mail/u/0/#inbox\nApplication API Documentation: https://developers.google.com/gmail/api/guides\nApplication Prerequisites\nBefore you get started, make sure you’ve a Google Cloud account and that the Gmail API is enabled:\nSet up a Google Cloud project\nEnable Gmail API from Google API Library\nIf application’s  “Publishing Status” is “Testing”, ensure users are added to your application\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\n\nGo to the Google Cloud Console\nNavigate to API \u0026 Services \u003e Credentials\ngptactions_BigQuery_auth.png\ngptactions_BigQuery_auth.png\nCreate new OAuth credentials (or use an existing one)\n\ngptactions_BigQuery_auth.png\n\nLocate your OAuth Client ID \u0026 Client Secret and store both values securely (see screenshot below)\n\ngptactions_BigQuery_auth.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID**: use Client ID from steps above\nClient Secret**: use Client Secret from steps above\nAuthorization URL**: https://accounts.google.com/o/oauth2/auth\nToken URL**: https://oauth2.googleapis.com/token\nScope**: https://mail.google.com/\nToken**: Default (POST)\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\nIn the “Authorized redirect URIs” (see screenshot above), add your callback URL\n\nFAQ \u0026 Troubleshooting\nCallback URL Error: If you get a callback URL error in ChatGPT, pay close attention to the screenshot above. You need to add the callback URL directly into GCP for the action to authenticate correctly\nSchema calls the wrong project or dataset: If ChatGPT calls the wrong project or dataset, consider updating your instructions to make it more explicit either (a) which project / dataset should be called or (b) to require the user provide those exact details before it runs the query\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a5:T145e,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Jira, Atlassian's tool for project and ticket management. This action assumes a user’s context and allows them to read and write to issues in a given project.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Jira Cloud\n\nExample Use Cases:\nA user can load up recent issues for a particular project and use ChatGPT to provide solutions\nA user can create and alter issues and sub-tasks and assign to specific users by instructing ChatGPT\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://.atlassian.net/jira\nApplication API Documentation: https://developer.atlassian.com/cloud/jira/platform/rest/v3/intro/\nApplication OAuth 2.0 Documentation: https://developer.atlassian.com/cloud/jira/platform/oauth-2-3lo-apps/\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nEnsure you have the access and permissions to create an application in the Atlassian Cloud Developer Console\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nNOTE: Replace the placeholder  in url with your cloud environment's unique ID. You can find this value by visiting https://.atlassian.net/_edge/tenant_info\n\nAuthentication Instructions\nBelow are instructions on setting up authentication with Jira. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nJira Steps\nCreate an Application: The first step is to create a new application in Jira for the integration with ChatGPT. This can be done by visiting the Atlassian Developer Console, Clicking Create and selecting OAuth 2.0 Integration.\ngptactions_jira_devconsole.png\nFrom here, simply enter the name of your integration and click Create.\ngptactions_jira_newapplication.png\nDefine Permissions: Next we need to provide the required permissions to our application. Within the new application, open the Permissions menu from the sidebar, locate Jira API and click Add and then Configure.\ngptactions_jira_permissions.png\nRequired permissions will vary depending on the intended functionality of the GPT. In this scenario we wish to read and write to Jira issues, so select the following scopes under Jira platform REST API by clicking Edit Scopes:\n\nread:jira-work\nwrite:jira-work\nread:jira-user\n\nOnce selected, click Save\ngptactions_jira_scopes.png\nConfigure Placeholder Callback URL: In order to complete the following step and obtain a Client ID and Secret for enabling secure authentication between ChatGPT and Jira, we first need to add a placeholder callback URL. We can achieve this by clicking on Authorization in the sidebar, and Configure next to OAuth 2.0 (3LO). From here simply enter a placeholder URL and click Save Changes.\n\ngptactions_jira_placeholder.png\nApplication Client ID/Secret: The next step is to locate the Client ID and Secret for enabling secure authentication between ChatGPT and Jira. We can find these values by clicking on Settings in the sidebar and scrolling down to Authentication Details.\n\n      Keep this page open as we will require these values in the next stage of configuration!\ngptactions_jira_clientsecret.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID: The **Client ID from Step 3 of Jira Configuration\nClient Secret: The **Secret from Step 3 of Jira Configuration\nAuthorization URL**: https://auth.atlassian.com/authorize\nToken URL**: https://auth.atlassian.com/oauth/token\nScope**: read:jira-work write:jira-work read:jira-user\nToken Exchange Method**: Default (POST Request)\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\ngptactions_jira_redirect.png\nIn your application in the Atlassian Developer Console, navigate to the Authorization sidebar tab, next to OAuth 2.0 (3L0) click Configure, and add your callback URL under Callback URL\n\ngptactions_jira_callback.png\nFAQ \u0026 Troubleshooting\nCallback URL Error**: If you get a callback URL error in ChatGPT, double check the Callback URL value as it can occasionally change depending on any alterations made to the authentication\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a6:Tebc,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Notion. This Action takes a user’s question, scans the relevant Notion pages using Notions search functionality, and then returns information on the matching pages.\nValue + Example Business Use Cases\nValue: Users can now harness ChatGPT’s natural language capabilities to directly connect to, query, and synthesize their knowledge within Notion. Administrators can explicitly share pages with the integration to manage access.\n\nExample Use Cases:\nA new employee seeks quick how-to information on setting up a new system\nA support agent needs to quickly retrieve information from Notion without reading the entire document\nUsers want to synthesize information and create summaries or transformations for use in other aspects of their work\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://www.notion.so/\nApplication API Documentation: https://developers.notion.com/reference/intro\nNotion Authorization Approach: https://developers.notion.com/docs/authorization\n    NOTE:  Notion only allows OAuth with \"Public Integrations.\"  Refer to the linked documentation to determine what is best suited for your needs\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nSet up a notion workspace with populated pages\nSharing pages through notion works best with specific Wikis.  Consider organizing your knowledge base into a wiki or set of wikis\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the Notion.\nGo to the Notion Settings Page for your workspace\nNavigate to My Connections \u003e Develop or Manage Integrations\nCreate new Integration marked as Internal\nLocate your integration and find the API Key labeled: Internal Integration Secret.  This is the bearer token for this integration.\n\nNOTE! You need to share specific pages, databases, or wikis with the integration in order to access them in ChatGPT.  Do this by selecting the ... button on the upper right of a page and select the appropriate connection.\n\nNOTE! Notion allows integrations to leverage OAuth if they are marked as \"Public.\"  Review Notion's Auth Documentation to determine what integration path is best for your needs.\nnotion_connections.png\nsharing_notion_pages.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"API Key\". Enter in the information below.\nAPI Key**: Use Internal Integration Secret from steps above\nAuth Type**: Bearer\nFAQ \u0026 Troubleshooting\nSearch returns nothing If you don't see any pages returned when running a search, double check that you've shared relevant pages with the application from Notion\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a7:T149c,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to Confluence, Atlassian's collaboration and documentation platform. This Action takes a user’s question, scans the relevant Confluence spaces and pages to gather the necessary information, then formulates a response to answer the user’s question. This cookbook does not address updating content in Confluence directly from ChatGPT, but it is technically feasible to accomplish with additional Actions and scopes.\nValue + Example Business Use Cases\nValue\n\nUsers can now leverage ChatGPT's natural language capability to connect directly to Confluence, enabling seamless interaction with their organization's knowledge base.\n\nExample Use Cases\nKnowledge Workers**: Easily retrieve information from Confluence pages and spaces to answer questions or gather details for reports and presentations.\nProject Managers**: Quickly access project documentation and updates stored in Confluence without manually searching through pages.\nCustomer Support Teams**: Provide accurate and timely responses to customer inquiries by pulling relevant information from the Confluence knowledge base.\nAll Users**: Gain more visibility into company-wide documentation, policies, and procedures, enhancing collaboration and knowledge sharing.\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://developer.atlassian.com/console/myapps/\nApplication API Documentation: https://developer.atlassian.com/cloud/confluence/rest/v2/intro/#about\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\nEnsure you have permissions to create an App in the Atlassian Developer Portal\nDetermine what interactions you would like your GPT to take (search, read, edit, etc.)\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, make sure you go through the following steps within the Atlassian Developer portal to create your Confluence app:\nSelect the Create drop-down\nChoose OAuth 2.0 integration\nGive a name, agree to terms, and click Create\nSelect \"Distribution\" on the left-hand menu and click “Edit”\nChange radio button to \"Sharing\"\nFill out required fields and Save Changes\nSelect \"Permissions\" on the left-hand menu\nAdd in the scopes you would like to include (e.g., User identity API and Confluence API so that the app can know what a user has access to and fetch from Confluence)\nSelect \"Authorization\" on the left-hand menu\nClick \"Add\" under Action in the row for OAuth 2.0\nEnter the callback URL from your GPT (note: you may need to add a placeholder for now and revisit this once you have created the Action and OAuth in your GPT so that you have the final callback URL)\nSelect \"Settings\" under the left-hand menu\nCopy your Client ID and Secret for us in OAuth setup in GPT\n\nconfluence_gpt.png\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\nClient ID**: use Client ID from steps above\nClient Secret**: use Client Secret from steps above\nAuthorization URL**: https://auth.atlassian.com/authorize\nToken URL**: https://auth.atlassian.com/oauth/token\nScope**: read:confluence-content.all search:confluence\nToken**: Default (POST)\nPost-Action Steps\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\nCopy the callback URL from the GPT Action\nIn the “Authorized redirect URIs” (see screenshot above), add your callback URL\n\nFAQ \u0026 Troubleshooting\nCallback URL Error: If you get a callback URL error in ChatGPT, pay close attention to the screenshot above. You need to add the callback URL directly into your Confluence app for the action to authenticate correctly\nSchema calls the wrong project or dataset: If ChatGPT calls the wrong project or dataset, consider updating your instructions to make it more explicit either (a) which project / dataset should be called or (b) to require the user provide those exact details before it runs the query\nLooping Actions: You may not have given the necessary scopes/permissions to your app to accomplish its intended purpose\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a8:T2491,"])</script><script>self.__next_f.push([1,"\nIntroduction\n\nThis is a guide for developers seeking to give ChatGPT the ability to query a SQL database using a GPT Action. Before reading this guide, please familiarize yourself with the following content:\n\n\n\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\n\nThis guide outlines the workflow required to connect ChatGPT to a SQL Database via a middleware application. We’ll use a PostgreSQL database for this example, but the process should be similar for all SQL databases (MySQL, MS SQL Server, Amazon Aurora, SQL Server on Google Cloud, etc.). This documentation outlines the steps required to create GPT Action which can:\n\n\n\nExecute read queries against a SQL Database\nReturn records via a text response\nReturn records via a CSV file\n\nValue + Example Business Use Cases\n\nValue: Users can now leverage ChatGPT's natural language capability to answer questions about data in a SQL database:\n\n\n\nBusiness users can access information contained in a SQL database without writing SQL or submitting a request to an analyst\nData analysts can perform complex analysis beyond what is possible with a SQL query by extracting data and analyzing it with ChatGPT\n\nExample Use Cases:\n\n\n\nA business user needs to answer questions about their sales funnel\nA data analyst needs to perform a regression analysis on a large dataset\n\nApplication Design Considerations\n\nGiven that most managed SQL databases do not provide REST APIs for submitting queries, you will need a middleware application to perform the following functions:\n\n\n\nAccept database queries via REST API requests\nForward queries to the integrated SQL database\nConvert database responses in to CSV files\nReturn CSV files to the requestor\n\nThere are two main approaches to designing the first function:\n\n\n\nThe middleware supports a single method for receiving arbitrary SQL queries generated by the GPT and forwards them to the database. The benefits of this approach include:\n    Ease of development\n    Flexibility (doesn’t require you to anticipate the types of queries users will make)\n    Low maintenance (doesn’t require you to update the API schema in response to database changes)\nThe middleware supports a number of methods corresponding to specific allowed queries. The benefits of this approach include:\n    More control\n    Less opportunity for model error when generating SQL\n\nThis guide will focus on option 1. For those interested in option 2, consider implementing a service like PostgREST or Hasura to streamline the process.\n\nAn application architecture diagram depicting the interaction between the user, GPT, middleware, and database\nApplication architecture diagram\n\nMiddleware Considerations\n\nDevelopers can either build custom middleware (commonly deployed as serverless functions with CSPs like AWS, GCP, or MS Azure) or use third-party solutions (like Mulesoft Anypoint or Retool Workflows). Using third-party middleware can accelerate your development process, but is less flexible than building it yourself.\n\nBuilding your own middleware gives you more control over the application’s behavior. For an example of custom middleware, see our Azure Functions cookbook.\n\nRather than focusing on the specifics of middleware setup, this guide will focus on the middleware’s interface with the GPT and SQL database.\n\nWorkflow Steps\n\n1) GPT generates a SQL query\n\nGPTs are very good at writing SQL queries based on a user’s natural language prompt. You can improve the GPT’s query generation capabilities by giving it access to the database schema in one of the following ways:\n\n\n\nInstruct the GPT to start by querying the database to retrieve the schema (this approach is demonstrated in more detail in our BigQuery cookbook).\nProvide the schema in the GPT instructions (works best for small, static schemata)\n\nHere are sample GPT instructions which include information about a simple database schema:\n\n2) GPT sends SQL query to middleware\n\nIn order for our GPT to communicate with our middleware, we’ll configure a GPT Action. The middleware needs to present a REST API endpoint which accepts a SQL query string. You can design this interface in several ways. Here is an example of an OpenAPI schema for a simple endpoint which accepts a “q” parameter in a POST operation:\nA note on authentication: The API interface in the above example accepts a single system-level API key which is stored along with the GPT’s configuration and used to authenticate requests for all GPT users. GPT Actions also support OAuth authentication, which enables user-level authentication and authorization. Learn more about GPT Action authentication options.\n\nBecause the user is authenticating with middleware and not directly with the underlying database, enforcing user-level access (table or row-level permissions) requires more effort. However, it may be required for GPTs where users have different levels of access to the underlying database.\n\n In order to enforce user-level permissions, your middleware should:\n\n\n\nReceive the user’s metadata provided by the IdP during the OAuth flow and extract their identifying information\nQuery the database to retrieve the user’s database permissions\nIssue a command to the database to enforce the relevant permissions for the remainder of the session\n\nIn order to maintain a good user experience, you’ll want to dynamically retrieve the available database schema for each user as opposed to including the schema data in the GPT instructions directly. This ensures that the GPT only has access to tables which it can query on behalf of the current user.\n\n3) Middleware forwards SQL query to database\n\nYour middleware will implement a database driver or client library to enable it to query the PostgreSQL database directly. If you are using third-party middleware, the middleware vendor should provide native connectors for SQL databases. If you are building your own middleware, you may need to implement a client library provided by the database vendor or a third-party. For example, here is a list of community-maintained client libraries for PostgreSQL: https://wiki.postgresql.org/wiki/List_of_drivers\n\nDuring this workflow step, the middleware application needs to extract the SQL string from the request it received from the GPT and forward it to the database using the methods provided by the client library.\n\nA note on read-only permissions: Given that this design pattern results in your database processing arbitrary AI-generated SQL queries, you should ensure that the middleware application has read-only permissions on the database. This ensures that the AI-generated queries cannot insert new data or modify existing data. If write access is required for your use-case, consider deploying operation-specific endpoints rather than accepting arbitrary SQL.\n\n4) Database returns records to middleware\n\nDepending on the client library you have implemented, your middleware may receive records in a variety of formats. One common pattern is for your middleware to receive an array of JSON objects, each object representing a database record matching the query:\n\n5) Middleware converts records into base64-encoded CSV file\n\nIn order for ChatGPT to analyze large numbers of records, it needs access to data in a CSV format. The GPT Actions interface allows GPTs to receive base64-encoded files of up to 10mb in size.\n\nYour middleware needs to perform two actions:\n\nConvert records into a CSV format\n\nMany programming languages include a native library for working with CSV files (the Python csv library, for example).\n\nHere’s an example of how your middleware could convert an array of JSON objects into a CSV file:\nBase64-encode the CSV file\n\nMany programming languages include a native library for working with base64 encodings (the Python base64 library, for example).\n\nHere’s an example of how your middleware could base64-encode the CSV file generated in the previous step:\n6) Middleware returns base64-encoded CSV file to GPT\n\nIn order for the GPT Actions interface to process the base-64 encoded CSV file, the response returned by your middleware must contain an openaiFileResponse parameter. The value provided must be an array of file objects or links to files (see the Actions documentation for more details). For the purposes of this example, we will work with an array of file objects.\n\nHere is an example of what a valid response body looks like:\n7) GPT processes returned file\n\nOnce your GPT receives the base64-encoded CSV file, it will automatically decode the file and process it to answer the user’s question. This may involve using code interpreter to perform additional analysis against the CSV file, which happens the same way as if a user had uploaded the CSV file via the prompt.\n\nNote: You must enable the Code Interpreter \u0026 Data Analysis capability in your GPT if you want to be able to perform additional analysis on the returned file.\n\nConclusion\n\nGPT Actions provide a flexible framework for retrieving data from external sources like SQL databases. Giving ChatGPT the ability to query a database can substantially expand its capabilities as a knowledge assistant and analyst.\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"a9:T1948,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis guide provides details on how to connect chatGPT with a Box.com account, the GPT requires two actions to pull data from Box. The GPT will interact with the Box API directly but requires middleware (ie Azure function) to properly format the response from Box to download and read the file contents. The azure function action is transparent to the end user, meaning the user will not need to explicity call the action.\n\nAction 1 : Box API Action - Leverages the Box API to query data from Box\nAction 2 : Azure function - Formats response from Box enabling chatGPT to download the file directly from Box\n\nValue + Example Business Use Cases\n\nExisting Box customers can leverage these guidelines to query details about files, contents of files and any metadata related. This enables a OpenAI powered analysis of any content stored in Box such as visualizing data sets and creating summaries across multiple folders and files. This GPT can access folders, files and business process data such as metadata in Box. Additionally Box admins can use this GPT action for visibility into audit trails and health checks.\nApplication Information\nApplication Key Links\nCheck out these links from Box and Azure before you get started:\n\nBox Action\nApplication Website: https://app.box.com\nApplication API Documentation: https://developer.box.com/reference/\n\n\n\nAzure Function\nApplication Website: https://learn.microsoft.com/en-us/azure/azure-functions/\nApplication API Documentation: https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference/\n\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your Box environment:\nThis requires a Box developer account to get started : https://developer.box.com/\nFollow the Box Developer site to create a custom app with OAuth 2.0 authentication type  : https://developer.box.com/guides/getting-started/first-application/\nNavigate to Configuration tab for the following values\n    OAuth 2.0 Credentials (Client ID / Client Secret) You will need both of these values for the chatGPT configuration\n    OAuth 2.0 Redirect URIs : You will fill this value in from chatGPT action configuration below\n    Application scopes (Read all files and folders in Box, Manage Enterprise properties)\n\nYou will want to keep this window open, the Redirct URIs needs to be filled in from the gpt configuration.\n\n\n\ngpt_actions_box_boxconfig1.png.png\n\nMiddleware information : required for Action 2\n\nMake sure you go through the following steps in your Azure environment:\nAzure Portal with access to create Azure Function Apps and Azure Entra App Registrations\nThere is a detailed section in this guide related to deploying and designing the function required to wrap the response from Box in order to view the contents of the file. Without the function the GPT will only be able to query data about the file and not the contents. Be sure to read this section after creating the first action.\n\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAction 1 : Box API Action\nOnce you've created a Custom GPT, you will need to create 2 actions. Copy the text below in the 1st Actions panel, this will be for the Box action. Have questions? Check out Getting Started Example to see how this step works in more detail.\nNote : this schema above does not contain all possible API endpoints, be sure to edit the schema to produce the appropriate actions from Box Developer documentation\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose OAuth\n\n gptactions_box_gptauth.png\nOAuth Connection\n\nClient ID - value from Box custom app you created earlier\nClient Secret - value from Box custom app you created earlier\nAuthorization URL - : https://account.box.com/api/oauth2/authorize?response_type=code\u0026client_id=[client ID from above]\u0026redirect_uri=[use a placeholder like chat.openai.com/aip//oauth/callback for now, you’ll update this later when you create the Action in ChatGPT]\nToken URL : https:api.box.com/oauth2/token\n\n\nYou need to save the configuration and navigate back to the gpt Configuration tab to copy the Callback URL, edit the configuration for the Box action Authorization URL and format the URL as https://account.box.com/api/oauth2/authorize?response_type=code\u0026client_id=[client_ID]\u0026redirect_uri=[callBack URL]\nPost-Action Steps\nUpdate the Box.com custom application\n\nCopy the CallBack URL from the gpt and add a OAuth 2.0 Redirect URIs in Box.com\n\n\ngpt_actions_box_boxconfig1.png.png\n\nAction 2 : Azure Function\nNow that we have the GPT created and authenticating against Box.com, we can create the azure function to handle the response formatting enabling the GPT to download the files from Box.\n\nFollow this Azure Cookbook Guide for further details deploying an Azure function. Below you will find sample code to add to the function.\n\nThis code is meant to be directional - while it should work out of the box, it is designed to be customized to your need.\n\n\n\nData flow\n\ngpt_actions_box_azureflow.png\n\n\n\n\nNow that you have the azure function created, add the sample code below:\nfunction_app.py\njwt_config.json.sample\nrequirements.txt\nMake sure to follow the rest of the Azure guide for post authentication steps and chatGPT configuration : Azure Cookbook Guide\nFAQ \u0026 Troubleshooting\nSchema calls the wrong project or dataset: If ChatGPT calls the wrong project or dataset, consider updating your instructions to make it more explicit either (a) which project / dataset should be called or (b) to require the user provide those exact details before it runs the query\nBox can return a large set of data in the event stream which can cause errors,\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"aa:T223c,"])</script><script>self.__next_f.push([1,"\nThis notebook provides step-by-step instructions on using Google Cloud BigQuery as a database with vector search capabilities, with OpenAI embeddings, then creating a Google Cloud Function on top to plug into a Custom GPT in ChatGPT.\n\nThis can be a solution for customers looking to set up RAG infrastructure contained within Google Cloud Platform (GCP), and exposing it as an endpoint to integrate that with other platforms such as ChatGPT.\n\nGoogle Cloud BigQuery is a fully-managed, serverless data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It allows developers to store and analyze massive datasets with ease.\n\nGoogle Cloud Functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events without managing servers or runtime environments.\n\nPre-requisites:\n\nTo run this cookbook, you must have:\nA GCP project you have access to\nGCP user with permission to create a BigQuery dataset and Google Cloud Function\nGCP CLI installed and connected\nOpenAI API key\nChatGPT Plus, Teams or Enterprise subscription\n\nArchitecture\n\nBelow is a diagram of the architecture of this solution, which we'll walk through step-by-step:\n\nbigquery-rag-architecture.png\n\nTable of Contents\n\nSetup of Environment Setup environment by installing and importing the required libraries and configuring our GCP settings. Includes:\n    Install and Import Required Libraries\n    Configure GCP project\n    Configure OpenAI Settings\n\n\nPrepare Data Prepare the data for uploading by embedding the documents, as well as capturing additional metadata. We will use a subset of OpenAI's docs as example data for this.\n\nCreate BigQuery Table with Vector search\nCreate a BigQuery table and upload the data we've prepared. Includes:\n\n    Create Dataset: Steps to create a dataset in BigQuery.\n    Create Table and upload data: Instructions to create a table in BigQuery.\n\nCreate GCP Function using gcloud CLI and environment variables computed previously\n\nInput in a Custom GPT in ChatGPT Perform searches on the embedded data in BigQuery:\n\n    Vector Search: Steps to perform vector-based search queries.\n    Metadata filtering Search: Instructions for performing metadata filtering.\nSet up environment\nInstall and import required libraries\nThe below libraries can be categorized as standard Python libraries, third-party libraries, and GCP-related libraries.\nConfigure GCP project\n\nIf not already set-up, we'll install GCP CLI's, authenticate to GCP and set your default project.\nConfigure OpenAI settings\n\nThis section guides you through setting up authentication for  OpenAI. Before going through this section, make sure you have your OpenAI API key.\nConfigure GCP BigQuery with Vector Search capabilities\n\nThis section explains how to create a dataset in BigQuery and store vectors of float, used for embeddings \u0026 vector search.\nPrepare data\nWe're going to embed and store a few pages of the OpenAI docs in the oai_docs folder. We'll first embed each, add it to a CSV, and then use that CSV to upload to the index.\nWe are going to use some techniques highlighted in this cookbook. This is a quick way to embed text, without taking into account variables like sections, using our vision model to describe images/graphs/diagrams, overlapping text between chunks for longer documents, etc.\nIn order to handle longer text files beyond the context of 8191 tokens, we can either use the chunk embeddings separately, or combine them in some way, such as averaging (weighted by the size of each chunk).\n\nWe will take a function from Python's own cookbook that breaks up a sequence into chunks.\nNow we define a function that encodes a string into tokens and then breaks it up into chunks. We'll use tiktoken, a fast open-source tokenizer by OpenAI.\n\nTo read more about counting tokens with Tiktoken, check out this cookbook.\n\nFinally, we can write a function that safely handles embedding requests, even when the input text is longer than the maximum context length, by chunking the input tokens and embedding each chunk individually. The average flag can be set to True to return the weighted average of the chunk embeddings, or False to simply return the unmodified list of chunk embeddings.\nNote: there are other techniques you can take here, including:\n- using GPT-4o to capture images/chart descriptions for embedding\n- chunking based on paragraphs or sections\n- adding more descriptive metadata about each article.\nNext, we can define a helper function that will capture additional metadata about the documents. In this example, I'll choose from a list of categories to use later on in a metadata filter\nNow, we can define some helper functions to process the .txt files in the oai_docs folder. Feel free to use this on your own data, this supports both .txt and .pdf files.\nWe'll now use this helper function to process our OpenAI documentation. Feel free to update this to use your own data by changing the folder in process_files below.\n\nNote that this will process the documents in chosen folder concurrently, so this should take \u003c30 seconds if using txt files, and slightly longer if using PDFs.\nWe now have an embedded_data.csv file with six columns that we can upload to our vector database!\nCreate BigQuery table with Vector Search\n\nCreate BigQuery dataset\n\nWe'll leverage Google SDK and create a dataset named \"oai_docs\" with a table name of \"embedded_data\", but feel free to change those variables (you can also change regions).\n\nPS: We won't create a BigQuery index, that could improve the performance of the vector search, because such index requires more than 1k rows in our dataset which we don't have in our example, but feel free to leverage that for your own use-case.\nCreating table and upload data\n\nWe'll create the table with the attribute name and types. Note the 'content_vector' attribute that allows to store a vector of float for a single row, which we'll use for our vector search.\n\nThis code will then loop on our CSVs previously created to insert the rows into Bigquery.\nIf you run this code multiple time, multiple identical rows will be inserted which will give less accurate results when doing search (you could put uniqueness on IDs or clean the DB each time).\nTest search\nNow that the data is uploaded, we'll test both pure vector similarity search and with metadata filtering locally below to make sure it is working as expected.\n\nYou can test both a pure vector search and metadata filtering.\n\nThe query below is pure vector search, where we don't filter out on category.\nPerform search with metadata filtering\nMetadata filtering allows to restrict findings that have certain attributes on top of having the closest semantic findings of vector search.\n\nThe provided code snippet demonstrates how to execute a query with metadata filtering:\nCreate GCP function\n\nExporting variables\n\nWe'll deploy the function in main.py in this folder (also available here).\n\nIn a first step, we'll export the variables to target our table/dataset as well as to generate Embeddings using OpenAI's API.\nDeploying the function\n\nWe will now create a google function called \"openai_docs_search\" for our current project, for that we'll launch the CLI command below, leveraging the previously created environment variables. Note that this function can be called from everywhere without authentication, do not use that for production or add additional authentication mechanism.\nInput in a Custom GPT in ChatGPT\n\nNow that we have a GCP Function that queries this Vector Search Index, let's put it as a GPT Action!\n\nSee documentation here on GPTs and here on GPT Actions. Use the below as the instructions for the GPT and as the OpenAPI spec for the GPT Action.\n\nCreate OpenAPI Spec\n\nBelow is a sample OpenAPI spec. When we run the block below, a functional spec should be copied to the clipboard to paste in the GPT Action.\n\nNote that this does not have any authentication by default, but you can set up GCP Functions with Auth by following GCP's docs here.\nCreate GPT Instructions\n\nFeel free to modify instructions as you see fit. Check out our docs here for some tips on prompt engineering.\nRecap\n\nWe've now succesfully integrated GCP BigQuery Vector Search with GPT Actions in ChatGPT by doing the following:\nEmbedded docs using OpenAI's embeddings, while adding some additional metadata using gpt-4o.\nUploaded that data to GCP BigQuery (raw data and vectors of embeddings)\nCreated an endpoint on GCP Functions to retrieve those\nIncorporated it into a custom GPT.\n\nOur GPT can now retrieve informaiton to help answer user queries, making it much more accurate and customized to our data. Here's the GPT in action:\ngcp-rag-quickstart-gpt.png"])</script><script>self.__next_f.push([1,"ab:T814,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\nThis GPT Action provides an overview of how to connect a GPT to Zapier.  Because the majority of configuration occurs on Zapier, we recommend reviewing this helpful guide from Zapier on connecting GPTs to custom Zapier Actions.\nValue + Example Business Use Cases\nValue: Users can now connect custom GPTs within ChatGPT to Zapier and get instant integration to 6,0000+ apps and 20,000+ actions across the tech stack.\n\nExample Use Cases:\nAn organization has already setup Zapier integrations, and would like to avoid additional integration work when connecting their tech ecosystem with ChatGPT\nBuild a Calendar Assistant GPT which looks up calendar events, and provides additional context based on attendees' LinkedIn profiles\nA CRM GPT to help connect Hubspot to ChatGPT allowing sales teams to update or review contacts and notes on the go\n\nApplication Information\nApplication Key Links\nCheck out these links from the application before you get started:\nApplication Website: https://zapier.com\nAI Actions URL: https://actions.zapier.com/gpt/actions/\nAutomatic OpenAPI Configuration: https://actions.zapier.com/gpt/api/v1/dynamic/openapi.json?tools=meta\nApplication Prerequisites\nBefore you get started, make sure you go through the following step in your Zapier:\nConfigure the desired AI Actions via the AI Action Manager\nzapier_ai_actions.png\nzapier_action_config.png\nIn ChatGPT\nIn ChatGPT, from the custom GPT creator screen, click on \"Actions\" and choose \"Import from URL\". Enter in Zapier URL for provisioning GPTs: https://actions.zapier.com/gpt/api/v1/dynamic/openapi.json?tools=meta\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"ac:Tcd8,"])</script><script>self.__next_f.push([1,"\nIn this cookbook, we will explore how to use Structured Outputs to build multi-agent systems.\n\nStructured Outputs is a new capability that builds upon JSON mode and function calling to enforce a strict schema in a model output.\n\nBy using the new parameter strict: true, we are able to guarantee the response abides by a provided schema.\n\nTo demonstrate the power of this feature, we will use it to build a multi-agent system.\n\nWhy build a Multi-Agent System?\n\nWhen using function calling, if the number of functions (or tools) increases, the performance may suffer.\n\nTo mitigate this, we can logically group the tools together and have specialized \"agents\" that are able to solve specific tasks or sub-tasks, which will increase the overall system performance.\nEnvironment set up\nAgents set up\n\nThe use case we will tackle is a data analysis task.\n\nLet's first set up our 4-agents system:\n\nTriaging agent: Decides which agent(s) to call\nData pre-processing Agent: Prepares data for analysis - for example by cleaning it up\nData Analysis Agent: Performs analysis on the data\nData Visualization Agent: Visualizes the output of the analysis to extract insights\n\nWe will start by defining the system prompts for each of these agents.\nWe will then define the tools for each agent.\n\nApart from the triaging agent, each agent will be equipped with tools specific to their role:\n\nData pre-processing agent\n\n\nClean data\nTransform data\nAggregate data\n\nData analysis agent\n\nStatistical analysis\nCorrelation analysis\nRegression Analysis\n\nData visualization agent\n\nCreate bar chart\nCreate line chart\nCreate pie chart\nTool execution\n\nWe need to write the code logic to:\nhandle passing the user query to the multi-agent system\nhandle the internal workings of the multi-agent system\nexecute the tool calls\n\nFor the sake of brevity, we will only define the logic for tools that are relevant to the user query.\nFrom the user query, we can infer that the tools we would need to call are clean_data, start_analysis and use_line_chart.\n\nWe will first define the execution function which runs tool calls.\n\nThis maps a tool call to the corresponding function. It then appends the output of the function to the conversation history.\nNext, we will create the tool handlers for each of the sub-agents.\n\nThese have a unique prompt and tool set passed to the model.\n\nThe output is then passed to an execution function which runs the tool calls.\n\nWe will also append the messages to the conversation history.\nFinally, we create the overarching tool to handle processing the user query.\n\nThis function takes the user query, gets a response from the model and handles passing it to the other agents to execute. In addition to this, we will keep the state of the ongoing conversation.\nMulti-agent system execution\n\nFinally, we run the overarching handle_user_message function on the user query and view the output.\nConclusion\n\nIn this cookbook, we've explored how to leverage Structured Outputs to build more robust multi-agent systems.\n\nUsing this new feature allows to make sure that tool calls follow the specified schema and avoids having to handle edge cases or validate arguments on your side.\n\nThis can be applied to many more use cases, and we hope you can take inspiration from this to build your own use case!"])</script><script>self.__next_f.push([1,"ad:Ted7,"])</script><script>self.__next_f.push([1,"\nStructured Outputs is a new capability in the Chat Completions API and Assistants API that guarantees the model will always generate responses that adhere to your supplied JSON Schema. In this cookbook, we will illustrate this capability with a few examples.\n\nStructured Outputs can be enabled by setting the parameter strict: true in an API call with either a defined response format or function definitions.\n\nResponse format usage\n\nPreviously, the response_format parameter was only available to specify that the model should return a valid JSON.\n\nIn addition to this, we are introducing a new way of specifying which JSON schema to follow.\n\nFunction call usage\n\nFunction calling remains similar, but with the new parameter strict: true, you can now ensure that the schema provided for the functions is strictly followed.\n\nExamples\n\nStructured Outputs can be useful in many ways, as you can rely on the outputs following a constrained schema.\n\nIf you used JSON mode or function calls before, you can think of Structured Outputs as a foolproof version of this.\n\nThis can enable more robust flows in production-level applications, whether you are relying on function calls or expecting the output to follow a pre-defined structure.\n\nExample use cases include:\n\nGetting structured answers to display them in a specific way in a UI (example 1 in this cookbook)\nPopulating a database with extracted content from documents (example 2 in this cookbook)\nExtracting entities from a user input to call tools with defined parameters (example 3 in this cookbook)\n\nMore generally, anything that requires fetching data, taking action, or that builds upon complex workflows could benefit from using Structured Outputs.\nSetup\nExample 1: Math tutor\n\nIn this example, we want to build a math tutoring tool that outputs steps to solving a math problem as an array of structured objects.\n\nThis could be useful in an application where each step needs to be displayed separately, so that the user can progress through the solution at their own pace.\nUsing the SDK parse helper\n\nThe new version of the SDK introduces a parse helper to provide your own Pydantic model instead of having to define the JSON schema. We recommend using this method if possible.\nRefusal\n\nWhen using Structured Outputs with user-generated input, the model may occasionally refuse to fulfill the request for safety reasons.\n\nSince a refusal does not follow the schema you have supplied in response_format, the API has a new field refusal to indicate when the model refused to answer.\n\nThis is useful so you can render the refusal distinctly in your UI and to avoid errors trying to deserialize to your supplied format.\nExample 2: Text summarization\n\nIn this example, we will ask the model to summarize articles following a specific schema.\n\nThis could be useful if you need to transform text or visual content into a structured object, for example to display it in a certain way or to populate database.\n\nWe will take AI-generated articles discussing inventions as an example.\nExample 3: Entity extraction from user input\n\nIn this example, we will use function calling to search for products that match a user's preference based on the provided input.\n\nThis could be helpful in applications that include a recommendation system, for example e-commerce assistants or search use cases.\nConclusion\n\nIn this cookbook, we've explored the new Structured Outputs capability through multiple examples.\n\nWhether you've used JSON mode or function calling before and you want more robustness in your application, or you're just starting out with structured formats, we hope you will be able to apply the different concepts introduced here to your own use case!\n\nStructured Outputs is only available with gpt-4o-mini , gpt-4o-2024-08-06, and future models."])</script><script>self.__next_f.push([1,"ae:T1fb0,"])</script><script>self.__next_f.push([1,"\nIntroduction\n\n\n\nThis page provides an instruction \u0026 guide for developers building middleware to connect a GPT Action to a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\n\nThis particular GPT Action provides an overview of how to build an Google Cloud Function, Google's cloud-based function builder. This documentation helps a user set up an OAuth-protected Google Cloud Function to connect to a GPT Action, and to a sample application.\n\nValue + Example Business Use Cases\n\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Google Cloud Function. This can in a few ways:\n\n100k character limit in GPT Actions: users can use the middleware to pre-process the text response from an API. For example, you can use OpenAI’s API in the middleware to summarize the text before sending it back to ChatGPT.\nTypically for actions, users are relying on the SaaS API to return text. You can convert the response for the vendor API into easily digestible text, and it can handle different data types such as structured and unstructured data.\nIt can return files instead of just text. This can be useful to surface CSV files for Data Analysis, or bring back an PDF file and ChatGPT will treat it like an upload.\n\n\nExample Use Cases:\nA user needs to look up query Google Cloud SQL, but needs a middleware app between ChatGPT and Google Cloud SQL\nA user has built several steps in a row in a Google Cloud function, and needs to be able to kick off that process using ChatGPT\n\nApplication Information\n\nApplication Key Links\n\nCheck out these links from the application before you get started:\nApplication Website: https://cloud.google.com/functions/docs\nApplication API Documentation: https://cloud.google.com/functions/docs/writing/write-http-functions\n\nApplication Prerequisites\n\nBefore you get started, make sure you go through the following steps in your application environment:\nGoogle Cloud Console with access to create Google Cloud Functions and Google Cloud APIs (you will need this to set up the OAuth Client)\n\nApplication Setup\n\nInstalling the app\n\nThere are 3 options to create and deploy the Google Cloud Functions\n\nIDE - create using your favorite IDE, e.g. VS Code\nGoogle Cloud Console - create using your browser\nGoogle Cloud CLI (gcloud) - create through command line\n\nYou can read up on the supported runtimes here\n\nOption 1: Use IDE (VSCode)\n\nSee Google's documentation here for how to deploy using VSCode. If you have familiarity with this approach, feel free to use it.\n\nOption 2: Directly in Google Cloud Console\n\nSee the documentation here for how to deploy using the Google Cloud Console.\n\nOption 3: Use the Google Cloud CLI (gcloud)\n\nSee the documentation here for how to deploy using the Google Cloud Console. We’ll walk through an example here step by step.\n\nPart 1: Install and initialize Google Cloud CLI (gcloud)\nFollow the steps here that are relevant to the OS you are runnning. The last step of this process is for you to run gcloud init and sign in to your Google account\n\nPart 2: Setup local development environment\nIn this example, we will be setting up a Node.js environment.\n\nmkdir\ncd\n\nInitialize the Node.js project\n\nnpm init\nAccept the default values for npm init\n\nPart 3: Create Function\nCreate the index.js file\n\nconst functions = require('@google-cloud/functions-framework');\nconst axios = require('axios');\n\nconst TOKENINFO_URL = 'https://oauth2.googleapis.com/tokeninfo';\n\n// Register an HTTP function with the Functions Framework that will be executed\n// when you make an HTTP request to the deployed function's endpoint.\nfunctions.http('executeGCPFunction', async (req, res) =\u003e {\n  const authHeader = req.headers.authorization;\n\n  if (!authHeader) {\n    return res.status(401).send('Unauthorized: No token provided');\n  }\n\n  const token = authHeader.split(' ')[1];\n  if (!token) {\n    return res.status(401).send('Unauthorized: No token provided');\n  }\n\n  try {\n    const tokenInfo = await validateAccessToken(token);\n    res.json(\"You have connected as an authenticated user to Google Functions\");\n  } catch (error) {\n    res.status(401).send('Unauthorized: Invalid token');\n  }\n});\n\nasync function validateAccessToken(token) {\n  try {\n    const response = await axios.get(TOKENINFO_URL, {\n      params: {\n        access_token: token,\n      },\n    });\n    return response.data;\n  } catch (error) {\n    throw new Error('Invalid token');\n  }\n}\nPart 4: Deploy Function\n\nThis step below will install and add the necessary dependencies in your package.json file\n\nnpm install @google-cloud/functions-framework\nnpm install axios\n\nnpx @google-cloud/functions-framework --target=executeGCPFunction\n\ngcloud functions deploy gcp-function-for-chatgpt \\\n  --gen2 \\\n  --runtime=nodejs20 \\\n  --region=us-central1 \\\n  --source=. \\\n  --entry-point=executeGCPFunction \\\n  --trigger-http \\\n  --allow-unauthenticated\n\nChatGPT Steps\n\nCustom GPT Instructions\n\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nWhen the user asks you to test the integration, you will make a call to the custom action and display the results\n\nOpenAPI Schema\n\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nBelow is an example of what connecting to this Middlware might look like. You'll need to insert your application's \u0026 function's information in this section.\n\nopenapi: 3.1.0\ninfo:\n  title: {insert title}\n  description: {insert description}\n  version: 1.0.0\nservers:\n  url: {url of your Google Cloud Function}\n    description: {insert description}\npaths:\n  /{your_function_name}:\n    get:\n      operationId: {create an operationID}\n      summary: {insert summary}\n      responses:\n        '200':\n          description: {insert description}\n          content:\n            text/plain:\n              schema:\n                type: string\n                example: {example of response}\n\nAuthentication Instructions\n\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nIn Google Cloud Console\nIn Google Cloud Console, you need to create OAuth client ID credentials. To navigate to the right page search for \"Credentials\" in Google Cloud Console or enter https://console.cloud.google.com/apis/credentials?project= in your browser. You can read more about it here.\n\nClick on \"CREATE CREDENTIALS\" and select \"Oauth client ID\". Select \"Web Application\" for \"Application type\" and enter the name of your application (see below).\n\n\n\nIn the \"OAuth client created\" modal dialog, please take note of the\n\nClient ID\nClient secret\n\nIn ChatGPT (refer to Step 2 in the Getting Started Example)\n\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\n\nClient ID*: *see step above\nClient Secret*: *see step above\nAuthorization URL**: https://accounts.google.com/o/oauth2/auth\nToken URL**: https://oauth2.googleapis.com/token\nScope**: https://www.googleapis.com/auth/userinfo.email\n\nBack in Google Cloud Console (while referring to Step 4 in the Getting Started Example)\n\nEdit the OAuth 2.0 Client ID you create in Google Cloud earlier and add the callback URL you received after creating your custom action.\n\nTest the GPT\n\nYou are now ready to test out the GPT. You can enter a simple prompt like \"Test Integration\" and expect to see the following:\n\nRequest to sign into Google\nAllow request to your Google Function\nResponse from ChatGPT showing the response from your function - e.g. \"You have connected as an authenticated user to Google Functions\"\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look.\n"])</script><script>self.__next_f.push([1,"af:T131c,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis solution enables a GPT action to retrieve data from Redshift and perform data analysis.It uses AWS Functions, performing every action from AWS ecosystem and network. The middleware (AWS function) will perform the SQL query, wait for its completion and return the data as a file. The code is provided for information purpose only and should be modified to your needs.\n\nThis solution uses the ability to retrieve files in Actions and use them as if you had uploaded them directly to a conversation.\n\nThis solution highlight a connection to Redshift serverless, the integration with a provisioned Redshift might differ slighltly to retrieve networks and set-up connection, the overall code and (minimal) integration should be similar.\nValue \u0026 Example Business Use Cases\nValue: Leverage ChatGPT's natural language capabilities to connect to Redshift's DWH.\n\nExample Use Cases:\nData scientists can connect to tables and run data analyses using ChatGPT's Data Analysis\nCitizen data users can ask basic questions of their transactional data\nUsers gain more visibility into their data \u0026 potential anomalies\nApplication Information\nApplication Prerequisites\nBefore you get started, make sure that:\nYou have access to a Redshift environment\nYou have the rights to deploy AWS function in the same VPC (Virtual Private Network)\nYour AWS CLI is authenticated\nMiddleware Information\nInstall required libraries\nInstall AWS CLI, required for AWS SAM (docs)\nInstall AWS SAM CLI (docs)\nInstall Python\nInstall yq docs\nMiddleware function\n\nTo create a function, follow the steps in the AWS Middleware Action cookbook.\n\nTo deploy specifically an application that connects to Redshift, use the following code instead of the \"hello-world\" GitHub repository referenced in the Middleware AWS Function cookbook. You can either clone the repository or take the code pasted below and modify it to your needs.\nThis code is meant to be directional - while it should work out of the box, it is designed to be customized to your needs (see examples towards the end of this document).\n\nTo get the code, you can clone openai-cookbook repository and navigate to the redshift-middleware directory\n\ngit clone https://github.com/pap-openai/redshift-middleware\ncd redshift-middleware\nRetrieve VPC information\n\nWe will need to connnect our function to Redshift, therefore we need to find the network used by Redshift. You can find this on your Redshift interface the AWS console, under Amazon Redshift Serverless \u003e Workgroup configuration \u003e your_workgroup \u003e Data access, or through the CLI:\nSet up AWS function\n\nCopy env.sample.yaml to env.yaml and replace with the values obtained above. You will need a Redshift user with access to your DB/schema.\n\ncp env.sample.yaml env.yaml\n\nFill in env.yaml with the values retrieved by the previous command as well as your credentials to Redshift.\nAlternatively, you can create a file named env.yaml manually and fill the following variables:\nRedshiftHost: default-workgroup.xxxxx.{region}.redshift-serverless.amazonaws.com\nRedshiftPort: 5439\nRedshiftUser: username\nRedshiftPassword: password\nRedshiftDb: my-db\nSecurityGroupId: sg-xx\nSubnetId1: subnet-xx\nSubnetId2: subnet-xx\nSubnetId3: subnet-xx\nSubnetId4: subnet-xx\nSubnetId5: subnet-xx\nSubnetId6: subnet-xx\n\nThis file will be used to deploy your function with parameters, as shown below:\n\nPARAM_FILE=\"env.yaml\"\nPARAMS=$(yq eval -o=json $PARAM_FILE | jq -r 'to_entries | map(\"\\(.key)=\\(.value|tostring)\") | join(\" \")')\nsam deploy --template-file template.yaml --stack-name redshift-middleware --capabilities CAPABILITY_IAM --parameter-overrides $PARAMS\n\nThe template.yaml has the following content:\nRetrieve the URL information from the previous command output, you can then run a cURL request, which should return data in a file format:\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, copy the text below in the Instructions panel.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel.\n\nThis expects a response that matches the file retrieval structure in our doc here and passes in a query as a parameter to execute.\n\nMake sure to follow the steps in the AWS Middleware cookbook to set up authentication.\nMake sure to switch the function app name based on your function deployment.\nConclusion\n\nYou now have deployed a GPT that uses a middleware in AWS, in an authenticated manner, that's able to connect to Redsfhit. Users with access (that are in Cognito) can now query your databases to perform data analysis task:\n\n../../../images/redshift_gpt.png"])</script><script>self.__next_f.push([1,"b0:T29ad,"])</script><script>self.__next_f.push([1,"Introduction\nThis particular GPT Action provides an overview of how to build an AWS Lambda function. This documentation helps a user set up an OAuth-protected AWS Function to connect to a GPT Action, and to a sample application. This example uses AWS SAM (Serverless Application Model) in this example to set-up the AWS stack.\nValue + Example Business Use Cases\nValue: Users can now leverage ChatGPT's capabilities to connect to an AWS Function. This enables you to connect to any services in AWS and run code/applications on this. This can in a few ways:\n\nAccess 3rd party services such as AWS Redshift, AWS DynamoDB, AWS S3 and even more!\nAllows pre-processing text responses from an API (overcoming context limits, adding context or metadata as examples).\nEnables to return files instead of retrieving text from 3rd party APIs. This can be useful to surface CSV files for Data Analysis, or bring back an PDF file and ChatGPT will treat it like an upload.\n\nExample Use Cases:\nA user needs to look up data in Redshift, but needs a middleware app between ChatGPT and Redshift to return files (data analysis data exactitude as well as large number of data)\nA user has built several steps in an AWS function, and needs to be able to kick off that process using ChatGPT.\nApplication information \u0026 prerequisites\nWe will leverage AWS Lambda services to create a middleware function. You can get familiar with this stack by visiting the following links:\n\nLambda Website: https://aws.amazon.com/lambda/\nLambda Documentation: https://docs.aws.amazon.com/lambda/\nAWS SAM docs: https://docs.aws.amazon.com/serverless-application-model/\nPrerequisites\nBefore you get started, make sure you have an AWS Console with access to create: Lambda Function, S3 Buckets, Application Stack, Cognito User Pool, Cognito User Pool App Clients, API Gateway, Lambda roles, CloudFormation stacks (this feels like a lot but creating those services is automated!).\nCreate AWS Lambda Function\nTo create an AWS Function you can use AWS SAM. An example of a SAM Template can be found here [0].\n\nThis template includes:\nA User Pool \u0026 User Pool Client, used for OAuth\nA Cognito Authorizer that ensure the function can only be called by authenticated users\nMapping the Lambda function to an existing VPC (useful to connect to other AWS services)\nHas parameters that can be set-up dynamically (e.g: credentials/variables)\nAn API Gateway that maps HTTP routes to the functions\n\nThis code is purely informational to help you get started and doesn't require pre-existing AWS resources. We recommend to map existing user pools if you have any instead of creating new ones, as well as setting up your Lambda in a VPC that has access to other AWS Resources (if you need to leverage those). You can see an example of a set-up like this in the RedShift cookbook.\n\nThe Cognito Authorizer is key to make sure your function can only be called/accessed by authenticated users so make sure to set this up correctly with your environment.\n\n[0]\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: \u003e\n  aws-middleware\n\n  AWS middleware function\n\nParameters:\n  CognitoUserPoolName:\n    Type: String\n    Default: MyCognitoUserPool\n  CognitoUserPoolClientName:\n    Type: String\n    Default: MyCognitoUserPoolClient\n\nResources:\n  MyCognitoUserPool:\n    Type: AWS::Cognito::UserPool\n    Properties:\n      UserPoolName: !Ref CognitoUserPoolName\n      Policies:\n        PasswordPolicy:\n          MinimumLength: 8\n      UsernameAttributes:\n        email\n      Schema:\n        AttributeDataType: String\n          Name: email\n          Required: false\n\n  MyCognitoUserPoolClient:\n    Type: AWS::Cognito::UserPoolClient\n    Properties:\n      UserPoolId: !Ref MyCognitoUserPool\n      ClientName: !Ref CognitoUserPoolClientName\n      GenerateSecret: true\n\n  MiddlewareApi:\n    Type: AWS::Serverless::Api\n    Properties:\n      StageName: Prod\n      Cors: \"'*'\"\n      Auth:\n        DefaultAuthorizer: MyCognitoAuthorizer\n        Authorizers:\n          MyCognitoAuthorizer:\n            AuthorizationScopes:\n              openid\n              email\n              profile\n            UserPoolArn: !GetAtt MyCognitoUserPool.Arn\n\n  MiddlewareFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: aws-middleware/\n      Handler: app.lambda_handler\n      Runtime: python3.11\n      Timeout: 45\n      Architectures:\n        x86_64\n      Events:\n        SqlStatement:\n          Type: Api\n          Properties:\n            Path: /my_route\n            Method: post\n            RestApiId: !Ref MiddlewareApi\n\nOutputs:\n  MiddlewareApi:\n    Description: \"API Gateway endpoint URL for Prod stage for SQL Statement function\"\n    Value: !Sub \"https://${MiddlewareApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/my_route\"\n  MiddlewareFunction:\n    Description: \"SQL Statement Lambda Function ARN\"\n    Value: !GetAtt MiddlewareFunction.Arn\n  MiddlewareFunctionIamRole:\n    Description: \"Implicit IAM Role created for SQL Statement function\"\n    Value: !GetAtt MiddlewareFunctionRole.Arn\n  CognitoUserPoolArn:\n    Description: \"ARN of the Cognito User Pool\"\n    Value: !GetAtt MyCognitoUserPool.Arn\n\nYou can clone the openai-cookbook repository \u0026 take the sample python code \u0026 SAM template from the lambda-middleware directory:\n\ngit clone https://github.com/pap-openai/lambda-middleware\ncd lambda-middleware\n\nTo build \u0026 deploy your function, run the following commands from this directory\n\nsam build\nsam deploy --template-file template.yaml --stack-name aws-middleware --capabilities CAPABILITY_IAM\n\nOnce you have this deployed, you can go check out the application on AWS Lambda:\n\n../../../images/aws_lambda_1.png\n\nYou can confirm that the function is not reachable unless authenticated by running a curl command without any authentication:\n\ncurl -d {}\n\nwhich should return {\"message\":\"Unauthorized\"}.\nSet up Auth in AWS Cognito\nOptional: do those steps only if you created a user pool and are not using an existing one\n\nLet's create a user in the newly user pool. To do that, fetch the output of CognitoUserPoolArn in the deploy command, and get the value after the \"/\", which should be in the format of: your-region_xxxxx.\n\naws cognito-idp admin-create-user \\\n    --user-pool-id \"your-region_xxxxx\" \\\n    --username johndoe@example.com \\\n    --user-attributes Name=email,Value=johndoe@example.com \\\n    --temporary-password \"TempPassword123\"\n\nLet's now make sure we create a webpage/domain on which we can log-in. Go to AWS Cognito, select the newly created user pool \u0026 go to App Integration tab:\n\n../../../images/aws_lambda_3.png\n\nCreate a Cognito Domain by clicking on \"Domains\" then \"Create Cognito Domain\"\n\n../../../images/aws_lambda_8.png\n\nScroll down to App client list on the App Integration page of your User Pool:\n\n../../../images/aws_lambda_9.png\n\nSelect your app client and edit the Hosted UI:\n\n../../../images/aws_lambda_10.png\n\nAnd add a callback URL, Authorization Scheme and OAuth scope:\n\n../../../images/aws_lambda_11.png\n\nNote that you'll come back to this step when ChatGPT will generate a callback URL for the authentication of your action. The postman URL, should be used only for development purpose.\n\nYou can try this connection in Postman, under Authorization for your `, copy/paste the value from AWS for the client_id, client_secret and the URL you set up for the auth domain, make sure to add openid` in the scope to get a valid access_token:\n\n../../../images/aws_lambda_12.png\n\n../../../images/aws_lambda_13.png\n\nIf you're now doing the request on Postman, using the access_token you just retrieve, you'll get a success JSON returned:\n\n../../../images/aws_lambda_14.png\nCreate Action in ChatGPT\nNow let's integrate this into ChatGPT.\n\nCreate an action and copy paste the following spec:\n\nopenapi: 3.1.0\ninfo:\n  title: Success API\n  description: API that returns a success message.\n  version: 1.0.0\nservers:\n  url: https://3ho5n15aef.execute-api.us-east-1.amazonaws.com/Prod\n    description: Main production server\npaths:\n  /my_route:\n    post:\n      operationId: postSuccess\n      summary: Returns a success message.\n      description: Endpoint to check the success status.\n      responses:\n        '200':\n          description: A JSON object indicating success.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  success:\n                    type: boolean\n                    example: true\n\nIf you try to test the action (you can click the \"Test\" Button), you'll see that you have a 401 as you're not authenticated.\n\nLet's now add authentication in the action.\n\nClick on Authentication \u003e OAuth.\nWe'll now need to fetch AWS Cognito's variables. Let's go on your User Pool \u003e User Pool App Client. From there you can retrieve your client ID and client Secret.\n\n../../../images/aws_lambda_15.png\n\nCopy paste those values in ChatGPT. Now let's add the Token URLs.\n\nFrom your User Pool you'll find the URL you've previously created for the hosted domain.\n\n../../../images/aws_lambda_16.png\n\nWe'll take this URL and append AWS routes for OAuth.\n\ntoken: /oauth2/token\nauthorization: /oauth2/authorize\n\nCopy paste those in ChatGPT.\n\nIn scope, add openid and click on Save.\nConfigure Cognito with ChatGPT URL\nNow go back on your GPT (moving out of the action subview), and you'll see a callback URL provided by ChatGPT for the Authentication:\n\n../../../images/aws_lambda_17.png\n\nGet this URL and edit the hosted UI of your User Pool App client \u0026 save the changes:\n\n../../../images/aws_lambda_18.png\nTesting the function\nYou can now test this action again:\n\n../../../images/aws_lambda_19.png\n\nYou will be redirected to AWS Cognito page, which you can log-in in using the credentials previously set-up.\n\nIf you now ask the GPT to run the same action, it will answer correctly as you're now authenticated and able to run this function!\n\n../../../images/aws_lambda_20.png\nConclusion\nYou've now set-up an action in ChatGPT that can talk with your applications in AWS, in an authenticated way! This cookbook shows you how to create the Cognito Pool from scratch using username/password, though, we recommend to set-up Cognito based on your needs (for example by plugging your own IDP into Cognito).\n\nAdditionally, the function is not connected to any other services, which is the advantage of being able to communicate to an AWS Lambda function in a safe way. You can therefore tweak the code and AWS SAM template to fit your need. An example of a more complex function is Redshift, that follows those steps to create the function and authentication but has a different code/deployment."])</script><script>self.__next_f.push([1,"b1:T1620,"])</script><script>self.__next_f.push([1,"\nIntroduction\n\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\n\n\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\n\nThis particular GPT Action provides an overview of how to connect to Google Drive, Google’s File storage system. This action will allow you to list and query against file names, load the file content into your GPT, and ultimately use that data as context in ChatGPT.  This set of actions is extensible by additional methods found via the Google Drive API.  This is great if you want a generalist GPT that can read smaller files, such as:\n\nMeetings minutes\nProduct design documents\nShort memos\nFrequently-asked questions\n\nFor something that wants to read longer memos such as entire books, complex CSVs with many rows, we suggest building a Google Docs or Google Sheets-specific GPT.\n\nValue + Example business case\n\nUsers can now leverage ChatGPT's natural language capability to connect directly to files in Google Drive\n\nExample Use Cases:\n\nA user needs to look up which files relate to a certain topic\nA user needs an answer to a critical question, buried deep in documents\n\nApplication Information\n\nApplication Key Links\n\nCheck out these links from the application before you get started:\n\n\nApplication Website: https://www.google.com/drive/\nApplication API Documentation: https://developers.google.com/drive/api/guides/about-sdk\n\nApplication Prerequisites\n\nBefore you get started, make sure you have a Google Cloud account and that the Drive API is enabled:\n\n\n\nSet up a Google Cloud project\nEnable Google Drive API from Google API Library\nIf application’s  “Publishing Status” is “Testing”, ensure users are added to your application\n\nChatGPT Steps\n\nExample Custom GPT Instructions\n\nOnce you've created a Custom GPT, to get started, copy the text below in the Instructions panel. You may have to add additional context specific to your use case.  In this way, it is worth testing additional instructions you add to optimize for clarity and accuracy. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nExample OpenAPI Schema\n\nOnce you've created a Custom GPT, copy the text below in the Actions panel.  This offers an example of what you could include within functions, but . Have questions? Check out Getting Started Example to see how this step works in more detail.  As well, try ActionsGPT, a CustomGPT OpenAI created to help with Actions.  The three examples are:\n\n\nList Files**: this is the core action that lists the files in your drive.  Within this are a few parameters, such as q, includeItemsFromAllDrives,supportsAllDrives\nGet Metadata**: in case list doesn't work, this can offer as a backup based on certain results - for example, if users attempt to make a search via “meeting from last week”, etc\nExport**: exports in a byte content.  For more reading, please consult https://developers.google.com/drive/api/reference/rest/v3/files/export\n\nGenerally, if ‘get’ is used, the model will attempt to download the file, which may be undesirable. Thus, Export is recommended instead.\n\nAuthentication Instructions\n\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nPre-Action Steps\n\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\n\n\n\nGo to the Google Cloud Console\nNavigate to Enabled API \u0026 Services and enable Google Drive API\n\nalt_text\n\nalt_text\n\n\nWithin the search bar, search Google Drive API:\n\nalt_text\n\n\nCreate new OAuth credentials (or use an existing one).  Note that if you haven’t set up an OAuth credentials screen, you will need to do that.\n\n\nalt_text\n\n\n\n\nWithin this process, you will need to grant access to the correct permissions, establish the primary tester as a testing email if Testing is enabled, and set up the OAuth rate limit.\nNext, go to credentials and click “+ Create Credentials” and click “Create Credentials”.  Below is an example of what this screen looks like when it’s already set up.\n\n\nalt_text\n\n\n\n\nLocate your OAuth Client ID \u0026 Client Secret and store both values securely (see screenshot below)\n\n\n\nalt_text\n\nIn ChatGPT\n\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\n\nClient ID**: use Client ID from steps above\nClient Secret**: use Client Secret from steps above\nAuthorization URL**: https://accounts.google.com/o/oauth2/auth\nToken URL**: https://oauth2.googleapis.com/token\nScope**: https://www.googleapis.com/auth/drive\n    Note: for a list of more detailed scopes enabled, please refer to Google’s OAuth 2.0 guide.\nToken**: Default (POST)\nPrivacy Policy**: https://policies.google.com/privacy?hl=en-US\n\nPost-Action Steps\n\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\n\n\n\nCopy the callback URL from the GPT Action\n\n\nalt_text\n\n\n\n\nIn the “Authorized redirect URIs”, add your callback URL\n\n\nalt_text\n\nFAQ \u0026 Troubleshooting\n\n\n\nCallback URL Error: If you get a callback URL error in ChatGPT, pay close attention to the screenshot above. You need to add the callback URL directly into GCP for the action to authenticate correctly.\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our GitHub, and we’ll take a look.\n\n\n\n\ngd2md-html: xyzzy Mon Aug 12 2024"])</script><script>self.__next_f.push([1,"b2:T1321,"])</script><script>self.__next_f.push([1,"Introduction\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\n\n\n\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Building a GPT Action from Scratch\nThis particular GPT Action provides an overview of how to connect to a Snowflake Data Warehouse. This Action takes a user’s question, scans the relevant tables to gather the data schema, then writes a SQL query to answer the user’s question.\n\nNote: This cookbook return back a ResultSet SQL statement, rather than the full result that is not limited by GPT Actions application/json payload limit. For production and advanced use-case, a middleware is required to return back a CSV file. You can follow instructions in the GPT Actions - Snowflake Middleware cookbook to implement this flow instead.\nValue + Example Business Use Cases\n\nValue: Users can now leverage ChatGPT's natural language capability to connect directly to Snowflake’s Data Warehouse..\n\nExample Use Cases:\n\n\n\nData scientists can connect to tables and run data analyses using ChatGPT's Data Analysis\nCitizen data users can ask basic questions of their transactional data\nUsers gain more visibility into their data \u0026 potential anomalies\nApplication Information\nApplication Key Links\n\nCheck out these links from the application before you get started:\n\nApplication Website: https://app.snowflake.com/\nApplication API Documentation: https://docs.snowflake.com/en/developer-guide/sql-api/intro\nApplication Prerequisites\nBefore you get started, make sure you go through the following steps in your application environment:\n\nProvision a Snowflake Data Warehouse\nEnsure that the user authenticating into Snowflake via ChatGPT has access to the database, schemas, and tables with the necessary role\nChatGPT Steps\n\nCustom GPT Instructions\n\nOnce you've created a Custom GPT, copy the text below in the Instructions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\n\nOpenAPI Schema\n\nOnce you've created a Custom GPT, copy the text below in the Actions panel. Have questions? Check out Getting Started Example to see how this step works in more detail.\nAuthentication Instructions\nBelow are instructions on setting up authentication with this 3rd party application. Have questions? Check out Getting Started Example to see how this step works in more detail.\nPre-Action Steps\nBefore you set up authentication in ChatGPT, please take the following steps in the application.\nReview the Snowflake OAuth Overview: https://docs.snowflake.com/en/user-guide/oauth-snowflake-overview\nGo to the Snowflake Worksheets\nCreate new OAuth credentials through a Security Integration - you will need a new one for each OAuth app/custom GPT since Snowflake Redirect URIs are 1-1 mapped to Security Integrations\n\n\nRetrieve your OAuth Client ID, Auth URL, and Token URL\n\n\nYou’ll find the required information in these 3 columns:\n\n../../../images/snowflake_direct_oauth.png\n\nRetrieve your OAuth Client Secret\n\nYou’ll find the Client Secret in OAUTH_CLIENT_SECRET.\n\nYou’re now set to test your action in Postman. Once you get a successful API response there, you can proceed with your GPT Action.\nIn ChatGPT\nIn ChatGPT, click on \"Authentication\" and choose \"OAuth\". Enter in the information below.\n\nClient ID: use Client ID from steps above\nClient Secret: use Client Secret from steps above\nAuthorization URL: use Authorization Endpoint from steps above\nToken URL: use Token Endpoint from steps above\nScope: \u0026lt;empty\u003e*\nToken: Default (POST)\n\n*Snowflake scopes pass the role, but you’ll notice the action itself also specifies the role as a parameter in runQuery, so the Scope is unnecessary. You may elect to pass roles in the scope instead of the action parameters if it makes more sense for your GPT.\nPost-Action Steps\n\nOnce you've set up authentication in ChatGPT, follow the steps below in the application to finalize the Action.\n\nCopy the callback URL from the GPT Action\nUpdate the Redirect URI in your Security Integration to the callback URL provided in ChatGPT.\n\nFAQ \u0026 Troubleshooting\n\nCallback URL Error: If you get a callback URL error in ChatGPT, pay close attention to the Post-Action Steps above. You need to add the callback URL directly into your Security Integration for the action to authenticate correctly\nSchema calls the wrong warehouse or database: If ChatGPT calls the wrong warehouse or database, consider updating your instructions to make it more explicit either (a) which warehouse / database should be called or (b) to require the user provide those exact details before it runs the query\n\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"b3:T49d3,"])</script><script>self.__next_f.push([1,"Introduction\n\nThis page provides an instruction \u0026 guide for developers building a GPT Action for a specific application. Before you proceed, make sure to first familiarize yourself with the following information:\n\nIntroduction to GPT Actions\nIntroduction to GPT Actions Library\nExample of Buliding a GPT Action from Scratch\n\nThis guide provides details on how to connect ChatGPT with a Snowflake Data Warehouse for the purposes of returning a SQL query to ChatGPT for use with Data Analysis. The GPT requires an action that interfaces with middleware (ie Azure function) so that the action can properly format the response from Snowflake for use in the Python notebook environment. Data must be returned as a file, so the middleware function should transform the SQL response into a CSV/Excel file, under 10MB in size.\n\nThis document will outline the Middleware function GPT action. For setting up the middleware function itself, see GPT Actions library (Middleware) - Azure Functions. You can combine this Snowflake middleware action with an action to Snowflake Directly to enable a GPT that can form and test SQL queries prior to executing them.\nValue + Example Business Use Cases\n\nExisting Snowflake customers can leverage these guidelines to query data from their data warehouse and load that data into the Data Analysis Python environment for further insights. This enables ChatGPT powered analysis such as visualizing data sets, identifying patterns/anomalies, or identifying gaps for data cleansing purposes. This GPT can be used to drive business decisions from relatively small datasets, or to explore subsets of data through AI to generate hypotheses as you explore the holistic dataset in your BI tool, saving time and money, while identifying previously unseen patterns.\nApplication Information\nApplication Key Links\n\nCheck out these links from Snowflake and Azure before you get started:\n\nSnowflake Action\n\nApplication Website: https://app.snowflake.com/\nApplication Python Connector Documentation: https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect\n\nAzure Function\n\nApplication Website: https://learn.microsoft.com/en-us/azure/azure-functions/\nApplication API Documentation: https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference/\n\nApplication Prerequisites\n\nBefore you get started, make sure you go through the following steps in your application environment:\n\nProvision a Snowflake Data Warehouse\nEnsure that the user authenticating into Snowflake via ChatGPT has access to the database, schemas, and tables with the necessary role\n\nIn addition, before creating your application in Azure Function App, you’ll need a way to handle user authentication. You’ll need to set up an OAuth App Registration in Azure Entra ID that can be linked with a Snowflake External OAuth security integration. Snowflake’s External OAuth security integrations allow external systems to issue access tokens that Snowflake can use for determining level of access. In this case, that external token provider is Azure Entra ID. Since ChatGPT will connect to Azure rather than Snowflake, the GPT user’s OAuth token will be provisioned by Azure associated with their user in Entra ID. Thus you’ll need a way to map users in Snowflake to their corresponding user in Azure.\n\nAll of the necessary steps for both the Azure side and the Snowflake side are laid out below.\nConfigure the OAuth resource in Azure Entra ID\n\nWe’ll set up a new App Registration, configure the necessary Snowflake Scopes in Azure that will be used, and retrieve all of the OAuth configuration parameters that will be needed in both Snowflake and ChatGPT. This section will all be in Azure so that in the next section, you’ll have the necessary info to link to this App Registration when configuring on the Snowflake side.\nNavigate to the Microsoft Azure Portal and authenticate.\nNavigate to Azure Entra ID (formerly Active Directory).\nClick on App Registrations under Manage.\nClick on New Registration.\nEnter Snowflake GPT OAuth Client, or similar value as the Name.\nVerify the Supported account types is set to Single Tenant.\nIgnore Redirect URI for now. You will come back for this once you are configuring your GPT\nClick Register.\nNote down the Directory (tenant) ID (TENANT_ID) under Essentials. You will use this to generate your AZURE_AD_ISSUER and AZURE_AD_JWS_KEY_ENDPOINT.\n    The AZURE_AD_ISSUER is https://sts.windows.net/TENANT_ID/\n    The AZURE_AD_JWS_KEY_ENDPOINT is https://login.microsoftonline.com/TENANT_ID/discovery/v2.0/keys\nClick on Endpoints in the Overview interface.\nOn the right-hand side, note the OAuth 2.0 authorization endpoint (v2) as the AZURE_AD_OAUTH_AUTHORIZATION_ENDPOINT  and OAuth 2.0 token endpoint (v2) as the AZURE_AD_OAUTH_TOKEN_ENDPOINT.\n    The endpoints should be similar to https://login.microsoftonline.com/90288a9b-97df-4c6d-b025-95713f21cef9/oauth2/v2.0/authorization and https://login.microsoftonline.com/90288a9b-97df-4c6d-b025-95713f21cef9/oauth2/v2.0/token.\nClick on Expose an API **under **Manage.\nClick on the Set link next to Application ID URI to set the Application ID URI.\n    The Application ID URI must be unique within your organization’s directory, such as https://your.company.com/4d2a8c2b-a5f4-4b86-93ca-294185f45f2e. This value will be referred to as the `` in the subsequent configuration steps.\nTo add a Snowflake Role as an OAuth scope for OAuth flows where the programmatic client acts on behalf of a user, click on Add a scope to add a scope representing the Snowflake role.\n    Enter the scope by having the name of the Snowflake role with the session:scope: prefix. For example, for the Snowflake Analyst role, enter session:scope:analyst.\n    Select who can consent.\n    Enter a display name for the scope (e.g.: Account Admin).\n    Enter a description for the scope (e.g.: Can administer the Snowflake account).\n    Click Add Scope.\n    Save the scope as AZURE_AD_SCOPE. It should be a concatenation of your Application ID URI and your Scope name\nIn the Overview section, copy the ClientID from the Application (client) ID field. This will be known as the OAUTH_CLIENT_ID in the following steps.\nClick on Certificates \u0026 secrets and then New client secret.\nAdd a description of the secret.\nSelect 730 days (24 months). For testing purposes, select secrets that don’t expire soon.\nClick Add. Copy the secret. This will be known as the OAUTH_CLIENT_SECRET in the following steps.\nFor programmatic clients that will request an Access Token on behalf of a user, configure Delegated permissions for Applications as follows.\n    Click on API Permissions.\n    Click on Add Permission.\n    Click on My APIs.\n    Click on the Snowflake OAuth Resource that you created in Configure the OAuth resource in Azure AD.\n    Click on the Delegated Permissions box.\n    Check on the Permission related to the Scopes defined in the Application that you wish to grant to this client.\n    Click Add Permissions.\n    Click on the Grant Admin Consent button to grant the permissions to the client. Note that for testing purposes, permissions are configured this way. However, in a production environment, granting permissions in this manner is not advisable.\n    Click Yes.\n\nCreate a security integration in Snowflake\nOnce the App Registration is complete in Azure Entra ID, the next step is to link that App Registration to Snowflake via an External OAuth Security Integration. The external_oauth_audience_list parameter of the security integration must match the Application ID URI that you specified while configuring Azure Entra ID.\n\nThe Issuer and the JWS Keys endpoint will also come from values collected in the previous steps. The User Mapping Attribute can either be set to EMAIL_ADDRESS or LOGIN_NAME, and this is how user’s Microsoft login credentials will be mapped to their user in Snowflake to ensure permissions in Snowflake are honored by the access token issued to ChatGPT.\nMiddleware information:\nMake sure you go through the following steps in your Azure environment:\n\nAzure Portal or VS Code with access to create Azure Function Apps and Azure Entra App Registrations\nThere is a detailed section in this guide related to deploying and designing the function required to wrap the response from Snowflake in order to return the query results as a CSV to ChatGPT. The Azure Function App allows your GPT to ingest larger datasets as ChatGPT can ingest more data from files responses rather than from application/json payloads. Additionally, those datasets will only be available for Data Analysis (aka Code Interpreter) with a response formatted as a CSV file.\nAzure Function App\nNow that we have the GPT created and handled Azure/Snowflake authentication, we can create the Azure Function App itself to execute the SQL query and handle the response formatting enabling the GPT to download the result as a CSV for use with Data Analysis.\n\nFollow this Azure Cookbook Guide for further details deploying an Azure Function App. Below you will find sample code to add to the function.\n\nThis code is meant to be directional - while it should work out of the box, you should customize it based on the needs specific to your GPT and your IT setup.\nApplication Code\nYou’ll need to setup the following flows in your Azure Function App:\n\nExtracting the token from the HTTP request and using it to connect to Snowflake\nExecuting the SQL query and writing the results to a CSV\nTemporarily storing that CSV in Blob Storage*\nGenerating a pre-signed URL to access that CSV securely*\nResponding with an openaiFileResponse\n\n*These steps may not be required if you use the file stream option instead of the url option for returning files to your GPT. More on this below.\n\nEnsure you have the necessary libraries installed and imported into your script. In addition to Python standard libraries, this sample script leveraged the following:\nConnecting to Snowflake\nTo connect to Snowflake, you’ll need to extract the access token assigned from Azure Entra ID from the Authorization header and use that token when connecting to the Snowflake server.\n\nIn this this example, Snowflake usernames are email addresses which simplifies the mapping of the Entra ID user extracted from the HTTP access token to the Snowflake user ID needed to connect. If this is not the case for your organization, you can map email addresses to Snowflake user IDs in your Python application.\n\nMy application was built to interface with a single Snowflake Account (i.e. ab12345.eastus2.azure) and Warehouse. If you need to access multiple accounts or warehouses, you may consider passing these parameters in your GPT action parameters so you can extract them from the HTTP request.\nExecute query and save CSV\nOnce you connect to Snowflake you’ll need to execute the query and store the results into a CSV. While the role in Snowflake should prevent any chance of harmful queries, you may want to sanitize your query in your application (not included below) just as you would any other programmatic SQL query execution.\nStoring the file in Blob Storage\nThere are 2 methods for returning files to ChatGPT for processing. You can either stream the base64 encoded data along with the mimeType and file name in the openaiFileResponse list response, or you can return a list of URLs. In this solution we’ll focus on the latter.\n\nTo do this, you’ll need to upload the CSV to Azure Blob Storage and return a pre-signed URL for accessing that file securely in ChatGPT. It is important to note that in order to download a URL in ChatGPT, you’ll need to ensure that URL includes a content_type and content_disposition, as in the below example. If you’d like to inspect whether a URL has the necessary headers, you can use `curl -I ` from any terminal.\n\nYou’ll need to get a connection String for your Azure storage bucket, as per instructions here.\nFormat openaiFileResponse\nLastly, you’ll need to format the response appropriately to instruct ChatGPT to process that response as a file or series of files. The openaiFileResponse is a list which can include up to 10 URLs (or base64 encodings if using the inline option).\n\nThere are a lot of moving pieces to this application, so testing your Azure Function App can be important. ChatGPT can be a difficult testing grounds given that requests and responses can sometimes be more opaque than needed for debugging. Initial testing of your application through cURL or Postman to invoke the HTTP request from a more controlled environment will allow you to debug and triage issues more easily. Once you determine that responses are being returned as expected in those tools, you are ready to build your GPT.\nChatGPT Steps\nCustom GPT Instructions\nOnce you've created a Custom GPT, use the text below in the Instructions panel for inspiration. Have questions? Check out Getting Started Example to see how this step works in more detail.\nExample Instructions\nIt is important that ChatGPT understands your table schema to properly form SQL queries. There are different methods for doing so, and this Instruction set represents the most direct way. We are working to publish additional instructions for different versions of Snowflake GPTs you may want to build to allow for working with multiple different tables, schemas and databases, or to even learn dynamically for schemas that tend to change over time.\n\nBelow are some basic instructions when working with a single schema and table. This GPT has been optimized for a single use case (analyzing flight data from January 2013 out of NYC) which allows for the most simple instructions to provide the most reliable GPT performance.\nYou are an expert at writing SQL queries to fetch data from Snowflake. You help users convert their prompts into SQL queries. Any question around flight data will be converted into a Snowflake SQL query that hits the table FLIGHTS.PUBLIC.JAN_2013_NYC. Pass any query into the \"sql_query\" parameter\n\n\nThe schema of the table includes\nID\tNUMBER\tA unique identifier for each flight\nYEAR\tNUMBER\tThe year of the flight\nMONTH\tNUMBER\tThe month of the flight\nDAY\t\tNUMBER\tThe day of the month on which the flight departed\nDEP_TIME\tNUMBER\tThe actual departure time of the flight\nSCHED_DEP_TIME\tNUMBER\tThe scheduled departure time of the flight\nDEP_DELAY\tNUMBER\tThe departure delay in minutes (negative values indicate early departures)\nARR_TIME\tNUMBER\tThe actual arrival time of the flight\nSCHED_ARR_TIME\tNUMBER\tThe scheduled arrival time of the flight\nARR_DELAY\tNUMBER\tThe arrival delay in minutes (negative values indicate early arrivals)\nCARRIER_CODE\tTEXT\tThe carrier code of the airline\nFLIGHT\tNUMBER\tThe flight number\nTAILNUM\tTEXT\tThe aircraft tail number\nORIGIN_AIRPORT_CODE\tTEXT\tThe origin airport code\nDEST_AIRPORT_CODE\tTEXT\tThe destination airport code\nAIR_TIME\tNUMBER\tThe total airtime of the flight in minutes\nDISTANCE\tNUMBER\tThe distance traveled by the flight in miles\nHOUR\tNUMBER\tThe hour part of the scheduled departure time\nMINUTE\tNUMBER\tThe minute part of the scheduled departure time\nTIME_HOUR\tNUMBER\tThe time at which the flight departed (rounded to the nearest hour)\nCARRIER_NAME\tTEXT\tThe full name of the airline carrier\nORIGIN_AIRPORT_NAME\tTEXT\tThe full name of the origin airport\nORIGIN_REGION\tTEXT\tThe region code of the origin airport\nORIGIN_MUNICIPALITY\tTEXT\tThe city where the origin airport is located\nORIGIN_COORDINATES\tTEXT\tThe geographical coordinates of the origin airport\nDEST_AIRPORT_NAME\tTEXT\tThe full name of the destination airport\nDEST_REGION\tTEXT\tThe region code of the destination airport\nDEST_MUNICIPALITY\tTEXT\tThe city where the destination airport is located\nDEST_COORDINATES\tTEXT\tThe geographical coordinates of the destination airport\n\n\nWhen a user asks for data around flights, perform the following:\n\n\nUse the executeSQL action to send a POST request to the Azure function endpoint\nReceive the file that is returned as part of the Action response. Display it as a spreadsheet\nPerform analysis on the file and provide the necessary information that the user has asked for\n\n\nThe user will wish to ask questions about the data in code interpreter, so use that for any data analysis insights from the dataset you pulled.\nOpenAPI Schema\nOnce you've created a Custom GPT, copy the text below in the Actions panel, replacing the placeholder values with your specific function details and updating your parameters based on any additional inputs you built into your Azure Function App.\n\nHave questions? Check out Getting Started Example to see how this step works in more detail.\nFAQ \u0026 Troubleshooting\nFiles returned to ChatGPT are limited in size to 10MB. Your request may fail if the file returned is larger than that. Ensure to include LIMITs on your SQL commands if you find you are running into these limitations.\nWhy is the Azure Function App requred in the first place? ChatGPT’s Data Analysis feature (aka Code Interpreter) depends on a secure Python environment that is separate from the model’s context window. Data passed to Data Analysis must be done so by uploading a file today. GPT actions returning data must then return that data as a CSV or other data file type. In order to return a file via GPT action, the response must be wrapped in an openaiFileResponse object. This requires custom code to properly format the response.\nMy company uses a different cloud provider than Azure. For connecting other middleware functions to ChatGPT via GPT action, please refer to other AWS or GCP middleware cookbooks. You can use the concepts discussed in this cookbook to advise on considerations when building your middleware app, but connecting that middleware to Snowflake may be different for different cloud providers. For example, Snowflake built an External OAuth integration specifically for linking with Azure Entra ID.\nHow do I limit the datasets that my GPT has access to? It can be imporant to limit the scope of access ChatGPT has within Snowflake. There are a few ways to do this:\n    Snowflake roles can limit who has access to which tables, and will be respected by the GPT user’s access token provisioned by Azure Entra ID\n    In your middleware function you can add sanity checks to verify the tables accessed are approved by for that application\n    You may want to generate an entirely new Database/Warehouse specific to integrating with ChatGPT that is scrubbed of anything sensitive, such as PII.\nSchema calls the wrong warehouse or dataset: If ChatGPT calls the wrong warehouse or database, consider updating your instructions to make it more explicit either (a) which warehouse / database should be called or (b) to require the user provide those exact details before it runs the query\n\nAre there integrations that you’d like us to prioritize? Are there errors in our integrations? File a PR or issue in our github, and we’ll take a look."])</script><script>self.__next_f.push([1,"7:[\"$\",\"nav\",null,{\"className\":\"flex items-center justify-between py-3 px-4 sm:px-8 border-b sticky top-0 bg-background h-16 z-50\",\"children\":[[\"$\",\"a\",null,{\"href\":\"/\",\"className\":\"flex items-center space-x-1.5\",\"children\":[[\"$\",\"svg\",null,{\"viewBox\":\"0 0 320 320\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-7 fill-black dark:fill-white min-[850px]:hidden\",\"children\":[\"$\",\"path\",null,{\"d\":\"$12\"}]}],[\"$\",\"svg\",null,{\"viewBox\":\"0 0 1180 320\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"h-7 fill-black dark:fill-white hidden min-[850px]:block\",\"children\":[[\"$\",\"path\",null,{\"d\":\"m367.44 153.84c0 52.32 33.6 88.8 80.16 88.8s80.16-36.48 80.16-88.8-33.6-88.8-80.16-88.8-80.16 36.48-80.16 88.8zm129.6 0c0 37.44-20.4 61.68-49.44 61.68s-49.44-24.24-49.44-61.68 20.4-61.68 49.44-61.68 49.44 24.24 49.44 61.68z\"}],[\"$\",\"path\",null,{\"d\":\"m614.27 242.64c35.28 0 55.44-29.76 55.44-65.52s-20.16-65.52-55.44-65.52c-16.32 0-28.32 6.48-36.24 15.84v-13.44h-28.8v169.2h28.8v-56.4c7.92 9.36 19.92 15.84 36.24 15.84zm-36.96-69.12c0-23.76 13.44-36.72 31.2-36.72 20.88 0 32.16 16.32 32.16 40.32s-11.28 40.32-32.16 40.32c-17.76 0-31.2-13.2-31.2-36.48z\"}],[\"$\",\"path\",null,{\"d\":\"m747.65 242.64c25.2 0 45.12-13.2 54-35.28l-24.72-9.36c-3.84 12.96-15.12 20.16-29.28 20.16-18.48 0-31.44-13.2-33.6-34.8h88.32v-9.6c0-34.56-19.44-62.16-55.92-62.16s-60 28.56-60 65.52c0 38.88 25.2 65.52 61.2 65.52zm-1.44-106.8c18.24 0 26.88 12 27.12 25.92h-57.84c4.32-17.04 15.84-25.92 30.72-25.92z\"}],[\"$\",\"path\",null,{\"d\":\"m823.98 240h28.8v-73.92c0-18 13.2-27.6 26.16-27.6 15.84 0 22.08 11.28 22.08 26.88v74.64h28.8v-83.04c0-27.12-15.84-45.36-42.24-45.36-16.32 0-27.6 7.44-34.8 15.84v-13.44h-28.8z\"}],[\"$\",\"path\",null,{\"d\":\"m1014.17 67.68-65.28 172.32h30.48l14.64-39.36h74.4l14.88 39.36h30.96l-65.28-172.32zm16.8 34.08 27.36 72h-54.24z\"}],[\"$\",\"path\",null,{\"d\":\"m1163.69 68.18h-30.72v172.32h30.72z\"}],[\"$\",\"path\",null,{\"d\":\"$13\"}]]}],[\"$\",\"span\",null,{\"className\":\"text-primary text-xl font-light mb-0.5\",\"children\":\"Cookbook\"}]]}],[\"$\",\"$L14\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex deskNav\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-1 sm:space-x-2 mx-3 sm:mx-4\",\"children\":[[\"$\",\"$L15\",null,{}],[\"$\",\"a\",null,{\"href\":\"/about\",\"target\":\"$undefined\",\"className\":\"text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 \",\"children\":[[\"$\",\"span\",null,{\"className\":\"hidden sm:inline\",\"children\":\"About\"}],[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"inline sm:hidden w-3.5 h-3.5\",\"children\":[\"$\",\"path\",null,{\"d\":\"$16\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}]]}],[\"$\",\"a\",null,{\"href\":\"https://platform.openai.com/docs/introduction\",\"target\":\"_blank\",\"className\":\"text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 \",\"children\":[[\"$\",\"span\",null,{\"className\":\"hidden sm:inline\",\"children\":\"API Docs\"}],[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"inline sm:hidden w-3.5 h-3.5\",\"children\":[\"$\",\"path\",null,{\"d\":\"M3 2.5C3 2.22386 3.22386 2 3.5 2H9.08579C9.21839 2 9.34557 2.05268 9.43934 2.14645L11.8536 4.56066C11.9473 4.65443 12 4.78161 12 4.91421V12.5C12 12.7761 11.7761 13 11.5 13H3.5C3.22386 13 3 12.7761 3 12.5V2.5ZM3.5 1C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H11.5C12.3284 14 13 13.3284 13 12.5V4.91421C13 4.51639 12.842 4.13486 12.5607 3.85355L10.1464 1.43934C9.86514 1.15804 9.48361 1 9.08579 1H3.5ZM4.5 4C4.22386 4 4 4.22386 4 4.5C4 4.77614 4.22386 5 4.5 5H7.5C7.77614 5 8 4.77614 8 4.5C8 4.22386 7.77614 4 7.5 4H4.5ZM4.5 7C4.22386 7 4 7.22386 4 7.5C4 7.77614 4.22386 8 4.5 8H10.5C10.7761 8 11 7.77614 11 7.5C11 7.22386 10.7761 7 10.5 7H4.5ZM4.5 10C4.22386 10 4 10.2239 4 10.5C4 10.7761 4.22386 11 4.5 11H10.5C10.7761 11 11 10.7761 11 10.5C11 10.2239 10.7761 10 10.5 10H4.5Z\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}],[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"hidden sm:inline w-3.5 h-3.5 ml-0.5 mt-0.5 -mr-0.5\",\"children\":[\"$\",\"path\",null,{\"d\":\"M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}]]}],[\"$\",\"a\",null,{\"href\":\"https://github.com/openai/openai-cookbook\",\"target\":\"_blank\",\"className\":\"text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 \",\"children\":[[\"$\",\"span\",null,{\"className\":\"hidden sm:inline\",\"children\":\"Contribute\"}],[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-3.5 h-3.5 sm:ml-1.5\",\"children\":[\"$\",\"path\",null,{\"d\":\"$17\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}]]}],[\"$\",\"$L18\",null,{}]]}],[\"$\",\"$L19\",null,{\"index\":[{\"name\":\"Using_logprobs.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Using_logprobs.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Using_logprobs.ipynb\",\"internal_path\":\"examples/using_logprobs\",\"title\":\"Using logprobs\",\"path\":\"examples/Using_logprobs.ipynb\",\"date\":\"2023-12-20\",\"authors\":[{\"name\":\"James Hills\",\"website\":\"https://twitter.com/jamesmhills\",\"avatar\":\"https://pbs.twimg.com/profile_images/1722092156691902464/44FGj7VT_400x400.jpg\",\"login\":\"jhills20\"},{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"}],\"search_text\":\"$1a\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Creating_slides_with_Assistants_API_and_DALL-E3.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb\",\"internal_path\":\"examples/creating_slides_with_assistants_api_and_dall-e3\",\"title\":\"Creating slides with the Assistants API and DALL·E 3\",\"path\":\"examples/Creating_slides_with_Assistants_API_and_DALL-E3.ipynb\",\"date\":\"2023-12-08\",\"authors\":[{\"name\":\"James Hills\",\"website\":\"https://twitter.com/jamesmhills\",\"avatar\":\"https://pbs.twimg.com/profile_images/1722092156691902464/44FGj7VT_400x400.jpg\",\"login\":\"jhills20\"}],\"search_text\":\"$1b\",\"type\":\"ipynb\",\"tags\":[\"assistants\",\"dall-e\"]},{\"name\":\"Chat_finetuning_data_prep.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Chat_finetuning_data_prep.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb\",\"internal_path\":\"examples/chat_finetuning_data_prep\",\"title\":\"Data preparation and analysis for chat model fine-tuning\",\"path\":\"examples/Chat_finetuning_data_prep.ipynb\",\"date\":\"2023-08-22\",\"authors\":[{\"name\":\"Michael Wu\",\"website\":\"https://www.linkedin.com/in/michael-wu-77440977/\",\"avatar\":\"https://media.licdn.com/dms/image/C5603AQFhQfx0I-raUg/profile-displayphoto-shrink_800_800/0/1527451059977?e=1704931200\u0026v=beta\u0026t=wkFOj0Rigp6e9wm7aZdziOQ6jHARyXj3EoK1K8jaaas\",\"login\":\"mwu1993\"},{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"}],\"search_text\":\"$1c\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"fine-tuning\",\"tiktoken\"]},{\"name\":\"Classification_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Classification_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Classification_using_embeddings.ipynb\",\"internal_path\":\"examples/classification_using_embeddings\",\"title\":\"Classification using embeddings\",\"path\":\"examples/Classification_using_embeddings.ipynb\",\"date\":\"2022-07-11\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"$1d\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Clustering.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Clustering.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb\",\"internal_path\":\"examples/clustering\",\"title\":\"Clustering\",\"path\":\"examples/Clustering.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"\\nWe use a simple k-means algorithm to demonstrate how clustering can be done. Clustering can help discover valuable, hidden groupings within the data. The dataset is created in the Get_embeddings_from_dataset Notebook.\\n1. Find the clusters using K-means\\nWe show the simplest use of K-means. You can pick the number of clusters that fits your use case best.\\nVisualization of clusters in a 2d projection. In this run, the green cluster (#1) seems quite different from the others. Let's see a few samples from each cluster.\\n2. Text samples in the clusters \u0026 naming the clusters\\n\\nLet's show random samples from each cluster. We'll use gpt-4 to name the clusters, based on a random sample of 5 reviews from that cluster.\\nIt's important to note that clusters will not necessarily match what you intend to use them for. A larger amount of clusters will focus on more specific patterns, whereas a small number of clusters will usually focus on largest discrepencies in the data.\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Clustering_for_transaction_classification.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Clustering_for_transaction_classification.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Clustering_for_transaction_classification.ipynb\",\"internal_path\":\"examples/clustering_for_transaction_classification\",\"title\":\"Clustering for transaction classification\",\"path\":\"examples/Clustering_for_transaction_classification.ipynb\",\"date\":\"2022-10-20\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$1e\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Code_search_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Code_search_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb\",\"internal_path\":\"examples/code_search_using_embeddings\",\"title\":\"Code search using embeddings\",\"path\":\"examples/Code_search_using_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"},{\"login\":\"eli64s\",\"name\":\"Eli Salamie\",\"avatar\":\"https://avatars.githubusercontent.com/u/43382407?v=4\",\"website\":\"https://github.com/eli64s\"}],\"search_text\":\"$1f\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Customizing_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Customizing_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\",\"internal_path\":\"examples/customizing_embeddings\",\"title\":\"Customizing embeddings\",\"path\":\"examples/Customizing_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$20\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Embedding_Wikipedia_articles_for_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\",\"internal_path\":\"examples/embedding_wikipedia_articles_for_search\",\"title\":\"Embedding Wikipedia articles for search\",\"path\":\"examples/Embedding_Wikipedia_articles_for_search.ipynb\",\"date\":\"2023-04-14\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$21\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Embedding_long_inputs.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Embedding_long_inputs.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb\",\"internal_path\":\"examples/embedding_long_inputs\",\"title\":\"Embedding texts that are longer than the model's maximum context length\",\"path\":\"examples/Embedding_long_inputs.ipynb\",\"date\":\"2023-01-18\",\"authors\":[{\"login\":\"filipeabperes\",\"name\":\"Filipe de Avila Belbute Peres\",\"avatar\":\"https://avatars.githubusercontent.com/u/10751509?v=4\",\"website\":\"https://github.com/filipeabperes\"}],\"search_text\":\"$22\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"tiktoken\"]},{\"name\":\"Entity_extraction_for_long_documents.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Entity_extraction_for_long_documents.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Entity_extraction_for_long_documents.ipynb\",\"internal_path\":\"examples/entity_extraction_for_long_documents\",\"title\":\"Long document content extraction\",\"path\":\"examples/Entity_extraction_for_long_documents.ipynb\",\"date\":\"2023-02-20\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$23\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Fine-tuned_classification.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Fine-tuned_classification.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb\",\"internal_path\":\"examples/fine-tuned_classification\",\"title\":\"Fine tuning classification example\",\"path\":\"examples/Fine-tuned_classification.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$24\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"fine-tuning\"]},{\"name\":\"Function_calling_finding_nearby_places.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Function_calling_finding_nearby_places.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb\",\"internal_path\":\"examples/function_calling_finding_nearby_places\",\"title\":\"Function calling for nearby places: Leveraging the Google Places API and customer profiles\",\"path\":\"examples/Function_calling_finding_nearby_places.ipynb\",\"date\":\"2023-08-11\",\"authors\":[{\"login\":\"prestontuggle\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/97747561?v=4\",\"website\":\"https://github.com/prestontuggle\"}],\"search_text\":\"$25\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"Using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Using_embeddings.ipynb\",\"internal_path\":\"examples/using_embeddings\",\"title\":\"Using embeddings\",\"path\":\"examples/Using_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"},{\"name\":\"Joe Beutler\",\"website\":\"https://joebeutler.com\",\"avatar\":\"https://avatars.githubusercontent.com/u/156261485?v=4\",\"login\":\"jbeutler-openai\"}],\"search_text\":\"\\nThis notebook contains some helpful snippets you can use to embed text with the text-embedding-3-small model via the OpenAI API.\\nIt's recommended to use the 'tenacity' package or another exponential backoff implementation to better manage API rate limits, as hitting the API too much too fast can trigger rate limits. Using the following function ensures you get your embeddings as fast as possible.\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"How_to_build_a_tool-using_agent_with_Langchain.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb\",\"internal_path\":\"examples/how_to_build_a_tool-using_agent_with_langchain\",\"title\":\"How to build a tool-using agent with LangChain\",\"path\":\"examples/How_to_build_a_tool-using_agent_with_Langchain.ipynb\",\"date\":\"2023-05-02\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$26\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"How_to_call_functions_for_knowledge_retrieval.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb\",\"internal_path\":\"examples/how_to_call_functions_for_knowledge_retrieval\",\"title\":\"How to use functions with a knowledge base\",\"path\":\"examples/How_to_call_functions_for_knowledge_retrieval.ipynb\",\"date\":\"2023-06-14\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$27\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"How_to_call_functions_with_chat_models.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_call_functions_with_chat_models.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb\",\"internal_path\":\"examples/how_to_call_functions_with_chat_models\",\"title\":\"How to call functions with chat models\",\"path\":\"examples/How_to_call_functions_with_chat_models.ipynb\",\"date\":\"2023-06-13\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"},{\"name\":\"Joe Palermo\",\"website\":\"https://www.linkedin.com/in/joe-palermo-99219237\",\"avatar\":\"https://media.licdn.com/dms/image/C4E03AQF_tsi7Kom0rg/profile-displayphoto-shrink_800_800/0/1630002100665?e=1704931200\u0026v=beta\u0026t=z3HKO9FmGHJIxhes9TXRzw-8iY-CBsEZYZc8zTogiLU\",\"login\":\"joe-at-openai\"}],\"search_text\":\"$28\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"How_to_count_tokens_with_tiktoken.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_count_tokens_with_tiktoken.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\",\"internal_path\":\"examples/how_to_count_tokens_with_tiktoken\",\"title\":\"How to count tokens with Tiktoken\",\"path\":\"examples/How_to_count_tokens_with_tiktoken.ipynb\",\"date\":\"2022-12-16\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$29\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"tiktoken\"]},{\"name\":\"How_to_finetune_chat_models.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_finetune_chat_models.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb\",\"internal_path\":\"examples/how_to_finetune_chat_models\",\"title\":\"How to fine-tune chat models\",\"path\":\"examples/How_to_finetune_chat_models.ipynb\",\"date\":\"2024-07-23\",\"authors\":[{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"},{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"}],\"search_text\":\"$2a\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"fine-tuning\"]},{\"name\":\"How_to_format_inputs_to_ChatGPT_models.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\",\"internal_path\":\"examples/how_to_format_inputs_to_chatgpt_models\",\"title\":\"How to format inputs to ChatGPT models\",\"path\":\"examples/How_to_format_inputs_to_ChatGPT_models.ipynb\",\"date\":\"2023-03-01\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$2b\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"tiktoken\"]},{\"name\":\"How_to_handle_rate_limits.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_handle_rate_limits.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\",\"internal_path\":\"examples/how_to_handle_rate_limits\",\"title\":\"How to handle rate limits\",\"path\":\"examples/How_to_handle_rate_limits.ipynb\",\"date\":\"2022-09-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$2c\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"How_to_stream_completions.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_stream_completions.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb\",\"internal_path\":\"examples/how_to_stream_completions\",\"title\":\"How to stream completions\",\"path\":\"examples/How_to_stream_completions.ipynb\",\"date\":\"2022-09-02\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$2d\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Multiclass_classification_for_transactions.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Multiclass_classification_for_transactions.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Multiclass_classification_for_transactions.ipynb\",\"internal_path\":\"examples/multiclass_classification_for_transactions\",\"title\":\"Multiclass Classification for Transactions\",\"path\":\"examples/Multiclass_classification_for_transactions.ipynb\",\"date\":\"2022-10-20\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$2e\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Get_embeddings_from_dataset.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Get_embeddings_from_dataset.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings_from_dataset.ipynb\",\"internal_path\":\"examples/get_embeddings_from_dataset\",\"title\":\"Get embeddings from dataset\",\"path\":\"examples/Get_embeddings_from_dataset.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"\\nThis notebook gives an example on how to get embeddings from a large dataset.\\n\\n1. Load the dataset\\n\\nThe dataset used in this example is fine-food reviews from Amazon. The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of this dataset, consisting of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).\\n\\nWe will combine the review summary and review text into a single combined text. The model will encode this combined text and it will output a single vector embedding.\\nTo run this notebook, you will need to install: pandas, openai, transformers, plotly, matplotlib, scikit-learn, torch (transformer dep), torchvision, and scipy.\\n2. Get embeddings and save them for future reuse\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Question_answering_using_a_search_API.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Question_answering_using_a_search_API.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb\",\"internal_path\":\"examples/question_answering_using_a_search_api\",\"title\":\"Question answering using a search API and re-ranking\",\"path\":\"examples/Question_answering_using_a_search_API.ipynb\",\"date\":\"2023-06-16\",\"authors\":[{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$2f\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Question_answering_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Question_answering_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\",\"internal_path\":\"examples/question_answering_using_embeddings\",\"title\":\"Question answering using embeddings-based search\",\"path\":\"examples/Question_answering_using_embeddings.ipynb\",\"date\":\"2022-06-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"MikeHeaton\",\"name\":\"Mike Heaton\",\"avatar\":\"https://avatars.githubusercontent.com/u/11911723?v=4\",\"website\":\"https://github.com/MikeHeaton\"}],\"search_text\":\"$30\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Recommendation_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Recommendation_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Recommendation_using_embeddings.ipynb\",\"internal_path\":\"examples/recommendation_using_embeddings\",\"title\":\"Recommendation using embeddings and nearest neighbor search\",\"path\":\"examples/Recommendation_using_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"$31\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Regression_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Regression_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Regression_using_embeddings.ipynb\",\"internal_path\":\"examples/regression_using_embeddings\",\"title\":\"Regression using the embeddings\",\"path\":\"examples/Regression_using_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"\\nRegression means predicting a number, rather than one of the categories. We will predict the score based on the embedding of the review's text. We split the dataset into a training and a testing set for all of the following tasks, so we can realistically evaluate performance on unseen data. The dataset is created in the Get_embeddings_from_dataset Notebook.\\n\\nWe're predicting the score of the review, which is a number between 1 and 5 (1-star being negative and 5-star positive).\\nWe can see that the embeddings are able to predict the scores with an average error of 0.53 per score prediction. This is roughly equivalent to predicting half of reviews perfectly, and half off by one star.\\nYou could also train a classifier to predict the label, or use the embeddings within an existing ML model to encode free text features.\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Search_reranking_with_cross-encoders.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Search_reranking_with_cross-encoders.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Search_reranking_with_cross-encoders.ipynb\",\"internal_path\":\"examples/search_reranking_with_cross-encoders\",\"title\":\"Search reranking with cross-encoders\",\"path\":\"examples/Search_reranking_with_cross-encoders.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$32\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Semantic_text_search_using_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Semantic_text_search_using_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb\",\"internal_path\":\"examples/semantic_text_search_using_embeddings\",\"title\":\"Semantic text search using embeddings\",\"path\":\"examples/Semantic_text_search_using_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"\\nWe can search through all our reviews semantically in a very efficient manner and at very low cost, by embedding our search query, and then finding the most similar reviews. The dataset is created in the Get_embeddings_from_dataset Notebook.\\nHere we compare the cosine similarity of the embeddings of the query and the documents, and show top_n best matches.\\nWe can search through these reviews easily. To speed up computation, we can use a special algorithm, aimed at faster search through embeddings.\\nAs we can see, this can immediately deliver a lot of value. In this example we show being able to quickly find the examples of delivery failures.\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Unit_test_writing_using_a_multi-step_prompt.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt.ipynb\",\"internal_path\":\"examples/unit_test_writing_using_a_multi-step_prompt\",\"title\":\"Unit test writing using a multi-step prompt\",\"path\":\"examples/Unit_test_writing_using_a_multi-step_prompt.ipynb\",\"date\":\"2022-11-15\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$33\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb\",\"internal_path\":\"examples/unit_test_writing_using_a_multi-step_prompt_with_older_completions_api\",\"title\":\"Unit test writing using a multi-step prompt with legacy Completions\",\"path\":\"examples/Unit_test_writing_using_a_multi-step_prompt_with_older_completions_API.ipynb\",\"date\":\"2023-05-19\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$34\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"User_and_product_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/User_and_product_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/User_and_product_embeddings.ipynb\",\"internal_path\":\"examples/user_and_product_embeddings\",\"title\":\"User and product embeddings\",\"path\":\"examples/User_and_product_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$35\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Visualizing_embeddings_in_2D.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Visualizing_embeddings_in_2D.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_2D.ipynb\",\"internal_path\":\"examples/visualizing_embeddings_in_2d\",\"title\":\"Visualizing the embeddings in 2D\",\"path\":\"examples/Visualizing_embeddings_in_2D.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"\\nWe will use t-SNE to reduce the dimensionality of the embeddings from 1536 to 2. Once the embeddings are reduced to two dimensions, we can plot them in a 2D scatter plot. The dataset is created in the Get_embeddings_from_dataset Notebook.\\n1. Reduce dimensionality\\n\\nWe reduce the dimensionality to 2 dimensions using t-SNE decomposition.\\n2. Plotting the embeddings\\n\\nWe colour each review by its star rating, ranging from red to green.\\nWe can observe a decent data separation even in the reduced 2 dimensions.\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Visualizing_embeddings_in_3D.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Visualizing_embeddings_in_3D.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb\",\"internal_path\":\"examples/visualizing_embeddings_in_3d\",\"title\":\"Visualizing embeddings in 3D\",\"path\":\"examples/Visualizing_embeddings_in_3D.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"The example uses PCA to reduce the dimensionality fo the embeddings from 1536 to 3. Then we can visualize the data points in a 3D plot. The small dataset dbpedia_samples.jsonl is curated by randomly sampling 200 samples from DBpedia validation dataset.\\n1. Load the dataset and query embeddings\\n2. Reduce the embedding dimensionality\\n3. Plot the embeddings of lower dimensionality\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Visualizing_embeddings_in_wandb.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/Visualizing_embeddings_in_wandb.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_wandb.ipynb\",\"internal_path\":\"examples/third_party/visualizing_embeddings_in_wandb\",\"title\":\"Visualizing embeddings in Weights and Biases\",\"path\":\"examples/third_party/Visualizing_embeddings_in_wandb.ipynb\",\"date\":\"2023-02-01\",\"authors\":[{\"login\":\"scottire\",\"name\":\"Scott Condron\",\"avatar\":\"https://avatars.githubusercontent.com/u/8557070?v=4\",\"website\":\"https://github.com/scottire\"}],\"search_text\":\"\\nWe will upload the data to Weights \u0026 Biases and use an Embedding Projector to visualize the embeddings using common dimension reduction algorithms like PCA, UMAP, and t-SNE. The dataset is created in the Get_embeddings_from_dataset Notebook.\\nWhat is Weights \u0026 Biases?\\n\\nWeights \u0026 Biases is a machine learning platform used by OpenAI and other ML teams to build better models faster. They use it to quickly track experiments, evaluate model performance, reproduce models, visualize results, and share findings with colleagues.\\n1. Log the data to W\u0026B\\n\\nWe create a W\u0026B Table with the original data and the embeddings. Each review is a new row and the 1536 embedding floats are given their own column named emb_{i}.\\n2. Render as 2D Projection\\nAfter navigating to the W\u0026B run link, we click the ⚙️ icon in the top right of the Table and change \\\"Render As:\\\" to \\\"Combined 2D Projection\\\".\\nExample: http://wandb.me/openai_embeddings\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Visualizing_embeddings_with_Atlas.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_with_Atlas.ipynb\",\"internal_path\":\"examples/third_party/visualizing_embeddings_with_atlas\",\"title\":\"Visualizing embeddings in Atlas\",\"path\":\"examples/third_party/Visualizing_embeddings_with_Atlas.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"AndriyMulyar\",\"name\":\"Andriy Mulyar\",\"avatar\":\"https://avatars.githubusercontent.com/u/13879686?v=4\",\"website\":\"https://github.com/AndriyMulyar\"},{\"login\":\"TDulka\",\"name\":\"Tomas Dulka\",\"avatar\":\"https://avatars.githubusercontent.com/u/26122417?v=4\",\"website\":\"https://github.com/TDulka\"}],\"search_text\":\"\\nIn this example, we will upload food review embeddings to Atlas to visualize the embeddings.\\nWhat is Atlas?\\n\\nAtlas is a machine learning tool used to visualize massive datasets of embeddings in your web browser. Upload millions of embeddings to Atlas and interact with them in your web browser or jupyter notebook.\\n1. Login to Atlas.\\n\\n2. Interact with your embeddings in Jupyter\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Whisper_correct_misspelling.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Whisper_correct_misspelling.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_correct_misspelling.ipynb\",\"internal_path\":\"examples/whisper_correct_misspelling\",\"title\":\"Addressing transcription misspellings: prompt vs post-processing\",\"path\":\"examples/Whisper_correct_misspelling.ipynb\",\"date\":\"2023-08-11\",\"authors\":[{\"login\":\"prestontuggle\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/97747561?v=4\",\"website\":\"https://github.com/prestontuggle\"}],\"search_text\":\"$36\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"whisper\"]},{\"name\":\"Whisper_processing_guide.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Whisper_processing_guide.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb\",\"internal_path\":\"examples/whisper_processing_guide\",\"title\":\"Enhancing Whisper transcriptions: pre- \u0026 post-processing techniques\",\"path\":\"examples/Whisper_processing_guide.ipynb\",\"date\":\"2023-08-11\",\"authors\":[{\"login\":\"prestontuggle\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/97747561?v=4\",\"website\":\"https://github.com/prestontuggle\"}],\"search_text\":\"$37\",\"type\":\"ipynb\",\"tags\":[\"whisper\"]},{\"name\":\"Whisper_prompting_guide.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Whisper_prompting_guide.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb\",\"internal_path\":\"examples/whisper_prompting_guide\",\"title\":\"Whisper prompting guide\",\"path\":\"examples/Whisper_prompting_guide.ipynb\",\"date\":\"2023-06-27\",\"authors\":[{\"login\":\"prestontuggle\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/97747561?v=4\",\"website\":\"https://github.com/prestontuggle\"}],\"search_text\":\"$38\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"whisper\"]},{\"name\":\"Zero-shot_classification_with_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Zero-shot_classification_with_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb\",\"internal_path\":\"examples/zero-shot_classification_with_embeddings\",\"title\":\"Zero-shot classification with embeddings\",\"path\":\"examples/Zero-shot_classification_with_embeddings.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"},{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"logankilpatrick\",\"name\":\"Logan Kilpatrick \",\"avatar\":\"https://avatars.githubusercontent.com/u/35577566?v=4\",\"website\":\"https://github.com/logankilpatrick\"}],\"search_text\":\"$39\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"chat.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/azure/chat.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat.ipynb\",\"internal_path\":\"examples/azure/chat\",\"title\":\"Azure Chat Completions example (preview)\",\"path\":\"examples/azure/chat.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"cmurtz-msft\",\"name\":\"Christian Mürtz\",\"avatar\":\"https://avatars.githubusercontent.com/u/120655914?v=4\",\"website\":\"https://github.com/cmurtz-msft\"},{\"login\":\"glecaros\",\"name\":\"Gerardo Lecaros\",\"avatar\":\"https://avatars.githubusercontent.com/u/10088504?v=4\",\"website\":\"https://github.com/glecaros\"},{\"login\":\"kristapratico\",\"name\":\"Krista Pratico\",\"avatar\":\"https://avatars.githubusercontent.com/u/31998003?v=4\",\"website\":\"https://github.com/kristapratico\"}],\"search_text\":\"$3a\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"chat_with_your_own_data.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/azure/chat_with_your_own_data.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb\",\"internal_path\":\"examples/azure/chat_with_your_own_data\",\"title\":\"Azure Chat Completion models with your own data (preview)\",\"path\":\"examples/azure/chat_with_your_own_data.ipynb\",\"date\":\"2023-09-11\",\"authors\":[{\"login\":\"kristapratico\",\"name\":\"Krista Pratico\",\"avatar\":\"https://avatars.githubusercontent.com/u/31998003?v=4\",\"website\":\"https://github.com/kristapratico\"}],\"search_text\":\"$3b\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/azure/embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/azure/embeddings.ipynb\",\"internal_path\":\"examples/azure/embeddings\",\"title\":\"Azure embeddings example\",\"path\":\"examples/azure/embeddings.ipynb\",\"date\":\"2022-07-12\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"cmurtz-msft\",\"name\":\"Christian Mürtz\",\"avatar\":\"https://avatars.githubusercontent.com/u/120655914?v=4\",\"website\":\"https://github.com/cmurtz-msft\"},{\"login\":\"glecaros\",\"name\":\"Gerardo Lecaros\",\"avatar\":\"https://avatars.githubusercontent.com/u/10088504?v=4\",\"website\":\"https://github.com/glecaros\"},{\"login\":\"kristapratico\",\"name\":\"Krista Pratico\",\"avatar\":\"https://avatars.githubusercontent.com/u/31998003?v=4\",\"website\":\"https://github.com/kristapratico\"}],\"search_text\":\"$3c\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"functions.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/azure/functions.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/azure/functions.ipynb\",\"internal_path\":\"examples/azure/functions\",\"title\":\"Azure functions example\",\"path\":\"examples/azure/functions.ipynb\",\"date\":\"2023-07-21\",\"authors\":[{\"login\":\"kristapratico\",\"name\":\"Krista Pratico\",\"avatar\":\"https://avatars.githubusercontent.com/u/31998003?v=4\",\"website\":\"https://github.com/kristapratico\"}],\"search_text\":\"$3d\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"translate_latex_book.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/book_translation/translate_latex_book.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/book_translation/translate_latex_book.ipynb\",\"internal_path\":\"examples/book_translation/translate_latex_book\",\"title\":\"Translate a book writen in LaTeX from Slovenian into English\",\"path\":\"examples/book_translation/translate_latex_book.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$3e\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"tiktoken\"]},{\"name\":\"How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\",\"internal_path\":\"examples/dalle/how_to_create_dynamic_masks_with_dall-e_and_segment_anything\",\"title\":\"How to create dynamic masks with DALL·E and Segment Anything\",\"path\":\"examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\",\"date\":\"2023-05-19\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$3f\",\"type\":\"ipynb\",\"tags\":[\"dall-e\"]},{\"name\":\"Image_generations_edits_and_variations_with_DALL-E.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb\",\"internal_path\":\"examples/dalle/image_generations_edits_and_variations_with_dall-e\",\"title\":\"How to use the DALL·E API\",\"path\":\"examples/dalle/Image_generations_edits_and_variations_with_DALL-E.ipynb\",\"date\":\"2022-11-04\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$40\",\"type\":\"ipynb\",\"tags\":[\"dall-e\"]},{\"name\":\"How_to_eval_abstractive_summarization.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb\",\"internal_path\":\"examples/evaluation/how_to_eval_abstractive_summarization\",\"title\":\"How to evaluate a summarization task\",\"path\":\"examples/evaluation/How_to_eval_abstractive_summarization.ipynb\",\"date\":\"2023-08-16\",\"authors\":[{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"},{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"}],\"search_text\":\"$41\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Getting_Started_with_OpenAI_Evals.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb\",\"internal_path\":\"examples/evaluation/getting_started_with_openai_evals\",\"title\":\"Getting Started with OpenAI Evals\",\"path\":\"examples/evaluation/Getting_Started_with_OpenAI_Evals.ipynb\",\"date\":\"2024-03-21\",\"authors\":[{\"name\":\"Roy Ziv\",\"website\":\"https://www.linkedin.com/in/roy-ziv-a46001149/\",\"avatar\":\"https://media.licdn.com/dms/image/D5603AQHkaEOOGZWtbA/profile-displayphoto-shrink_200_200/0/1699500606122?e=1723075200\u0026v=beta\u0026t=2h6JQpVYm0YncvJRCxCHMB-PmGLg2m3UNbfOB0Yc8rM\",\"login\":\"royziv11\"},{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"}],\"search_text\":\"$42\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Developing_hallucination_guardrails.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Developing_hallucination_guardrails.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Developing_hallucination_guardrails.ipynb\",\"internal_path\":\"examples/developing_hallucination_guardrails\",\"title\":\"Developing Hallucination Guardrails\",\"path\":\"examples/Developing_hallucination_guardrails.ipynb\",\"date\":\"2024-05-29\",\"authors\":[{\"name\":\"Roy Ziv\",\"website\":\"https://www.linkedin.com/in/roy-ziv-a46001149/\",\"avatar\":\"https://media.licdn.com/dms/image/D5603AQHkaEOOGZWtbA/profile-displayphoto-shrink_200_200/0/1699500606122?e=1723075200\u0026v=beta\u0026t=2h6JQpVYm0YncvJRCxCHMB-PmGLg2m3UNbfOB0Yc8rM\",\"login\":\"royziv11\"}],\"search_text\":\"$43\",\"type\":\"ipynb\",\"tags\":[\"guardrails\"]},{\"name\":\"olympics-1-collect-data.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-1-collect-data.ipynb\",\"internal_path\":\"examples/fine-tuned_qa/olympics-1-collect-data\",\"title\":\"Fine-Tuned Q\u0026A - collect data\",\"path\":\"examples/fine-tuned_qa/olympics-1-collect-data.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$44\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"fine-tuning\"]},{\"name\":\"olympics-2-create-qa.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-2-create-qa.ipynb\",\"internal_path\":\"examples/fine-tuned_qa/olympics-2-create-qa\",\"title\":\"Fine-Tuned Q\u0026A - create Q\u0026A\",\"path\":\"examples/fine-tuned_qa/olympics-2-create-qa.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$45\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"fine-tuning\"]},{\"name\":\"olympics-3-train-qa.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/olympics-3-train-qa.ipynb\",\"internal_path\":\"examples/fine-tuned_qa/olympics-3-train-qa\",\"title\":\"Fine-Tuned Q\u0026A - train\",\"path\":\"examples/fine-tuned_qa/olympics-3-train-qa.ipynb\",\"date\":\"2022-03-10\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"login\":\"BorisPower\",\"name\":\"Boris Power\",\"avatar\":\"https://avatars.githubusercontent.com/u/81998504?v=4\",\"website\":\"https://github.com/BorisPower\"}],\"search_text\":\"$46\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"fine-tuning\"]},{\"name\":\"Visualizing_embeddings_in_Kangas.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Visualizing_embeddings_in_Kangas.ipynb\",\"internal_path\":\"examples/third_party/visualizing_embeddings_in_kangas\",\"title\":\"Visualizing the embeddings in Kangas\",\"path\":\"examples/third_party/Visualizing_embeddings_in_Kangas.ipynb\",\"date\":\"2023-07-11\",\"authors\":[{\"login\":\"dsblank\",\"name\":\"Douglas Blank\",\"avatar\":\"https://avatars.githubusercontent.com/u/168568?v=4\",\"website\":\"https://github.com/dsblank\"}],\"search_text\":\"$47\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"financial_document_analysis_with_llamaindex.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb\",\"internal_path\":\"examples/third_party/financial_document_analysis_with_llamaindex\",\"title\":\"Financial document analysis with LlamaIndex\",\"path\":\"examples/third_party/financial_document_analysis_with_llamaindex.ipynb\",\"date\":\"2023-06-22\",\"authors\":[{\"login\":\"Disiok\",\"name\":\"Simon Suo\",\"avatar\":\"https://avatars.githubusercontent.com/u/5567282?v=4\",\"website\":\"https://github.com/Disiok\"}],\"search_text\":\"$48\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/README.md\",\"internal_path\":\"examples/vector_databases/readme\",\"title\":\"Vector databases\",\"path\":\"examples/vector_databases/README.md\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"},{\"login\":\"moizsajid\",\"name\":\"Moiz Sajid\",\"avatar\":\"https://avatars.githubusercontent.com/u/5734680?v=4\",\"website\":\"https://github.com/moizsajid\"}],\"search_text\":\"\\nThis section of the OpenAI Cookbook showcases many of the vector databases available to support your semantic search use cases.\\n\\nVector databases can be a great accompaniment for knowledge retrieval applications, which reduce hallucinations by providing the LLM with the relevant context to answer questions.\\n\\nEach provider has their own named directory, with a standard notebook to introduce you to using our API with their product, and any supplementary notebooks they choose to add to showcase their functionality.\\n\\nGuides \u0026 deep dives\\nAnalyticDB\\nCassandra/Astra DB\\nAzure AI Search\\nAzure SQL Database\\nChroma\\nElasticsearch\\nHologres\\nKusto\\nMilvus\\nMyScale\\nMongoDB\\nNeon Postgres\\nPinecone\\nPolarDB\\nQdrant\\nRedis\\nSingleStoreDB\\nSupabase\\nTembo\\nTypesense\\nVespa AI\\nWeaviate\\nZilliz\\n\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_PolarDB_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/polardb/getting_started_with_polardb_and_openai\",\"title\":\"Using PolarDB-PG as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/PolarDB/Getting_started_with_PolarDB_and_OpenAI.ipynb\",\"date\":\"2023-07-11\",\"authors\":[{\"login\":\"liuchengshan-lcs\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/135313292?v=4\",\"website\":\"https://github.com/liuchengshan-lcs\"}],\"search_text\":\"$49\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"OpenAI_wikipedia_semantic_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb\",\"internal_path\":\"examples/vector_databases/singlestoredb/openai_wikipedia_semantic_search\",\"title\":\"Semantic search with SingleStoreDB\",\"path\":\"examples/vector_databases/SingleStoreDB/OpenAI_wikipedia_semantic_search.ipynb\",\"date\":\"2023-05-22\",\"authors\":[{\"login\":\"arno756\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/8846480?v=4\",\"website\":\"https://github.com/arno756\"}],\"search_text\":\"This notebook is an example on how you can use SingleStoreDB vector storage and functions to build an interactive Q\u0026A application with ChatGPT. If you start a Trial in SingleStoreDB, you can find the same notebook in our sample notebooks with native connection.\\nFirst let's talk directly to ChatGPT and try and get back a response\\nLet's connect to OpenAI and see the result we get when asking for a date beyond 2021\\nGet the data about Winter Olympics and provide the information to ChatGPT as context\\n1. Setup\\nStep 1 - Grab the data from CSV and prepare it\\n2. Set up SingleStore DB\\n3. Populate the Table with our dataframe df and use JSON_ARRAY_PACK to compact it\\n4. Do a semantic search with the same question from above and use the response to send to OpenAI again\\n\\n5. Send the right context to ChatGPT for a more accurate answer\\n6. Get an answer from Chat GPT\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/SingleStoreDB/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/SingleStoreDB/README.md\",\"internal_path\":\"examples/vector_databases/singlestoredb/readme\",\"title\":\"SingleStoreDB\",\"path\":\"examples/vector_databases/SingleStoreDB/README.md\",\"date\":\"2023-05-22\",\"authors\":[{\"login\":\"arno756\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/8846480?v=4\",\"website\":\"https://github.com/arno756\"}],\"search_text\":\"$4a\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Getting_started_with_AnalyticDB_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/analyticdb/getting_started_with_analyticdb_and_openai\",\"title\":\"Using AnalyticDB as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/analyticdb/Getting_started_with_AnalyticDB_and_OpenAI.ipynb\",\"date\":\"2023-04-06\",\"authors\":[{\"login\":\"wangxuqi\",\"name\":\"Richy Wang\",\"avatar\":\"https://avatars.githubusercontent.com/u/13748374?v=4\",\"website\":\"https://github.com/wangxuqi\"}],\"search_text\":\"$4b\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/analyticdb/qa_with_langchain_analyticdb_and_openai\",\"title\":\"Question answering with Langchain, AnalyticDB and OpenAI\",\"path\":\"examples/vector_databases/analyticdb/QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb\",\"date\":\"2023-05-05\",\"authors\":[{\"login\":\"wangxuqi\",\"name\":\"Richy Wang\",\"avatar\":\"https://avatars.githubusercontent.com/u/13748374?v=4\",\"website\":\"https://github.com/wangxuqi\"}],\"search_text\":\"$4c\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"tiktoken\"]},{\"name\":\"Getting_started_with_azure_ai_search_and_openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb\",\"internal_path\":\"examples/vector_databases/azuresearch/getting_started_with_azure_ai_search_and_openai\",\"title\":\"Azure AI Search as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/azuresearch/Getting_started_with_azure_ai_search_and_openai.ipynb\",\"date\":\"2023-09-11\",\"authors\":[{\"login\":\"farzad528\",\"name\":\"Farzad Sunavala\",\"avatar\":\"https://avatars.githubusercontent.com/u/40604067?v=4\",\"website\":\"https://github.com/farzad528\"}],\"search_text\":\"$4d\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Philosophical_Quotes_CQL.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb\",\"internal_path\":\"examples/vector_databases/cassandra_astradb/philosophical_quotes_cql\",\"title\":\"Philosophy with vector embeddings, OpenAI and Cassandra / Astra DB\",\"path\":\"examples/vector_databases/cassandra_astradb/Philosophical_Quotes_CQL.ipynb\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"hemidactylus\",\"name\":\"Stefano Lottini\",\"avatar\":\"https://avatars.githubusercontent.com/u/14221764?v=4\",\"website\":\"https://github.com/hemidactylus\"}],\"search_text\":\"$4e\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Philosophical_Quotes_cassIO.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb\",\"internal_path\":\"examples/vector_databases/cassandra_astradb/philosophical_quotes_cassio\",\"title\":\"Philosophy with vector embeddings, OpenAI and Cassandra / Astra DB\",\"path\":\"examples/vector_databases/cassandra_astradb/Philosophical_Quotes_cassIO.ipynb\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"hemidactylus\",\"name\":\"Stefano Lottini\",\"avatar\":\"https://avatars.githubusercontent.com/u/14221764?v=4\",\"website\":\"https://github.com/hemidactylus\"}],\"search_text\":\"$4f\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/cassandra_astradb/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/cassandra_astradb/README.md\",\"internal_path\":\"examples/vector_databases/cassandra_astradb/readme\",\"title\":\"Cassandra / Astra DB\",\"path\":\"examples/vector_databases/cassandra_astradb/README.md\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"hemidactylus\",\"name\":\"Stefano Lottini\",\"avatar\":\"https://avatars.githubusercontent.com/u/14221764?v=4\",\"website\":\"https://github.com/hemidactylus\"}],\"search_text\":\"$50\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_Chroma_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/chroma/using_chroma_for_embeddings_search\",\"title\":\"Using Chroma for embeddings search\",\"path\":\"examples/vector_databases/chroma/Using_Chroma_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"},{\"login\":\"atroyn\",\"name\":\"Anton Troynikov\",\"avatar\":\"https://avatars.githubusercontent.com/u/1302641?v=4\",\"website\":\"https://github.com/atroyn\"}],\"search_text\":\"$51\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"hyde-with-chroma-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/chroma/hyde-with-chroma-and-openai\",\"title\":\"Robust question answering with Chroma and OpenAI\",\"path\":\"examples/vector_databases/chroma/hyde-with-chroma-and-openai.ipynb\",\"date\":\"2023-04-06\",\"authors\":[{\"login\":\"atroyn\",\"name\":\"Anton Troynikov\",\"avatar\":\"https://avatars.githubusercontent.com/u/1302641?v=4\",\"website\":\"https://github.com/atroyn\"}],\"search_text\":\"$52\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/elasticsearch/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/README.md\",\"internal_path\":\"examples/vector_databases/elasticsearch/readme\",\"title\":\"Elasticsearch\",\"path\":\"examples/vector_databases/elasticsearch/README.md\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"leemthompo\",\"name\":\"Liam Thompson\",\"avatar\":\"https://avatars.githubusercontent.com/u/32779855?v=4\",\"website\":\"https://github.com/leemthompo\"}],\"search_text\":\"\\nElasticsearch is a popular search/analytics engine and vector database.\\nElasticsearch offers an efficient way to create, store, and search vector embeddings at scale.\\n\\nFor technical details, refer to the Elasticsearch documentation.\\n\\nThe elasticsearch-labs repo contains executable Python notebooks, sample apps, and resources for testing out the Elastic platform.\\n\\nOpenAI cookbook notebooks 📒\\n\\nCheck out our notebooks in this repo for working with OpenAI, using Elasticsearch as your vector database.\\n\\nSemantic search\\n\\nIn this notebook you'll learn how to:\\n\\n Index the OpenAI Wikipedia embeddings dataset into Elasticsearch\\n Encode a question with the openai ada-02 model\\n Perform a semantic search\\n\\nRetrieval augmented generation\\n\\nThis notebooks builds on the semantic search notebook by:\\n\\nSelecting the top hit from a semantic search\\nSending that result to the OpenAI Chat Completions API endpoint for retrieval augmented generation (RAG)\\n\\n\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"elasticsearch-retrieval-augmented-generation.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb\",\"internal_path\":\"examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation\",\"title\":\"Retrieval augmented generation using Elasticsearch and OpenAI\",\"path\":\"examples/vector_databases/elasticsearch/elasticsearch-retrieval-augmented-generation.ipynb\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"leemthompo\",\"name\":\"Liam Thompson\",\"avatar\":\"https://avatars.githubusercontent.com/u/32779855?v=4\",\"website\":\"https://github.com/leemthompo\"}],\"search_text\":\"$53\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"elasticsearch-semantic-search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb\",\"internal_path\":\"examples/vector_databases/elasticsearch/elasticsearch-semantic-search\",\"title\":\"Semantic search using Elasticsearch and OpenAI\",\"path\":\"examples/vector_databases/elasticsearch/elasticsearch-semantic-search.ipynb\",\"date\":\"2023-08-29\",\"authors\":[{\"login\":\"leemthompo\",\"name\":\"Liam Thompson\",\"avatar\":\"https://avatars.githubusercontent.com/u/32779855?v=4\",\"website\":\"https://github.com/leemthompo\"}],\"search_text\":\"$54\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Getting_started_with_Hologres_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/hologres/getting_started_with_hologres_and_openai\",\"title\":\"Using Hologres as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/hologres/Getting_started_with_Hologres_and_OpenAI.ipynb\",\"date\":\"2023-05-19\",\"authors\":[{\"login\":\"zcgeng\",\"name\":\"Changgeng Zhao\",\"avatar\":\"https://avatars.githubusercontent.com/u/13724617?v=4\",\"website\":\"https://github.com/zcgeng\"}],\"search_text\":\"$55\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_kusto_and_openai_embeddings.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb\",\"internal_path\":\"examples/vector_databases/kusto/getting_started_with_kusto_and_openai_embeddings\",\"title\":\"Kusto as a vector database for embeddings\",\"path\":\"examples/vector_databases/kusto/Getting_started_with_kusto_and_openai_embeddings.ipynb\",\"date\":\"2023-05-10\",\"authors\":[{\"login\":\"Anshul Sharma\"}],\"search_text\":\"$56\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/kusto/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/kusto/README.md\",\"internal_path\":\"examples/vector_databases/kusto/readme\",\"title\":\"Kusto as a vector database\",\"path\":\"examples/vector_databases/kusto/README.md\",\"date\":\"2023-05-10\",\"authors\":[{\"login\":\"Anshul Sharma\"}],\"search_text\":\"$57\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"Filtered_search_with_Milvus_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/milvus/filtered_search_with_milvus_and_openai\",\"title\":\"Filtered search with Milvus and OpenAI\",\"path\":\"examples/vector_databases/milvus/Filtered_search_with_Milvus_and_OpenAI.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"filip-halt\",\"name\":\"Filip Haltmayer\",\"avatar\":\"https://avatars.githubusercontent.com/u/81822489?v=4\",\"website\":\"https://github.com/filip-halt\"}],\"search_text\":\"$58\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_Milvus_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/milvus/getting_started_with_milvus_and_openai\",\"title\":\"Getting started with Milvus and OpenAI\",\"path\":\"examples/vector_databases/milvus/Getting_started_with_Milvus_and_OpenAI.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"filip-halt\",\"name\":\"Filip Haltmayer\",\"avatar\":\"https://avatars.githubusercontent.com/u/81822489?v=4\",\"website\":\"https://github.com/filip-halt\"}],\"search_text\":\"$59\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_MyScale_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/myscale/getting_started_with_myscale_and_openai\",\"title\":\"Using MyScale as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/myscale/Getting_started_with_MyScale_and_OpenAI.ipynb\",\"date\":\"2023-05-01\",\"authors\":[{\"login\":\"melovy\",\"name\":\"qingdi\",\"avatar\":\"https://avatars.githubusercontent.com/u/3429899?v=4\",\"website\":\"https://github.com/melovy\"}],\"search_text\":\"$5a\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_MyScale_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/myscale/using_myscale_for_embeddings_search\",\"title\":\"Using MyScale for embeddings search\",\"path\":\"examples/vector_databases/myscale/Using_MyScale_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$5b\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"GPT4_Retrieval_Augmentation.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb\",\"internal_path\":\"examples/vector_databases/pinecone/gpt4_retrieval_augmentation\",\"title\":\"Retrieval augmentation for GPT-4 using Pinecone\",\"path\":\"examples/vector_databases/pinecone/GPT4_Retrieval_Augmentation.ipynb\",\"date\":\"2023-03-24\",\"authors\":[{\"login\":\"jamescalam\",\"name\":\"James Briggs\",\"avatar\":\"https://avatars.githubusercontent.com/u/35938317?v=4\",\"website\":\"https://github.com/jamescalam\"}],\"search_text\":\"$5c\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"tiktoken\"]},{\"name\":\"Gen_QA.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/pinecone/Gen_QA.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb\",\"internal_path\":\"examples/vector_databases/pinecone/gen_qa\",\"title\":\"Retrieval augmented generative question answering with Pinecone\",\"path\":\"examples/vector_databases/pinecone/Gen_QA.ipynb\",\"date\":\"2023-02-07\",\"authors\":[{\"login\":\"jamescalam\",\"name\":\"James Briggs\",\"avatar\":\"https://avatars.githubusercontent.com/u/35938317?v=4\",\"website\":\"https://github.com/jamescalam\"}],\"search_text\":\"$5d\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/pinecone/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/README.md\",\"internal_path\":\"examples/vector_databases/pinecone/readme\",\"title\":\"Pinecone vector database\",\"path\":\"examples/vector_databases/pinecone/README.md\",\"date\":\"2023-03-24\",\"authors\":[{\"login\":\"jamescalam\",\"name\":\"James Briggs\",\"avatar\":\"https://avatars.githubusercontent.com/u/35938317?v=4\",\"website\":\"https://github.com/jamescalam\"}],\"search_text\":\"$5e\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Semantic_Search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/pinecone/Semantic_Search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Semantic_Search.ipynb\",\"internal_path\":\"examples/vector_databases/pinecone/semantic_search\",\"title\":\"Semantic search with Pinecone and OpenAI\",\"path\":\"examples/vector_databases/pinecone/Semantic_Search.ipynb\",\"date\":\"2023-03-24\",\"authors\":[{\"login\":\"jamescalam\",\"name\":\"James Briggs\",\"avatar\":\"https://avatars.githubusercontent.com/u/35938317?v=4\",\"website\":\"https://github.com/jamescalam\"}],\"search_text\":\"$5f\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_Pinecone_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/pinecone/using_pinecone_for_embeddings_search\",\"title\":\"Using Pinecone for embeddings search\",\"path\":\"examples/vector_databases/pinecone/Using_Pinecone_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$60\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_Qdrant_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/qdrant/getting_started_with_qdrant_and_openai\",\"title\":\"Using Qdrant as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/qdrant/Getting_started_with_Qdrant_and_OpenAI.ipynb\",\"date\":\"2023-02-16\",\"authors\":[{\"login\":\"kacperlukawski\",\"name\":\"Kacper Łukawski\",\"avatar\":\"https://avatars.githubusercontent.com/u/2649301?v=4\",\"website\":\"https://github.com/kacperlukawski\"}],\"search_text\":\"$61\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"QA_with_Langchain_Qdrant_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/qdrant/qa_with_langchain_qdrant_and_openai\",\"title\":\"Question answering with Langchain, Qdrant and OpenAI\",\"path\":\"examples/vector_databases/qdrant/QA_with_Langchain_Qdrant_and_OpenAI.ipynb\",\"date\":\"2023-02-16\",\"authors\":[{\"login\":\"kacperlukawski\",\"name\":\"Kacper Łukawski\",\"avatar\":\"https://avatars.githubusercontent.com/u/2649301?v=4\",\"website\":\"https://github.com/kacperlukawski\"}],\"search_text\":\"$62\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_Qdrant_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/qdrant/using_qdrant_for_embeddings_search\",\"title\":\"Using Qdrant for embeddings search\",\"path\":\"examples/vector_databases/qdrant/Using_Qdrant_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"},{\"login\":\"kacperlukawski\",\"name\":\"Kacper Łukawski\",\"avatar\":\"https://avatars.githubusercontent.com/u/2649301?v=4\",\"website\":\"https://github.com/kacperlukawski\"}],\"search_text\":\"$63\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/README.md\",\"internal_path\":\"examples/vector_databases/redis/readme\",\"title\":\"Redis\",\"path\":\"examples/vector_databases/redis/README.md\",\"date\":\"2023-02-13\",\"authors\":[{\"login\":\"Spartee\",\"name\":\"Sam Partee\",\"avatar\":\"https://avatars.githubusercontent.com/u/13009163?v=4\",\"website\":\"https://github.com/Spartee\"}],\"search_text\":\"$64\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Using_Redis_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/redis/using_redis_for_embeddings_search\",\"title\":\"Using Redis for embeddings search\",\"path\":\"examples/vector_databases/redis/Using_Redis_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$65\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"getting-started-with-redis-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/redis/getting-started-with-redis-and-openai\",\"title\":\"Using Redis as a vector database with OpenAI\",\"path\":\"examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb\",\"date\":\"2023-02-13\",\"authors\":[{\"login\":\"Spartee\",\"name\":\"Sam Partee\",\"avatar\":\"https://avatars.githubusercontent.com/u/13009163?v=4\",\"website\":\"https://github.com/Spartee\"}],\"search_text\":\"$66\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"redis-hybrid-query-examples.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redis-hybrid-query-examples.ipynb\",\"internal_path\":\"examples/vector_databases/redis/redis-hybrid-query-examples\",\"title\":\"Running hybrid VSS queries with Redis and OpenAI\",\"path\":\"examples/vector_databases/redis/redis-hybrid-query-examples.ipynb\",\"date\":\"2023-05-11\",\"authors\":[{\"login\":\"Michael Yuan\"}],\"search_text\":\"$67\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"redisjson.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/redisjson/redisjson.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisjson/redisjson.ipynb\",\"internal_path\":\"examples/vector_databases/redis/redisjson/redisjson\",\"title\":\"Redis vectors as JSON with OpenAI\",\"path\":\"examples/vector_databases/redis/redisjson/redisjson.ipynb\",\"date\":\"2023-05-10\",\"authors\":[{\"login\":\"Michael Yuan\"}],\"search_text\":\"This notebook expands on the other Redis OpenAI-cookbook examples with examples of how to use JSON with vectors.\\nStoring Vectors in JSON\\n\\nPrerequisites\\nRedis instance with the Redis Search and Redis JSON modules\\nRedis-py client lib\\nOpenAI API key\\n\\nInstallation\\nInstall Python modules necessary for the examples.\\nOpenAI API Key\\nCreate a .env file and add your OpenAI key to it\\nCreate Text Vectors\\nCreate embeddings (array of floats) of the news excerpts below.\\nStart the Redis Stack Docker container\\nConnect Redis client\\nCreate Index\\nFT.CREATE\\nLoad Data into Redis as JSON objects\\nRedis JSON\\nSemantic Search\\nGiven a sports-related article, search Redis via Vector Similarity Search (VSS) for similar articles.\\nKNN Search\\nHybrid Search\\nUse a combination of full text search and VSS to find a matching article.  For this scenario, we filter on a full text search of the term 'recession' and then find the KNN articles.  In this case, business-related.  Reminder document #1 was about a recession in Japan.\\nHybrid Queries\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"redisqna.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/redis/redisqna/redisqna.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/redisqna/redisqna.ipynb\",\"internal_path\":\"examples/vector_databases/redis/redisqna/redisqna\",\"title\":\"Redis as a context store with Chat Completions\",\"path\":\"examples/vector_databases/redis/redisqna/redisqna.ipynb\",\"date\":\"2023-05-11\",\"authors\":[{\"login\":\"Michael Yuan\"}],\"search_text\":\"$68\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Getting_started_with_Tair_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/tair/getting_started_with_tair_and_openai\",\"title\":\"Using Tair as a vector database for OpenAI embeddings\",\"path\":\"examples/vector_databases/tair/Getting_started_with_Tair_and_OpenAI.ipynb\",\"date\":\"2023-09-11\",\"authors\":[{\"login\":\"dongqqcom\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/32085836?v=4\",\"website\":\"https://github.com/dongqqcom\"}],\"search_text\":\"$69\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"QA_with_Langchain_Tair_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/tair/qa_with_langchain_tair_and_openai\",\"title\":\"Question answering with Langchain, Tair and OpenAI\",\"path\":\"examples/vector_databases/tair/QA_with_Langchain_Tair_and_OpenAI.ipynb\",\"date\":\"2023-09-11\",\"authors\":[{\"login\":\"dongqqcom\",\"name\":null,\"avatar\":\"https://avatars.githubusercontent.com/u/32085836?v=4\",\"website\":\"https://github.com/dongqqcom\"}],\"search_text\":\"$6a\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"tiktoken\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/typesense/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/README.md\",\"internal_path\":\"examples/vector_databases/typesense/readme\",\"title\":\"Typesense\",\"path\":\"examples/vector_databases/typesense/README.md\",\"date\":\"2023-04-13\",\"authors\":[{\"login\":\"jasonbosco\",\"name\":\"Jason Bosco\",\"avatar\":\"https://avatars.githubusercontent.com/u/458383?v=4\",\"website\":\"https://github.com/jasonbosco\"}],\"search_text\":\"$6b\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_Typesense_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/typesense/using_typesense_for_embeddings_search\",\"title\":\"Using Typesense for embeddings search\",\"path\":\"examples/vector_databases/typesense/Using_Typesense_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$6c\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/README.md\",\"internal_path\":\"examples/vector_databases/weaviate/readme\",\"title\":\"Weaviate \u003c\u003e OpenAI\",\"path\":\"examples/vector_databases/weaviate/README.md\",\"date\":\"2023-02-13\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$6d\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"Using_Weaviate_for_embeddings_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb\",\"internal_path\":\"examples/vector_databases/weaviate/using_weaviate_for_embeddings_search\",\"title\":\"Using Weaviate for embeddings search\",\"path\":\"examples/vector_databases/weaviate/Using_Weaviate_for_embeddings_search.ipynb\",\"date\":\"2023-06-28\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$6e\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"generative-search-with-weaviate-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai\",\"title\":\"Using Weaviate with generative OpenAI module for generative search\",\"path\":\"examples/vector_databases/weaviate/generative-search-with-weaviate-and-openai.ipynb\",\"date\":\"2023-05-22\",\"authors\":[{\"login\":\"sebawita\",\"name\":\"Sebastian Witalec\",\"avatar\":\"https://avatars.githubusercontent.com/u/6089763?v=4\",\"website\":\"https://github.com/sebawita\"}],\"search_text\":\"$6f\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"getting-started-with-weaviate-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai\",\"title\":\"Using Weaviate with OpenAI vectorize module for embeddings search\",\"path\":\"examples/vector_databases/weaviate/getting-started-with-weaviate-and-openai.ipynb\",\"date\":\"2023-02-13\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$70\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"hybrid-search-with-weaviate-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai\",\"title\":\"Using Weaviate with OpenAI vectorize module for hybrid search\",\"path\":\"examples/vector_databases/weaviate/hybrid-search-with-weaviate-and-openai.ipynb\",\"date\":\"2023-02-13\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$71\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"question-answering-with-weaviate-and-openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb\",\"internal_path\":\"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai\",\"title\":\"Question Answering in Weaviate with OpenAI Q\u0026A module\",\"path\":\"examples/vector_databases/weaviate/question-answering-with-weaviate-and-openai.ipynb\",\"date\":\"2023-02-13\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$72\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Filtered_search_with_Zilliz_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/zilliz/filtered_search_with_zilliz_and_openai\",\"title\":\"Filtered Search with Zilliz and OpenAI\",\"path\":\"examples/vector_databases/zilliz/Filtered_search_with_Zilliz_and_OpenAI.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"filip-halt\",\"name\":\"Filip Haltmayer\",\"avatar\":\"https://avatars.githubusercontent.com/u/81822489?v=4\",\"website\":\"https://github.com/filip-halt\"}],\"search_text\":\"$73\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"Getting_started_with_Zilliz_and_OpenAI.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb\",\"internal_path\":\"examples/vector_databases/zilliz/getting_started_with_zilliz_and_openai\",\"title\":\"Getting Started with Zilliz and OpenAI\",\"path\":\"examples/vector_databases/zilliz/Getting_started_with_Zilliz_and_OpenAI.ipynb\",\"date\":\"2023-03-28\",\"authors\":[{\"login\":\"filip-halt\",\"name\":\"Filip Haltmayer\",\"avatar\":\"https://avatars.githubusercontent.com/u/81822489?v=4\",\"website\":\"https://github.com/filip-halt\"}],\"search_text\":\"$74\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"techniques_to_improve_reliability.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/techniques_to_improve_reliability.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md\",\"internal_path\":\"articles/techniques_to_improve_reliability\",\"title\":\"Techniques to improve reliability\",\"path\":\"articles/techniques_to_improve_reliability.md\",\"date\":\"2022-09-12\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$75\",\"type\":\"md\",\"tags\":[\"completions\"],\"redirects\":[\"techniques_to_improve_reliability\"]},{\"name\":\"how_to_work_with_large_language_models.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/how_to_work_with_large_language_models.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/how_to_work_with_large_language_models.md\",\"internal_path\":\"articles/how_to_work_with_large_language_models\",\"title\":\"How to work with large language models\",\"path\":\"articles/how_to_work_with_large_language_models.md\",\"date\":\"2023-01-20\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$76\",\"type\":\"md\",\"tags\":[\"completions\"],\"redirects\":[\"how_to_work_with_large_language_models\"]},{\"name\":\"text_comparison_examples.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/text_comparison_examples.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/text_comparison_examples.md\",\"internal_path\":\"articles/text_comparison_examples\",\"title\":\"Use cases for embeddings\",\"path\":\"articles/text_comparison_examples.md\",\"date\":\"2023-01-20\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$77\",\"type\":\"md\",\"tags\":[\"embeddings\"],\"redirects\":[\"text_comparison_examples\"]},{\"name\":\"related_resources.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/related_resources.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/related_resources.md\",\"internal_path\":\"articles/related_resources\",\"title\":\"Related resources from around the web\",\"path\":\"articles/related_resources.md\",\"date\":\"2023-01-20\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"},{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"}],\"search_text\":\"$78\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"],\"redirects\":[\"related_resources\"]},{\"name\":\"ft_retrieval_augmented_generation_qdrant.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb\",\"internal_path\":\"examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant\",\"title\":\"Fine-Tuning for retrieval augmented generation (RAG) with Qdrant\",\"path\":\"examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant.ipynb\",\"date\":\"2023-09-04\",\"authors\":[{\"login\":\"NirantK\",\"name\":\"Nirant\",\"avatar\":\"https://avatars.githubusercontent.com/u/3250749?v=4\",\"website\":\"https://github.com/NirantK\"}],\"search_text\":\"$79\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"fine-tuning\"]},{\"name\":\"How_to_automate_S3_storage_with_functions.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/How_to_automate_S3_storage_with_functions.ipynb\",\"internal_path\":\"examples/third_party/how_to_automate_s3_storage_with_functions\",\"title\":\"How to automate AWS tasks with function calling\",\"path\":\"examples/third_party/How_to_automate_S3_storage_with_functions.ipynb\",\"date\":\"2023-09-27\",\"authors\":[{\"login\":\"Barqawiz\",\"name\":\"cybercoder\",\"avatar\":\"https://avatars.githubusercontent.com/u/2751950?v=4\",\"website\":\"https://github.com/Barqawiz\"}],\"search_text\":\"$7a\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\",\"functions\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/neon/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/README.md\",\"internal_path\":\"examples/vector_databases/neon/readme\",\"title\":\"Neon as a vector database\",\"path\":\"examples/vector_databases/neon/README.md\",\"date\":\"2023-09-28\",\"authors\":[{\"login\":\"Barqawiz\",\"name\":\"cybercoder\",\"avatar\":\"https://avatars.githubusercontent.com/u/2751950?v=4\",\"website\":\"https://github.com/Barqawiz\"}],\"search_text\":\"$7b\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"neon-postgres-vector-search-pgvector.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb\",\"internal_path\":\"examples/vector_databases/neon/neon-postgres-vector-search-pgvector\",\"title\":\"Vector similarity search using Neon Postgres\",\"path\":\"examples/vector_databases/neon/neon-postgres-vector-search-pgvector.ipynb\",\"date\":\"2023-09-28\",\"authors\":[{\"login\":\"danieltprice\",\"name\":\"Daniel\",\"avatar\":\"https://avatars.githubusercontent.com/u/10074684?v=4\",\"website\":\"https://github.com/danieltprice\"}],\"search_text\":\"$7c\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"deeplake_langchain_qa.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb\",\"internal_path\":\"examples/vector_databases/deeplake/deeplake_langchain_qa\",\"title\":\"Question answering with LangChain, Deep Lake, \u0026 OpenAI\",\"path\":\"examples/vector_databases/deeplake/deeplake_langchain_qa.ipynb\",\"date\":\"2023-09-30\",\"authors\":[{\"login\":\"FayazRahman\",\"name\":\"Fayaz Rahman\",\"avatar\":\"https://avatars.githubusercontent.com/u/23637495?v=4\",\"website\":\"https://github.com/FayazRahman\"}],\"search_text\":\"$7d\",\"type\":\"ipynb\",\"tags\":[\"embeddings\"]},{\"name\":\"GPT_finetuning_with_wandb.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/GPT_finetuning_with_wandb.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/GPT_finetuning_with_wandb.ipynb\",\"internal_path\":\"examples/third_party/gpt_finetuning_with_wandb\",\"title\":\"Fine-tuning OpenAI models with Weights \u0026 Biases\",\"path\":\"examples/third_party/GPT_finetuning_with_wandb.ipynb\",\"date\":\"2023-10-04\",\"authors\":[{\"login\":\"ash0ts\",\"name\":\"Anish Shah\",\"avatar\":\"https://avatars.githubusercontent.com/u/93145909?v=4\",\"website\":\"https://github.com/ash0ts\"}],\"search_text\":\"$7e\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"fine-tuning\",\"tiktoken\"]},{\"name\":\"Openai_monitoring_with_wandb_weave.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/third_party/Openai_monitoring_with_wandb_weave.ipynb\",\"internal_path\":\"examples/third_party/openai_monitoring_with_wandb_weave\",\"title\":\"OpenAI API Monitoring with Weights \u0026 Biases Weave\",\"path\":\"examples/third_party/Openai_monitoring_with_wandb_weave.ipynb\",\"date\":\"2023-10-04\",\"authors\":[{\"login\":\"ash0ts\",\"name\":\"Anish Shah\",\"avatar\":\"https://avatars.githubusercontent.com/u/93145909?v=4\",\"website\":\"https://github.com/ash0ts\"}],\"search_text\":\"$7f\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"tiktoken\"]},{\"name\":\"How_to_build_an_agent_with_the_node_sdk.mdx\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_build_an_agent_with_the_node_sdk.mdx\",\"internal_path\":\"examples/how_to_build_an_agent_with_the_node_sdk\",\"title\":\"How to build an agent with the OpenAI Node.js SDK\",\"path\":\"examples/How_to_build_an_agent_with_the_node_sdk.mdx\",\"date\":\"2023-10-05\",\"authors\":[{\"login\":\"perborgen\",\"name\":\"Per Harald Borgen\",\"avatar\":\"https://avatars.githubusercontent.com/u/2429547?v=4\",\"website\":\"https://github.com/perborgen\"}],\"search_text\":\"$80\",\"type\":\"mdx\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"Named_Entity_Recognition_to_enrich_text.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Named_Entity_Recognition_to_enrich_text.ipynb\",\"internal_path\":\"examples/named_entity_recognition_to_enrich_text\",\"title\":\"Named Entity Recognition to Enrich Text\",\"path\":\"examples/Named_Entity_Recognition_to_enrich_text.ipynb\",\"date\":\"2023-10-20\",\"authors\":[{\"login\":\"dcarpintero\",\"name\":\"D. Carpintero\",\"avatar\":\"https://avatars.githubusercontent.com/u/6709785?v=4\",\"website\":\"https://github.com/dcarpintero\"}],\"search_text\":\"$81\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"what_makes_documentation_good.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/what_makes_documentation_good.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/what_makes_documentation_good.md\",\"internal_path\":\"articles/what_makes_documentation_good\",\"title\":\"What makes documentation good\",\"path\":\"articles/what_makes_documentation_good.md\",\"date\":\"2023-09-01\",\"authors\":[{\"login\":\"ted-at-openai\",\"name\":\"Ted Sanders\",\"avatar\":\"https://avatars.githubusercontent.com/u/95656834?v=4\",\"website\":\"https://github.com/ted-at-openai\"}],\"search_text\":\"$82\",\"type\":\"md\",\"tags\":[],\"redirects\":[\"what_makes_documentation_good\"]},{\"name\":\"Function_calling_with_an_OpenAPI_spec.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_with_an_OpenAPI_spec.ipynb\",\"internal_path\":\"examples/function_calling_with_an_openapi_spec\",\"title\":\"Function calling with an OpenAPI specification\",\"path\":\"examples/Function_calling_with_an_OpenAPI_spec.ipynb\",\"date\":\"2023-10-15\",\"authors\":[{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"},{\"name\":\"Simón Fishman\",\"website\":\"https://twitter.com/simonpfish\",\"avatar\":\"https://pbs.twimg.com/profile_images/1687643030243901441/61DpxNfc_400x400.jpg\",\"login\":\"simonpfish\"}],\"search_text\":\"$83\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"Fine_tuning_for_function_calling.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Fine_tuning_for_function_calling.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb\",\"internal_path\":\"examples/fine_tuning_for_function_calling\",\"title\":\"Fine tuning for function calling\",\"path\":\"examples/Fine_tuning_for_function_calling.ipynb\",\"date\":\"2023-11-07\",\"authors\":[{\"name\":\"James Hills\",\"website\":\"https://twitter.com/jamesmhills\",\"avatar\":\"https://pbs.twimg.com/profile_images/1722092156691902464/44FGj7VT_400x400.jpg\",\"login\":\"jhills20\"},{\"name\":\"Ilan Bigio\",\"website\":\"https://twitter.com/ilanbigio\",\"avatar\":\"https://pbs.twimg.com/profile_images/1688302223250378752/z-99TOMH_400x400.jpg\",\"login\":\"ibigio\"},{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"},{\"name\":\"Teodora Musatoiu\",\"website\":\"https://www.linkedin.com/in/teodora-musatoiu/\",\"avatar\":\"https://avatars.githubusercontent.com/u/156829031?s=400\u0026u=af40fe04d9255139eb3bbf8dc83422cc694e862b\u0026v=4\",\"login\":\"teomusatoiu\"}],\"search_text\":\"$84\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"fine-tuning\",\"functions\"]},{\"name\":\"GPT_with_vision_for_video_understanding.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/GPT_with_vision_for_video_understanding.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/GPT_with_vision_for_video_understanding.ipynb\",\"internal_path\":\"examples/gpt_with_vision_for_video_understanding\",\"title\":\"Processing and narrating a video with GPT's visual capabilities and the TTS API\",\"path\":\"examples/GPT_with_vision_for_video_understanding.ipynb\",\"date\":\"2023-11-06\",\"authors\":[{\"name\":\"Kai Chen\",\"website\":\"https://github.com/cathykc\",\"avatar\":\"https://pbs.twimg.com/profile_images/1657816900817272832/ioGq5O0t_400x400.jpg\",\"login\":\"cathykc\"}],\"search_text\":\"$85\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"speech\",\"vision\"]},{\"name\":\"what_is_new_with_dalle_3.mdx\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/articles/what_is_new_with_dalle_3.mdx\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/articles/what_is_new_with_dalle_3.mdx\",\"internal_path\":\"articles/what_is_new_with_dalle_3\",\"title\":\"What's new with DALL·E 3?\",\"path\":\"articles/what_is_new_with_dalle_3.mdx\",\"date\":\"2023-11-06\",\"authors\":[{\"name\":\"Will Depue\",\"website\":\"http://depue.design\",\"avatar\":\"https://pbs.twimg.com/profile_images/1619879636057595904/NdjEAqPm_400x400.jpg\",\"login\":\"0hq\"}],\"search_text\":\"$86\",\"type\":\"mdx\",\"tags\":[\"dall-e\"]},{\"name\":\"Reproducible_outputs_with_the_seed_parameter.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb\",\"internal_path\":\"examples/reproducible_outputs_with_the_seed_parameter\",\"title\":\"How to make your completions outputs consistent with the new seed parameter\",\"path\":\"examples/Reproducible_outputs_with_the_seed_parameter.ipynb\",\"date\":\"2023-11-06\",\"authors\":[{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"}],\"search_text\":\"$87\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"Assistants_API_overview_python.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Assistants_API_overview_python.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Assistants_API_overview_python.ipynb\",\"internal_path\":\"examples/assistants_api_overview_python\",\"title\":\"Assistants API Overview (Python SDK)\",\"path\":\"examples/Assistants_API_overview_python.ipynb\",\"date\":\"2023-11-10\",\"authors\":[{\"name\":\"Ilan Bigio\",\"website\":\"https://twitter.com/ilanbigio\",\"avatar\":\"https://pbs.twimg.com/profile_images/1688302223250378752/z-99TOMH_400x400.jpg\",\"login\":\"ibigio\"}],\"search_text\":\"$88\",\"type\":\"ipynb\",\"tags\":[\"assistants\",\"functions\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/mongodb_atlas/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/README.md\",\"internal_path\":\"examples/vector_databases/mongodb_atlas/readme\",\"title\":\"MongoDB Atlas Vector Search\",\"path\":\"examples/vector_databases/mongodb_atlas/README.md\",\"date\":\"2023-11-21\",\"authors\":[{\"name\":\"Prakul Agarwal\",\"website\":\"https://www.linkedin.com/in/prakulagarwal\",\"avatar\":\"https://media.licdn.com/dms/image/D5603AQEUug83qKgRBg/profile-displayphoto-shrink_800_800/0/1675384960197?e=1706140800\u0026v=beta\u0026t=qxkDbBr-Bk2ASpcwbR5JVPD6yS-vzmIwNHAa8ApyDq4\",\"login\":\"prakul\"}],\"search_text\":\"\\n\\nAtlas Vector Search is a fully managed service that simplifies the process of effectively indexing high-dimensional vector data within MongoDB and being able to perform fast vector similarity searches. With Atlas Vector Search, you can use MongoDB as a standalone vector database for a new project or augment your existing MongoDB collections with vector search functionality. With Atlas Vector Search, you can use the powerful capabilities of vector search in any major public cloud (AWS, Azure, GCP) and achieve massive scalability and data security out of the box while being enterprise-ready with provisions like FedRamp, SoC2 compliance.\\n\\nDocumentation - link\",\"type\":\"md\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"semantic_search_using_mongodb_atlas_vector_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb\",\"internal_path\":\"examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search\",\"title\":\"Semantic search using MongoDB Atlas Vector Search and OpenAI\",\"path\":\"examples/vector_databases/mongodb_atlas/semantic_search_using_mongodb_atlas_vector_search.ipynb\",\"date\":\"2023-11-21\",\"authors\":[{\"name\":\"Prakul Agarwal\",\"website\":\"https://www.linkedin.com/in/prakulagarwal\",\"avatar\":\"https://media.licdn.com/dms/image/D5603AQEUug83qKgRBg/profile-displayphoto-shrink_800_800/0/1675384960197?e=1706140800\u0026v=beta\u0026t=qxkDbBr-Bk2ASpcwbR5JVPD6yS-vzmIwNHAa8ApyDq4\",\"login\":\"prakul\"}],\"search_text\":\"$89\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"Evaluate_RAG_with_LlamaIndex.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\",\"internal_path\":\"examples/evaluation/evaluate_rag_with_llamaindex\",\"title\":\"Evaluate RAG with LlamaIndex\",\"path\":\"examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb\",\"date\":\"2023-11-06\",\"authors\":[{\"login\":\"Ravi Theja\"}],\"search_text\":\"$8a\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"RAG_with_graph_db.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/RAG_with_graph_db.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb\",\"internal_path\":\"examples/rag_with_graph_db\",\"title\":\"RAG with a Graph database\",\"path\":\"examples/RAG_with_graph_db.ipynb\",\"date\":\"2023-12-08\",\"authors\":[{\"name\":\"Katia Gil Guzman\",\"website\":\"https://katia.gg\",\"avatar\":\"https://avatars.githubusercontent.com/u/16519462?v=4\",\"login\":\"katiagg\"}],\"search_text\":\"$8b\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"embeddings\"]},{\"name\":\"README.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/supabase/README.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/README.md\",\"internal_path\":\"examples/vector_databases/supabase/readme\",\"title\":\"Supabase Vector Database\",\"path\":\"examples/vector_databases/supabase/README.md\",\"date\":\"2023-12-04\",\"authors\":[{\"name\":\"Greg Richardson\",\"website\":\"https://twitter.com/ggrdson\",\"avatar\":\"https://pbs.twimg.com/profile_images/1371549909820657664/ZG-HDNlI_400x400.jpg\",\"login\":\"ggrn\"}],\"search_text\":\"$8c\",\"type\":\"md\",\"tags\":[\"embeddings\"]},{\"name\":\"semantic-search.mdx\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/vector_databases/supabase/semantic-search.mdx\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/supabase/semantic-search.mdx\",\"internal_path\":\"examples/vector_databases/supabase/semantic-search\",\"title\":\"Semantic search using Supabase Vector\",\"path\":\"examples/vector_databases/supabase/semantic-search.mdx\",\"date\":\"2023-12-04\",\"authors\":[{\"name\":\"Greg Richardson\",\"website\":\"https://twitter.com/ggrdson\",\"avatar\":\"https://pbs.twimg.com/profile_images/1371549909820657664/ZG-HDNlI_400x400.jpg\",\"login\":\"ggrn\"}],\"search_text\":\"$8d\",\"type\":\"mdx\",\"tags\":[\"embeddings\"]},{\"name\":\"How_to_use_guardrails.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_use_guardrails.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb\",\"internal_path\":\"examples/how_to_use_guardrails\",\"title\":\"How to implement LLM guardrails\",\"path\":\"examples/How_to_use_guardrails.ipynb\",\"date\":\"2023-12-19\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$8e\",\"type\":\"ipynb\",\"tags\":[\"guardrails\"]},{\"name\":\"How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb\",\"internal_path\":\"examples/how_to_combine_gpt4o_with_rag_outfit_assistant\",\"title\":\"How to combine GPT4o mini with RAG to create a clothing matchmaker app\",\"path\":\"examples/How_to_combine_GPT4o_with_RAG_Outfit_Assistant.ipynb\",\"date\":\"2024-07-18\",\"authors\":[{\"name\":\"Teodora Musatoiu\",\"website\":\"https://www.linkedin.com/in/teodora-musatoiu/\",\"avatar\":\"https://avatars.githubusercontent.com/u/156829031?s=400\u0026u=af40fe04d9255139eb3bbf8dc83422cc694e862b\u0026v=4\",\"login\":\"teomusatoiu\"}],\"search_text\":\"$8f\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"vision\"]},{\"name\":\"Parse_PDF_docs_for_RAG.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Parse_PDF_docs_for_RAG.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Parse_PDF_docs_for_RAG.ipynb\",\"internal_path\":\"examples/parse_pdf_docs_for_rag\",\"title\":\"How to parse PDF docs for RAG\",\"path\":\"examples/Parse_PDF_docs_for_RAG.ipynb\",\"date\":\"2024-02-28\",\"authors\":[{\"name\":\"Katia Gil Guzman\",\"website\":\"https://katia.gg\",\"avatar\":\"https://avatars.githubusercontent.com/u/16519462?v=4\",\"login\":\"katiagg\"}],\"search_text\":\"$90\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"vision\"]},{\"name\":\"Tag_caption_images_with_GPT4V.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Tag_caption_images_with_GPT4V.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Tag_caption_images_with_GPT4V.ipynb\",\"internal_path\":\"examples/tag_caption_images_with_gpt4v\",\"title\":\"Using GPT4o mini to tag and caption images\",\"path\":\"examples/Tag_caption_images_with_GPT4V.ipynb\",\"date\":\"2024-07-18\",\"authors\":[{\"name\":\"Katia Gil Guzman\",\"website\":\"https://katia.gg\",\"avatar\":\"https://avatars.githubusercontent.com/u/16519462?v=4\",\"login\":\"katiagg\"}],\"search_text\":\"$91\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"vision\"]},{\"name\":\"How_to_use_moderation.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/How_to_use_moderation.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_moderation.ipynb\",\"internal_path\":\"examples/how_to_use_moderation\",\"title\":\"How to use the moderation API\",\"path\":\"examples/How_to_use_moderation.ipynb\",\"date\":\"2024-03-05\",\"authors\":[{\"name\":\"Teodora Musatoiu\",\"website\":\"https://www.linkedin.com/in/teodora-musatoiu/\",\"avatar\":\"https://avatars.githubusercontent.com/u/156829031?s=400\u0026u=af40fe04d9255139eb3bbf8dc83422cc694e862b\u0026v=4\",\"login\":\"teomusatoiu\"}],\"search_text\":\"$92\",\"type\":\"ipynb\",\"tags\":[\"moderation\"]},{\"name\":\"Summarizing_long_documents.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Summarizing_long_documents.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Summarizing_long_documents.ipynb\",\"internal_path\":\"examples/summarizing_long_documents\",\"title\":\"Summarizing Long Documents\",\"path\":\"examples/Summarizing_long_documents.ipynb\",\"date\":\"2024-04-19\",\"authors\":[{\"name\":\"Joe Palermo\",\"website\":\"https://www.linkedin.com/in/joe-palermo-99219237\",\"avatar\":\"https://media.licdn.com/dms/image/C4E03AQF_tsi7Kom0rg/profile-displayphoto-shrink_800_800/0/1630002100665?e=1704931200\u0026v=beta\u0026t=z3HKO9FmGHJIxhes9TXRzw-8iY-CBsEZYZc8zTogiLU\",\"login\":\"joe-at-openai\"}],\"search_text\":\"$93\",\"type\":\"ipynb\",\"tags\":[\"chat\"]},{\"name\":\"Using_GPT4_Vision_With_Function_Calling.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/multimodal/Using_GPT4_Vision_With_Function_Calling.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/multimodal/Using_GPT4_Vision_With_Function_Calling.ipynb\",\"internal_path\":\"examples/multimodal/using_gpt4_vision_with_function_calling\",\"title\":\"Using GPT4 Vision with Function Calling\",\"path\":\"examples/multimodal/Using_GPT4_Vision_With_Function_Calling.ipynb\",\"date\":\"2024-04-09\",\"authors\":[{\"name\":\"Shyamal Anadkat\",\"website\":\"https://twitter.com/shyamalanadkat\",\"avatar\":\"https://pbs.twimg.com/profile_images/1590564338682560512/3bbZJqxZ_400x400.jpg\",\"login\":\"shyamal-anadkat\"}],\"search_text\":\"$94\",\"type\":\"ipynb\",\"tags\":[\"chat\",\"vision\"]},{\"name\":\"SDG1.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/SDG1.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/SDG1.ipynb\",\"internal_path\":\"examples/sdg1\",\"title\":\"Synthetic data generation (Part 1)\",\"path\":\"examples/SDG1.ipynb\",\"date\":\"2024-04-10\",\"authors\":[{\"name\":\"Dylan Royan Almeida\",\"website\":\"https://www.linkedin.com/in/dylan-almeida-604522167/\",\"avatar\":\"https://avatars.githubusercontent.com/u/149511600?v=4\",\"login\":\"dylanra-openai\"}],\"search_text\":\"$95\",\"type\":\"ipynb\",\"tags\":[\"completions\"]},{\"name\":\"custom_image_embedding_search.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/custom_image_embedding_search.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/custom_image_embedding_search.ipynb\",\"internal_path\":\"examples/custom_image_embedding_search\",\"title\":\"CLIP embeddings to improve multimodal RAG with GPT-4 Vision\",\"path\":\"examples/custom_image_embedding_search.ipynb\",\"date\":\"2024-04-10\",\"authors\":[{\"name\":\"Dylan Royan Almeida\",\"website\":\"https://www.linkedin.com/in/dylan-almeida-604522167/\",\"avatar\":\"https://avatars.githubusercontent.com/u/149511600?v=4\",\"login\":\"dylanra-openai\"}],\"search_text\":\"$96\",\"type\":\"ipynb\",\"tags\":[\"embeddings\",\"vision\"]},{\"name\":\"batch_processing.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/batch_processing.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/batch_processing.ipynb\",\"internal_path\":\"examples/batch_processing\",\"title\":\"Batch processing with the Batch API\",\"path\":\"examples/batch_processing.ipynb\",\"date\":\"2024-04-24\",\"authors\":[{\"name\":\"Katia Gil Guzman\",\"website\":\"https://katia.gg\",\"avatar\":\"https://avatars.githubusercontent.com/u/16519462?v=4\",\"login\":\"katiagg\"}],\"search_text\":\"$97\",\"type\":\"ipynb\",\"tags\":[\"batch\",\"completions\"]},{\"name\":\"Using_tool_required_for_customer_service.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Using_tool_required_for_customer_service.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Using_tool_required_for_customer_service.ipynb\",\"internal_path\":\"examples/using_tool_required_for_customer_service\",\"title\":\"Using tool required for customer service\",\"path\":\"examples/Using_tool_required_for_customer_service.ipynb\",\"date\":\"2024-05-01\",\"authors\":[{\"name\":\"Colin Jarvis\",\"website\":\"https://twitter.com/colintjarvis\",\"avatar\":\"https://pbs.twimg.com/profile_images/1727207339034345472/IM8v8tlC_400x400.jpg\",\"login\":\"colin-openai\"}],\"search_text\":\"$98\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"introduction_to_gpt4o.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/gpt4o/introduction_to_gpt4o.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/gpt4o/introduction_to_gpt4o.ipynb\",\"internal_path\":\"examples/gpt4o/introduction_to_gpt4o\",\"title\":\"Introduction to GPT-4o and GPT-4o mini\",\"path\":\"examples/gpt4o/introduction_to_gpt4o.ipynb\",\"date\":\"2024-07-18\",\"authors\":[{\"name\":\"Juston Forte\",\"website\":\"https://www.linkedin.com/in/justonforte/\",\"avatar\":\"https://avatars.githubusercontent.com/u/96567547?s=400\u0026u=08b9757200906ab12e3989b561cff6c4b95a12cb\u0026v=4\",\"login\":\"justonf\"}],\"search_text\":\"$99\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"vision\",\"whisper\"]},{\"name\":\"Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb\",\"internal_path\":\"examples/chatgpt/rag-quickstart/azure/azure_ai_search_with_azure_functions_and_gpt_actions_in_chatgpt\",\"title\":\"Azure AI Search with Azure Functions and GPT Actions in ChatGPT\",\"path\":\"examples/chatgpt/rag-quickstart/azure/Azure_AI_Search_with_Azure_Functions_and_GPT_Actions_in_ChatGPT.ipynb\",\"date\":\"2024-07-08\",\"authors\":[{\"login\":\"maxreid-openai\",\"name\":\"Max Reid\",\"avatar\":\"https://avatars.githubusercontent.com/u/164893837?v=4\",\"website\":\"https://github.com/maxreid-openai\"}],\"search_text\":\"$9a\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"completions\",\"embeddings\",\"tiktoken\"]},{\"name\":\".gpt_action_getting_started.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/.gpt_action_getting_started.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/.gpt_action_getting_started.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/.gpt_action_getting_started\",\"title\":\"GPT Actions library - getting started\",\"path\":\"examples/chatgpt/gpt_actions_library/.gpt_action_getting_started.ipynb\",\"date\":\"2024-07-09\",\"authors\":[{\"name\":\"Aaron Wilkowitz\",\"website\":\"https://www.linkedin.com/in/aaronwilkowitz/\",\"avatar\":\"https://avatars.githubusercontent.com/u/157151487\",\"login\":\"aaronwilkowitz-openai\"}],\"search_text\":\"$9b\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_bigquery.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_bigquery.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_bigquery.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_bigquery\",\"title\":\"GPT Actions library - BigQuery\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_bigquery.ipynb\",\"date\":\"2024-07-09\",\"authors\":[{\"name\":\"Aaron Wilkowitz\",\"website\":\"https://www.linkedin.com/in/aaronwilkowitz/\",\"avatar\":\"https://avatars.githubusercontent.com/u/157151487\",\"login\":\"aaronwilkowitz-openai\"}],\"search_text\":\"$9c\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"Data_extraction_transformation.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Data_extraction_transformation.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Data_extraction_transformation.ipynb\",\"internal_path\":\"examples/data_extraction_transformation\",\"title\":\"Data Extraction and Transformation in ELT Workflows using GPT-4o as an OCR Alternative\",\"path\":\"examples/Data_extraction_transformation.ipynb\",\"date\":\"2024-07-09\",\"authors\":[{\"name\":\"Charu Jaiswal\",\"website\":\"https://www.linkedin.com/in/charu-j-8a866471\",\"avatar\":\"https://avatars.githubusercontent.com/u/18404643?v=4\",\"login\":\"charuj\"}],\"search_text\":\"$9d\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"vision\"]},{\"name\":\"gpt_action_outlook.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_outlook.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_outlook.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_outlook\",\"title\":\"GPT Actions library - Outlook\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_outlook.ipynb\",\"date\":\"2024-07-15\",\"authors\":[{\"name\":\"Rupert Truman\",\"website\":\"https://www.linkedin.com/in/rupert-truman/\",\"avatar\":\"https://avatars.githubusercontent.com/u/171234447\",\"login\":\"rupert-openai\"}],\"search_text\":\"$9e\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_sharepoint_doc.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc\",\"title\":\"GPT Actions library - Sharepoint (Return Docs)\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_doc.ipynb\",\"date\":\"2024-05-24\",\"authors\":[{\"login\":\"maxreid-openai\",\"name\":\"Max Reid\",\"avatar\":\"https://avatars.githubusercontent.com/u/164893837?v=4\",\"website\":\"https://github.com/maxreid-openai\"}],\"search_text\":\"$9f\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_sharepoint_text.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text\",\"title\":\"GPT Actions library - Sharepoint (Return Text)\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sharepoint_text.ipynb\",\"date\":\"2024-05-24\",\"authors\":[{\"login\":\"maxreid-openai\",\"name\":\"Max Reid\",\"avatar\":\"https://avatars.githubusercontent.com/u/164893837?v=4\",\"website\":\"https://github.com/maxreid-openai\"}],\"search_text\":\"$a0\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_middleware_azure_function.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function\",\"title\":\"GPT Actions library (Middleware) - Azure Functions\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function.ipynb\",\"date\":\"2024-05-24\",\"authors\":[{\"login\":\"maxreid-openai\",\"name\":\"Max Reid\",\"avatar\":\"https://avatars.githubusercontent.com/u/164893837?v=4\",\"website\":\"https://github.com/maxreid-openai\"}],\"search_text\":\"$a1\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_canvaslms.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_canvaslms.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_canvaslms.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_canvaslms\",\"title\":\"GPT Actions library - Canvas LMS\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_canvaslms.ipynb\",\"date\":\"2024-07-17\",\"authors\":[{\"name\":\"Keelan Schule\",\"website\":\"https://www.linkedin.com/in/keelanschule/\",\"avatar\":\"https://avatars.githubusercontent.com/u/175734151?v=4\",\"login\":\"keelan-openai\"}],\"search_text\":\"$a2\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_salesforce.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_salesforce.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_salesforce\",\"title\":\"GPT Actions library - Salesforce\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_salesforce.ipynb\",\"date\":\"2024-07-18\",\"authors\":[{\"name\":\"Allison August\",\"website\":\"https://www.linkedin.com/in/allisonaugust/\",\"avatar\":\"https://avatars.githubusercontent.com/u/172655668?v=4\",\"login\":\"aa-openai\"}],\"search_text\":\"$a3\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_gmail.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_gmail.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_gmail.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_gmail\",\"title\":\"GPT Actions library - Gmail\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_gmail.ipynb\",\"date\":\"2024-07-24\",\"authors\":[{\"name\":\"Anthony Westmoreland\",\"website\":\"https://www.linkedin.com/in/westmorelandanthony/\",\"avatar\":\"https://avatars2.githubusercontent.com/u/175844346\",\"login\":\"alwestmo-openai\"}],\"search_text\":\"$a4\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_jira.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_jira\",\"title\":\"GPT Actions library - Jira\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_jira.ipynb\",\"date\":\"2024-07-24\",\"authors\":[{\"name\":\"Rupert Truman\",\"website\":\"https://www.linkedin.com/in/rupert-truman/\",\"avatar\":\"https://avatars.githubusercontent.com/u/171234447\",\"login\":\"rupert-openai\"}],\"search_text\":\"$a5\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_notion.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_notion.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_notion.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_notion\",\"title\":\"GPT Actions library - Notion\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_notion.ipynb\",\"date\":\"2024-07-25\",\"authors\":[{\"name\":\"Daniel Halpern\",\"website\":\"https://www.linkedin.com/in/dhalp/\",\"avatar\":\"https://avatars.githubusercontent.com/u/175367970\",\"login\":\"dan-openai\"}],\"search_text\":\"$a6\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_confluence.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_confluence.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_confluence.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_confluence\",\"title\":\"GPT Actions library - Confluence\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_confluence.ipynb\",\"date\":\"2024-07-31\",\"authors\":[{\"name\":\"Eric Szuhany\",\"website\":\"https://www.linkedin.com/in/szuhany/\",\"avatar\":\"https://avatars.githubusercontent.com/u/164391912\",\"login\":\"eszuhany-openai\"}],\"search_text\":\"$a7\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_sql_database.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sql_database\",\"title\":\"GPT Actions library - SQL Database\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_sql_database.ipynb\",\"date\":\"2024-07-31\",\"authors\":[{\"name\":\"Evan Weiss\",\"website\":\"https://www.linkedin.com/in/evanpweiss/\",\"avatar\":\"https://avatars.githubusercontent.com/u/150647345\",\"login\":\"evanweiss-openai\"}],\"search_text\":\"$a8\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_box.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_box\",\"title\":\"GPT Actions library - Box\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_box.ipynb\",\"date\":\"2024-08-02\",\"authors\":[{\"name\":\"Keelan Schule\",\"website\":\"https://www.linkedin.com/in/keelanschule/\",\"avatar\":\"https://avatars.githubusercontent.com/u/175734151?v=4\",\"login\":\"keelan-openai\"}],\"search_text\":\"$a9\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"Getting_started_with_bigquery_vector_search_and_openai.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb\",\"internal_path\":\"examples/chatgpt/rag-quickstart/gcp/getting_started_with_bigquery_vector_search_and_openai\",\"title\":\"GCP BigQuery Vector Search with GCP Functions and GPT Actions in ChatGPT\",\"path\":\"examples/chatgpt/rag-quickstart/gcp/Getting_started_with_bigquery_vector_search_and_openai.ipynb\",\"date\":\"2024-08-02\",\"authors\":[{\"name\":\"Pierre-Antoine Porte\",\"website\":\"https://www.linkedin.com/in/portepa/\",\"avatar\":\"https://avatars.githubusercontent.com/u/174109416?v=4\",\"login\":\"pap-openai\"},{\"login\":\"maxreid-openai\",\"name\":\"Max Reid\",\"avatar\":\"https://avatars.githubusercontent.com/u/164893837?v=4\",\"website\":\"https://github.com/maxreid-openai\"}],\"search_text\":\"$aa\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"completions\",\"embeddings\",\"tiktoken\"]},{\"name\":\"gpt_action_zapier.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_zapier.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_zapier.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_zapier\",\"title\":\"GPT Actions library - Zapier\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_zapier.ipynb\",\"date\":\"2024-08-05\",\"authors\":[{\"name\":\"Daniel Halpern\",\"website\":\"https://www.linkedin.com/in/dhalp/\",\"avatar\":\"https://avatars.githubusercontent.com/u/175367970\",\"login\":\"dan-openai\"}],\"search_text\":\"$ab\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"Structured_outputs_multi_agent.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Structured_outputs_multi_agent.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Structured_outputs_multi_agent.ipynb\",\"internal_path\":\"examples/structured_outputs_multi_agent\",\"title\":\"Structured Outputs for Multi-Agent Systems\",\"path\":\"examples/Structured_outputs_multi_agent.ipynb\",\"date\":\"2024-08-06\",\"authors\":[{\"name\":\"Dylan Royan Almeida\",\"website\":\"https://www.linkedin.com/in/dylan-almeida-604522167/\",\"avatar\":\"https://avatars.githubusercontent.com/u/149511600?v=4\",\"login\":\"dylanra-openai\"}],\"search_text\":\"$ac\",\"type\":\"ipynb\",\"tags\":[\"assistants\",\"completions\",\"functions\"]},{\"name\":\"Structured_Outputs_Intro.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/Structured_Outputs_Intro.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/Structured_Outputs_Intro.ipynb\",\"internal_path\":\"examples/structured_outputs_intro\",\"title\":\"Introduction to Structured Outputs\",\"path\":\"examples/Structured_Outputs_Intro.ipynb\",\"date\":\"2024-08-06\",\"authors\":[{\"name\":\"Katia Gil Guzman\",\"website\":\"https://katia.gg\",\"avatar\":\"https://avatars.githubusercontent.com/u/16519462?v=4\",\"login\":\"katiagg\"}],\"search_text\":\"$ad\",\"type\":\"ipynb\",\"tags\":[\"completions\",\"functions\"]},{\"name\":\"gpt_middleware_google_cloud_function.md\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function\",\"title\":\"GPT Actions library (Middleware) - Google Cloud Function\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_google_cloud_function.md\",\"date\":\"2024-08-11\",\"authors\":[{\"name\":\"Girish Dusane\",\"website\":\"https://www.linkedin.com/in/girishdusane/\",\"avatar\":\"https://avatars.githubusercontent.com/u/272708\",\"login\":\"girishd\"}],\"search_text\":\"$ae\",\"type\":\"md\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_redshift.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_redshift\",\"title\":\"GPT Actions library - AWS Redshift\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_redshift.ipynb\",\"date\":\"2024-08-09\",\"authors\":[{\"name\":\"Pierre-Antoine Porte\",\"website\":\"https://www.linkedin.com/in/portepa/\",\"avatar\":\"https://avatars.githubusercontent.com/u/174109416?v=4\",\"login\":\"pap-openai\"}],\"search_text\":\"$af\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_middleware_aws_function.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function\",\"title\":\"GPT Actions library - AWS Middleware\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_middleware_aws_function.ipynb\",\"date\":\"2024-08-09\",\"authors\":[{\"name\":\"Pierre-Antoine Porte\",\"website\":\"https://www.linkedin.com/in/portepa/\",\"avatar\":\"https://avatars.githubusercontent.com/u/174109416?v=4\",\"login\":\"pap-openai\"}],\"search_text\":\"$b0\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_google_drive.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_google_drive.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_google_drive.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_google_drive\",\"title\":\"GPT Actions library - Google Drive\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_google_drive.ipynb\",\"date\":\"2024-08-11\",\"authors\":[{\"name\":\"Luke Xing\",\"website\":\"https://www.linkedin.com/in/lukexing/\",\"avatar\":\"https://avatars.githubusercontent.com/u/176698727\",\"login\":\"lxing-oai\"}],\"search_text\":\"$b1\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_snowflake_direct.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct\",\"title\":\"GPT Actions library - Snowflake Direct\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_snowflake_direct.ipynb\",\"date\":\"2024-08-13\",\"authors\":[{\"name\":\"Kevin Gladstone\",\"website\":\"https://github.com/gladstone-openai\",\"avatar\":\"https://avatars.githubusercontent.com/u/149190645\",\"login\":\"gladstone-openai\"}],\"search_text\":\"$b2\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]},{\"name\":\"gpt_action_snowflake_middleware.ipynb\",\"download_url\":\"https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb\",\"github_url\":\"https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb\",\"internal_path\":\"examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware\",\"title\":\"GPT Actions library - Snowflake Middleware\",\"path\":\"examples/chatgpt/gpt_actions_library/gpt_action_snowflake_middleware.ipynb\",\"date\":\"2024-08-14\",\"authors\":[{\"name\":\"Kevin Gladstone\",\"website\":\"https://github.com/gladstone-openai\",\"avatar\":\"https://avatars.githubusercontent.com/u/149190645\",\"login\":\"gladstone-openai\"}],\"search_text\":\"$b3\",\"type\":\"ipynb\",\"tags\":[\"chatgpt\",\"gpt-actions-library\"]}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"b4:I{\"id\":8716,\"chunks\":[\"809:static/chunks/809-c808b3b3b5db5f45.js\",\"408:static/chunks/408-69aa55354c6889e0.js\",\"884:static/chunks/app/[...path]/page-67f505025e0f5475.js\"],\"name\":\"CodeWithCopyButton\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"article\",null,{\"className\":\"prose prose-sm sm:prose-base max-w-none dark:prose-invert\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"When GPT-3 fails on a task, what should you do?\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Search for a better prompt that elicits more reliable answers?\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Invest in thousands of examples to fine-tune a custom model?\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Assume the model is incapable of the task, and move on?\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":\"There is no simple answer - it depends. However, if your task involves logical reasoning or complexity, consider trying the techniques in this article to build more reliable, high-performing prompts.\"}],\"\\n\",[\"$\",\"h2\",\"h2-0\",{\"id\":\"why-gpt-3-fails-on-complex-tasks\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#why-gpt-3-fails-on-complex-tasks\",\"children\":\"Why GPT-3 fails on complex tasks\"}]}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":\"If you were asked to multiply 13 by 17, would the answer pop immediately into your mind? For most of us, probably not. Yet, that doesn't mean humans are incapable of two-digit multiplication. With a few seconds, and some pen and paper, it's not too taxing to work out that 13 x 17 = 130 + 70 + 21 = 221.\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":\"Similarly, if you give GPT-3 a task that's too complex to do in the time it takes to calculate its next token, it may confabulate an incorrect guess. Yet, akin to humans, that doesn't necessarily mean the model is incapable of the task. With some time and space to reason things out, the model still may be able to answer reliably.\"}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":[\"As an example, if you ask \",[\"$\",\"code\",\"code-0\",{\"children\":\"gpt-3.5-turbo-instruct\"}],\" the following math problem about juggling balls, it answers incorrectly:\"]}],\"\\n\",[\"$\",\"div\",\"div-0\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"A:\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-1\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"There are 8 blue golf balls.\"}]}]}]}]}],\"\\n\",[\"$\",\"p\",\"p-5\",{\"children\":[\"Does this mean that GPT-3 cannot do simple math problems? No; in fact, it turns out that by prompting the model with \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\", the model solves the problem reliably:\"]}],\"\\n\",[\"$\",\"div\",\"div-2\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"A: Let's think step by step.\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-3\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"There are 16 balls in total.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Half of the balls are golf balls.\"}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"That means that there are 8 golf balls.\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Half of the golf balls are blue.\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"That means that there are 4 blue golf balls.\"}]}]]}]}]}],\"\\n\",[\"$\",\"p\",\"p-6\",{\"children\":[\"Of course, it's hard to tell from only a single example whether this \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" trick actually works in general or just got lucky on this particular problem. But it really does work. On a benchmark of word math problems, the \",[\"$\",\"code\",\"code-1\",{\"children\":\"Let's think step by step\"}],\" trick raised GPT-3's solve rate massively, from a worthless 18% to a decent 79%!\"]}],\"\\n\",[\"$\",\"h2\",\"h2-1\",{\"id\":\"model-capabilities-depend-on-context\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#model-capabilities-depend-on-context\",\"children\":\"Model capabilities depend on context\"}]}],\"\\n\",[\"$\",\"p\",\"p-7\",{\"children\":\"When learning to work with GPT-3, one common conceptual mistake is to believe that its capabilities are fixed across all contexts. E.g., if GPT-3 gets a simple logic question wrong, then it must be incapable of simple logic.\"}],\"\\n\",[\"$\",\"p\",\"p-8\",{\"children\":[\"But as the \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" example illustrates, apparent failures of GPT-3 can sometimes be remedied with a better prompt that helps the model steer itself toward the correct output.\"]}],\"\\n\",[\"$\",\"h2\",\"h2-2\",{\"id\":\"how-to-improve-reliability-on-complex-tasks\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#how-to-improve-reliability-on-complex-tasks\",\"children\":\"How to improve reliability on complex tasks\"}]}],\"\\n\",[\"$\",\"p\",\"p-9\",{\"children\":\"The rest of this article shares techniques for improving reliability of large language models on complex tasks. Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks, e.g.:\"}],\"\\n\",[\"$\",\"ul\",\"ul-1\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Give clearer instructions\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Split complex tasks into simpler subtasks\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Structure the instruction to keep the model on task\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"Prompt the model to explain before answering\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"Ask for justifications of many possible answers, and then synthesize\"}],\"\\n\",[\"$\",\"li\",\"li-5\",{\"children\":\"Generate many outputs, and then use the model to pick the best one\"}],\"\\n\",[\"$\",\"li\",\"li-6\",{\"children\":\"Fine-tune custom models to maximize performance\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",\"h2-3\",{\"id\":\"split-complex-tasks-into-simpler-tasks\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#split-complex-tasks-into-simpler-tasks\",\"children\":\"Split complex tasks into simpler tasks\"}]}],\"\\n\",[\"$\",\"p\",\"p-10\",{\"children\":\"One way to give a model more time and space to think is to break tasks into simpler pieces.\"}],\"\\n\",[\"$\",\"p\",\"p-11\",{\"children\":[\"As an example, consider a task where we ask the model a multiple-choice question about some text - in this case, a game of Clue. When asked directly, \",[\"$\",\"code\",\"code-0\",{\"children\":\"gpt-3.5-turbo-instruct\"}],\" isn't able to put clues 3 \u0026 5 together, and answers incorrectly:\"]}],\"\\n\",[\"$\",\"div\",\"div-4\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Use the following clues to answer the following multiple-choice question.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clues:\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"1. Miss Scarlett was the only person in the lounge.\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"2. The person with the pipe was in the kitchen.\"}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"3. Colonel Mustard was the only person in the observatory.\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"4. Professor Plum was not in the library nor the billiard room.\"}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"5. The person with the candlestick was in the observatory.\"}]}],\"\\n\",[\"$\",\"span\",\"span-8\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-9\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Question: Was Colonel Mustard in the observatory with the candlestick?\"}]}],\"\\n\",[\"$\",\"span\",\"span-10\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(a) Yes; Colonel Mustard was in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-11\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(b) No; Colonel Mustard was not in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-12\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-13\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-14\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Solution:\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-5\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\"}]}]}]}]}],\"\\n\",[\"$\",\"p\",\"p-12\",{\"children\":\"Although clues 3 and 5 establish that Colonel Mustard was the only person in the observatory and that the person in the observatory had the candlestick, the model fails to combine them into a correct answer of (a) Yes.\"}],\"\\n\",[\"$\",\"p\",\"p-13\",{\"children\":\"However, instead of asking for the answer directly, we can split the task into three pieces:\"}],\"\\n\",[\"$\",\"ul\",\"ul-2\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"First, go through the clues one by one and consider whether the clue is potentially relevant\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Second, combine the relevant clues to reason out the answer to the question\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Third, write the final answer: either (a), (b), or (c)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-6\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Use the following clues to answer the following multiple-choice question, using the following procedure:\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(1) First, go through the clues one by one and consider whether the clue is potentially relevant\"}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(2) Second, combine the relevant clues to reason out the answer to the question\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clues:\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"1. Miss Scarlett was the only person in the lounge.\"}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"2. The person with the pipe was in the kitchen.\"}]}],\"\\n\",[\"$\",\"span\",\"span-8\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"3. Colonel Mustard was the only person in the observatory.\"}]}],\"\\n\",[\"$\",\"span\",\"span-9\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"4. Professor Plum was not in the library nor the billiard room.\"}]}],\"\\n\",[\"$\",\"span\",\"span-10\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"5. The person with the candlestick was in the observatory.\"}]}],\"\\n\",[\"$\",\"span\",\"span-11\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-12\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Question: Was Colonel Mustard in the observatory with the candlestick?\"}]}],\"\\n\",[\"$\",\"span\",\"span-13\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(a) Yes; Colonel Mustard was in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-14\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(b) No; Colonel Mustard was not in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-15\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick\"}]}],\"\\n\",[\"$\",\"span\",\"span-16\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-17\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Solution:\"}]}],\"\\n\",[\"$\",\"span\",\"span-18\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(1) First, go through the clues one by one and consider whether the clue is potentially relevant:\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-7\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 2: The person with the pipe was in the kitchen. This clue is not relevant.\"}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 5: The person with the candlestick was in the observatory. This clue is relevant.\"}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(2) Second, combine the relevant clues to reason out the answer to the question:\"}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Clue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.\"}]}],\"\\n\",[\"$\",\"span\",\"span-8\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-9\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(3) Third, write the final answer:\"}]}],\"\\n\",[\"$\",\"span\",\"span-10\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(a) Yes; Colonel Mustard was in the observatory with the candlestick\"}]}]]}]}]}],\"\\n\",[\"$\",\"p\",\"p-14\",{\"children\":\"By giving the model more time and space to think, and guiding it along a reasoning plan, it's able to figure out the correct answer of (a) Yes.\"}],\"\\n\",[\"$\",\"p\",\"p-15\",{\"children\":\"Another benefit of splitting complex instructions into smaller subtasks is that it can help keep the model focused on each subtask.\"}],\"\\n\",[\"$\",\"p\",\"p-16\",{\"children\":[\"For example, if we ask \",[\"$\",\"code\",\"code-0\",{\"children\":\"gpt-3.5-turbo-instruct\"}],\" to summarize a text in its original language, the model can lapse back into English:\"]}],\"\\n\",[\"$\",\"div\",\"div-8\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Summarize the text using the original language of the text. The summary should be one sentence long.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Text:\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"La estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \\\"hombre de Estado\\\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\"}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Summary:\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-9\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"The text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.\"}]}]}]}]}],\"\\n\",[\"$\",\"p\",\"p-17\",{\"children\":\"However, if we first ask the model to identify the language of the text, and then summarize the text, it becomes more reliable:\"}],\"\\n\",[\"$\",\"div\",\"div-10\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"First, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Text:\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"La estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, \\\"hombre de Estado\\\")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.\"}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Language:\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-11\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Spanish\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"La estadística es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.\"}]}]]}]}]}],\"\\n\",[\"$\",\"h2\",\"h2-4\",{\"id\":\"prompt-the-model-to-explain-before-answering\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#prompt-the-model-to-explain-before-answering\",\"children\":\"Prompt the model to explain before answering\"}]}],\"\\n\",[\"$\",\"p\",\"p-18\",{\"children\":\"Another powerful technique for improving the reliability of answers is to prompt the model to gradually reason out the answer rather than jumping immediately to the final answer. By 'thinking aloud' the model can be far more likely to arrive at the correct answer.\"}],\"\\n\",[\"$\",\"h3\",\"h3-0\",{\"id\":\"zero-shot\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#zero-shot\",\"children\":\"Zero-shot\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-0\",{\"id\":\"method\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-19\",{\"children\":[\"Published by \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":\"Takeshi Kojima et al. in 2022\"}],\", the easiest way to prompt a model to reason out the answer is to simply prepend answers with \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step.\"}],\" Figure 2 illustrates an example:\"]}],\"\\n\",[\"$\",\"p\",\"p-20\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/zero-shot_reasoners_fig2.png\",\"alt\":\"zero-shot reasoning example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Large Language Models are Zero-Shot Reasoners\"}],\" by Takeshi Kojima et al. (2022).\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-1\",{\"id\":\"results\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-21\",{\"children\":[\"Applying this simple trick to the MultiArith math dataset, the authors found \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" quadrupled the accuracy, from 18% to 79%!\"]}],\"\\n\",[\"$\",\"p\",\"p-22\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/zero-shot_reasoners_tab5.png\",\"alt\":\"zero-shot reasoning example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Large Language Models are Zero-Shot Reasoners\"}],\" by Takeshi Kojima et al. (2022).\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-2\",{\"id\":\"implications\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-23\",{\"children\":[\"Although the \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" trick works well on math problems, it's not effective on all tasks. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn't help with simple math problems or common sense questions, and presumably wouldn't help with many other non-reasoning tasks either.\"]}],\"\\n\",[\"$\",\"p\",\"p-24\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/zero-shot_reasoners_tab1.png\",\"alt\":\"zero-shot reasoning example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Large Language Models are Zero-Shot Reasoners\"}],\" by Takeshi Kojima et al. (2022).\"]}]}],\"\\n\",[\"$\",\"p\",\"p-25\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"p\",\"p-26\",{\"children\":[\"If you apply this technique to your own tasks, don't be afraid to experiment with customizing the instruction. \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" is rather generic, so you may find better performance with instructions that hew to a stricter format customized to your use case. For example, you can try more structured variants like \",[\"$\",\"code\",\"code-1\",{\"children\":\"First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.\"}],\". And you can even give the model an example format to help keep it on track, e.g.:\"]}],\"\\n\",[\"$\",\"div\",\"div-12\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Using the IRS guidance below, answer the following questions using this format:\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(1) For each criterion, determine whether it is met by the vehicle purchase\"}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- {Criterion} Let's think step by step. {explanation} {yes or no, or if the question does not apply then N/A}.\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(2) After considering each criterion in turn, phrase the final answer as \\\"Because of {reasons}, the answer is likely {yes or no}.\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"IRS guidance:\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"You may be eligible for a federal tax credit under Section 30D if you purchased a car or truck that meets the following criteria:\"}]}],\"\\n\",[\"$\",\"span\",\"span-8\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle have at least four wheels?\"}]}],\"\\n\",[\"$\",\"span\",\"span-9\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle weigh less than 14,000 pounds?\"}]}],\"\\n\",[\"$\",\"span\",\"span-10\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source?\"}]}],\"\\n\",[\"$\",\"span\",\"span-11\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Was the vehicle purchased in a year before 2022?\"}]}],\"\\n\",[\"$\",\"span\",\"span-12\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"  - If so, has the manufacturer sold less than 200,000 qualifying vehicles? (Tesla and GM have sold more than 200,000 qualifying vehicles.)\"}]}],\"\\n\",[\"$\",\"span\",\"span-13\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Was the vehicle purchased in a year after 2022?\"}]}],\"\\n\",[\"$\",\"span\",\"span-14\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"  - If so, is the vehicle present in the following list of North American-assembled vehicles? (The only electric vehicles assembled in North America are the Audi Q5, BMW 330e, BMW X5, Chevrolet Bolt EUV, Chevrolet Bolt EV, Chrysler Pacifica PHEV, Ford Escape PHEV, Ford F Series, Ford Mustang MACH E, Ford Transit Van, GMC Hummer Pickup, GMC Hummer SUV, Jeep Grand Cherokee PHEV, Jeep Wrangler PHEV, Lincoln Aviator PHEV, Lincoln Corsair Plug-in, Lucid Air, Nissan Leaf, Rivian EDV, Rivian R1S, Rivian R1T, Tesla Model 3, Tesla Model S, Tesla Model X, Tesla Model Y, Volvo S60, BMW 330e, Bolt EV, Cadillac Lyriq, Mercedes EQS SUV, and Nissan Leaf.)\"}]}],\"\\n\",[\"$\",\"span\",\"span-15\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"\\\"\\\"\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-16\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-17\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Question: Can I claim a federal tax credit for my Toyota Prius Prime bought in 2021?\"}]}],\"\\n\",[\"$\",\"span\",\"span-18\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-19\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Solution:\"}]}],\"\\n\",[\"$\",\"span\",\"span-20\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-21\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(1) For each criterion, determine whether it is met by the vehicle purchase\"}]}],\"\\n\",[\"$\",\"span\",\"span-22\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle have at least four wheels? Let's think step by step.\"}]}]]}]}]}],\"\\n\",[\"$\",\"div\",\"div-13\",{\"data-rehype-pretty-code-fragment\":\"\",\"children\":[\"$\",\"$Lb4\",null,{\"style\":{\"backgroundColor\":\"#24292e\"},\"tabIndex\":0,\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"children\":[\"$\",\"code\",\"code-0\",{\"data-language\":\"gpt-3.5-turbo-instruct\",\"data-theme\":\"default\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",\"span-0\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\" The Toyota Prius Prime has four wheels, so the answer is yes.\"}]}],\"\\n\",[\"$\",\"span\",\"span-1\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle weigh less than 14,000 pounds? Let's think step by step. The Toyota Prius Prime weighs less than 14,000 pounds, so the answer is yes.\"}]}],\"\\n\",[\"$\",\"span\",\"span-2\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source? Let's think step by step. The Toyota Prius Prime has a battery with at least 4 kilowatt hours that may be recharged from an external source, so the answer is yes.\"}]}],\"\\n\",[\"$\",\"span\",\"span-3\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Was the vehicle purchased in a year before 2022? Let's think step by step. The Toyota Prius Prime was purchased in 2021, which is before 2022, so the answer is yes.\"}]}],\"\\n\",[\"$\",\"span\",\"span-4\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- Was the vehicle purchased in a year after 2022? N/A\"}]}],\"\\n\",[\"$\",\"span\",\"span-5\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"- If so, is the vehicle present in the following list of North American-assembled vehicles? N/A\"}]}],\"\\n\",[\"$\",\"span\",\"span-6\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-7\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"(2) After considering each criterion in turn, phrase the final answer as \\\"Because of {reasons}, the answer is likely {yes or no}.\\\"\"}]}],\"\\n\",[\"$\",\"span\",\"span-8\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"}}]}],\"\\n\",[\"$\",\"span\",\"span-9\",{\"data-line\":\"\",\"children\":[\"$\",\"span\",\"span-0\",{\"style\":{\"color\":\"#e1e4e8\"},\"children\":\"Because the Toyota Prius Prime meets all of the criteria for a federal tax credit, the answer is likely yes.\"}]}]]}]}]}],\"\\n\",[\"$\",\"h3\",\"h3-1\",{\"id\":\"few-shot-examples\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#few-shot-examples\",\"children\":\"Few-shot examples\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-3\",{\"id\":\"method-1\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-1\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-27\",{\"children\":[\"Prompting the model to reason out its answers can be done in many ways. One way is to demonstrate with a few examples ('few-shot'), as studied by \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\",\"children\":\"Jason Wei and Denny Zhou et al. from Google\"}],\". Here's an example few-shot chain-of-thought prompt:\"]}],\"\\n\",[\"$\",\"p\",\"p-28\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/chain_of_thought_fig1.png\",\"alt\":\"chain of thought example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"}],\" Jason Wei and Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-29\",{\"children\":\"More demonstrations of reasoning chains written by human labelers:\"}],\"\\n\",[\"$\",\"p\",\"p-30\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/chain_of_thought_fig3.png\",\"alt\":\"chain of thought example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"}],\" Jason Wei and Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-31\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://twitter.com/Meaningness/status/1561062170074370048?s=20\u0026t=mpHt8f3RRboztXxdhLFnWQ\",\"children\":\"(Note that it has been called into question whether pears actually float)\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-4\",{\"id\":\"results-1\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-1\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-32\",{\"children\":\"Testing on grade school math problems, the authors found that chain of thought prompting tripled the solve rate, from 18% to 57%.\"}],\"\\n\",[\"$\",\"p\",\"p-33\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/chain_of_thought_fig5.png\",\"alt\":\"chain of thought example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"}],\" Jason Wei and Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-34\",{\"children\":\"In addition to math problems, chain of thought prompting also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation. In most cases, not many examples were need to saturate the performance gains (less than 8 or so).\"}],\"\\n\",[\"$\",\"p\",\"p-35\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/chain_of_thought_fig11.png\",\"alt\":\"chain of thought example\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"}],\" Jason Wei and Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-36\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2201.11903\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h4\",\"h4-5\",{\"id\":\"implications-1\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-1\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-37\",{\"children\":[\"One advantage of the few-shot example-based approach relative to the \",[\"$\",\"code\",\"code-0\",{\"children\":\"Let's think step by step\"}],\" technique is that you can more easily specify the format, length, and style of reasoning that you want the model to perform before landing on its final answer. This can be particularly helpful in cases where the model isn't initially reasoning in the right way or depth.\"]}],\"\\n\",[\"$\",\"h3\",\"h3-2\",{\"id\":\"fine-tuned\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#fine-tuned\",\"children\":\"Fine-tuned\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-6\",{\"id\":\"method-2\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-2\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-38\",{\"children\":\"In general, to eke out maximum performance on a task, you'll need to fine-tune a custom model. However, fine-tuning a model using explanations may take thousands of example explanations, which are costly to write.\"}],\"\\n\",[\"$\",\"p\",\"p-39\",{\"children\":\"In 2022, Eric Zelikman and Yuhuai Wu et al. published a clever procedure for using a few-shot prompt to generate a dataset of explanations that could be used to fine-tune a model. The idea is to use a few-shot prompt to generate candidate explanations, and only keep the explanations that produce the correct answer. Then, to get additional explanations for some of the incorrect answers, retry the few-shot prompt but with correct answers given as part of the question. The authors called their procedure STaR (Self-taught Reasoner):\"}],\"\\n\",[\"$\",\"p\",\"p-40\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.14465\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/star_fig1.png\",\"alt\":\"STaR procedure\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"STaR: Bootstrapping Reasoning With Reasoning\"}],\" by Eric Zelikman and Yujuai Wu et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-41\",{\"children\":\"With this technique, you can combine the benefits of fine-tuning with the benefits of chain-of-thought prompting without needing to write thousands of example explanations.\"}],\"\\n\",[\"$\",\"h4\",\"h4-7\",{\"id\":\"results-2\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-2\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-42\",{\"children\":\"When the authors applied this technique to a Common Sense Q\u0026A dataset, they found that STaR outperformed both chain-of-thought prompting alone (73% \u003e 37%) and fine-tuning alone (73% \u003e 60%):\"}],\"\\n\",[\"$\",\"p\",\"p-43\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.14465\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/star_tab1.png\",\"alt\":\"STaR results\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"STaR: Bootstrapping Reasoning With Reasoning\"}],\" by Eric Zelikman and Yujuai Wu et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-44\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.14465\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h4\",\"h4-8\",{\"id\":\"implications-2\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-2\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-45\",{\"children\":\"Using a few-shot prompt to extend or modify a fine-tuning dataset is an idea that can be generalized beyond explanation writing. For example, if you have large quantities of unstructured text that you want to train on, you may find opportunities to use a prompt to extract a structured dataset from your unstructured text, and then fine-tune a custom model on that structured dataset.\"}],\"\\n\",[\"$\",\"h2\",\"h2-5\",{\"id\":\"extensions-to-chain-of-thought-prompting\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#extensions-to-chain-of-thought-prompting\",\"children\":\"Extensions to chain-of-thought prompting\"}]}],\"\\n\",[\"$\",\"p\",\"p-46\",{\"children\":\"A number of extensions of chain-of-thought prompting have been published as well.\"}],\"\\n\",[\"$\",\"h3\",\"h3-3\",{\"id\":\"selection-inference-prompting\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#selection-inference-prompting\",\"children\":\"Selection-inference prompting\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-9\",{\"id\":\"method-3\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-3\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-47\",{\"children\":\"Published by Antonia Creswell et al., one extension of the chain-of-thought technique is to split the single prompt for generating explanations and answers into smaller parts. First, a prompt selects a relevant subset of facts from the text ('selection prompt'). Then, a second prompt infers a conclusion from the selected facts ('inference prompt'). These prompts are then alternated in a loop to generate multiple steps of reasoning and eventually land on a final answer. The authors illustrate the idea in the following figure:\"}],\"\\n\",[\"$\",\"p\",\"p-48\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.09712\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/selection-inference_fig1.png\",\"alt\":\"Selection-inference prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-10\",{\"id\":\"results-3\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-3\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-49\",{\"children\":\"When applied to a 7B-parameter model, the authors found that selection-inference prompting substantially improved performance relative to chain-of-thought prompting on the bAbi and Proof Writer benchmark tasks (both of which require longer sequences of reasoning steps). The best performance they achieved combined both selection-inference prompting with fine-tuning.\"}],\"\\n\",[\"$\",\"p\",\"p-50\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.09712\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/selection-inference_fig4.png\",\"alt\":\"Selection-inference prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-11\",{\"id\":\"implications-3\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-3\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-51\",{\"children\":\"Although the gains on these benchmarks were large, these benchmarks were specifically chosen because they required longer sequences of reasoning. On problems that don't require reasoning with many steps, the gains are likely smaller.\"}],\"\\n\",[\"$\",\"p\",\"p-52\",{\"children\":\"The results highlight a couple of general lessons for working with large language models. One, splitting up complex tasks into smaller tasks is a great way to improve reliability and performance; the more atomic the task, the less room there is for the model to err. Two, getting maximum performance often means combining fine-tuning with whatever approach you've chosen.\"}],\"\\n\",[\"$\",\"p\",\"p-53\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.09712\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h3\",\"h3-4\",{\"id\":\"faithful-reasoning-architecture\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#faithful-reasoning-architecture\",\"children\":\"Faithful reasoning architecture\"}]}],\"\\n\",[\"$\",\"p\",\"p-54\",{\"children\":\"A few months after publishing the selection-inference prompting technique, the authors extended the technique in a follow-up paper, with ideas for:\"}],\"\\n\",[\"$\",\"ul\",\"ul-3\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"figuring out when the selection-inference cycle should stop or continue\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"adding a value function to help search over multiple reasoning paths\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"reducing hallucination of fake facts by fine-tuning a model to reason about sentence labels (e.g., sen1) rather than writing out the sentences themselves\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",\"h4-12\",{\"id\":\"method-4\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-4\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-55\",{\"children\":\"In the original selection-inference technique, specialized 'selection' and 'inference' prompts are alternated to select facts and make inferences from those facts, combining to generate a sequence of reasoning steps.\"}],\"\\n\",[\"$\",\"p\",\"p-56\",{\"children\":\"The authors extend this technique with two additional components.\"}],\"\\n\",[\"$\",\"p\",\"p-57\",{\"children\":\"First, the authors add a 'halter' model that, after each inference step, is asked whether the inferences thus far are sufficient to answer the question. If yes, then the model generates a final answer.\"}],\"\\n\",[\"$\",\"p\",\"p-58\",{\"children\":\"The halter models brings a couple of advantages:\"}],\"\\n\",[\"$\",\"ul\",\"ul-4\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"it can tell the selection-inference process to stop or keep going, as necessary.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"if the process never halts, you'll get no answer, which is often preferable to a hallucinated guess\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-59\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_fig3.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-60\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_fig5.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-61\",{\"children\":\"Second, the authors add a value function, which is used to assess the quality of reasoning steps and search over multiple reasoning trajectories. This echoes a common theme for increasing reliability; instead of generating a single answer from the model, generate a set of answers and then use some type of value function / discriminator / verifier model to pick the best one.\"}],\"\\n\",[\"$\",\"p\",\"p-62\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_fig7.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"p\",\"p-63\",{\"children\":\"In addition to these two extensions, the authors also use a trick to reduce hallucination of fake facts. Rather than asking the model to write out factual sentences, they fine-tune a model to work with sentence labels (e.g., sen1) instead. This helps prevent the model from hallucinating fake facts not mentioned in the prompt context.\"}],\"\\n\",[\"$\",\"p\",\"p-64\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_fig4.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-13\",{\"id\":\"results-4\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-4\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-65\",{\"children\":[\"The authors evaluated their technique on two benchmarks: the ProofWriter task (not shown) and \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://allenai.org/data/entailmentbank\",\"children\":\"EntailmentBankQA\"}],\" (shown). The technique increased accuracy substantially, especially on harder reasoning problems.\"]}],\"\\n\",[\"$\",\"p\",\"p-66\",{\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_tab2.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)](\",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":\"https://arxiv.org/abs/2208.14271\"}],\")\"]}],\"\\n\",[\"$\",\"p\",\"p-67\",{\"children\":\"In addition, their sentence label manipulation trick essentially eliminated hallucination!\"}],\"\\n\",[\"$\",\"p\",\"p-68\",{\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/faithful-reasoning_tab5.png\",\"alt\":\"Faithful reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Faithful Reasoning Using Large Language Models\"}],\" by Antonia Creswell et al. (2022)](\",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":\"https://arxiv.org/abs/2208.14271\"}],\")\"]}],\"\\n\",[\"$\",\"h4\",\"h4-14\",{\"id\":\"implications-4\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-4\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-69\",{\"children\":\"This paper illustrates a number of helpful lessons for improving the reliability of large language models:\"}],\"\\n\",[\"$\",\"ul\",\"ul-5\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Split complex tasks into smaller, more reliable subtasks\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Generate your answer in a step-by-step fashion, evaluating it along the way\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Generate many possible answers and use another model or function to pick the ones that look best\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"Reduce hallucination by constraining what the model can say (e.g., by using sentence labels instead of sentences)\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"Maximize performance of models by fine-tuning them on specialized tasks\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-70\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.09712\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h3\",\"h3-5\",{\"id\":\"least-to-most-prompting\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#least-to-most-prompting\",\"children\":\"Least-to-most prompting\"}]}],\"\\n\",[\"$\",\"p\",\"p-71\",{\"children\":\"In addition to doing poorly on long reasoning chains (where selection-inference shines), chain-of-thought prompting can especially struggle when the examples are short but the task is long.\"}],\"\\n\",[\"$\",\"h4\",\"h4-15\",{\"id\":\"method-5\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-5\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-72\",{\"children\":[\"Least-to-most prompting is another technique that splits up reasoning tasks into smaller, more reliable subtasks. The idea is to elicit a subtask from the model by prompting it with something like \",[\"$\",\"code\",\"code-0\",{\"children\":\"To solve {question}, we need to first solve: \\\"\"}],\". Then, with that subtask in hand, the model can generate a solution. The solution is appended to the original question and the process is repeated until a final answer is produced.\"]}],\"\\n\",[\"$\",\"p\",\"p-73\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.10625\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/least-to-most_fig1.png\",\"alt\":\"Least-to-most prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Least-to-most Prompting Enables Complex Reasoning in Large Language Models\"}],\" by Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-16\",{\"id\":\"results-5\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-5\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-74\",{\"children\":[\"When applied to benchmarks involving long reasoning chains using \",[\"$\",\"code\",\"code-0\",{\"children\":\"code-davinci-002\"}],\" (which is optimized for code but can still understand text), the authors measured gains as large as 16% -\u003e 99.7%!\"]}],\"\\n\",[\"$\",\"p\",\"p-75\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.10625\",\"children\":[\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"/images/least-to-most_tab4.png\",\"alt\":\"Least-to-most prompting results on last-letter-concatenation task\"}],\"\\n\",[\"$\",\"img\",\"img-1\",{\"src\":\"/images/least-to-most_tab9.png\",\"alt\":\"Least-to-most prompting results on SCAN\"}],\"\\n\",[\"$\",\"img\",\"img-2\",{\"src\":\"/images/least-to-most_tab11.png\",\"alt\":\"Least-to-most prompting results on DROP numerical reasoning\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Least-to-most Prompting Enables Complex Reasoning in Large Language Models\"}],\" by Denny Zhou et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-17\",{\"id\":\"implications-5\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-5\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-76\",{\"children\":\"Although the above gains from least-to-most prompting are impressive, they are measured on a very narrow set of tasks that require long reasoning chains.\"}],\"\\n\",[\"$\",\"p\",\"p-77\",{\"children\":\"Still, they illustrate a common theme: increase reliability by (a) breaking complex tasks into smaller subtasks and (b) giving the model more time and space to work out the answer.\"}],\"\\n\",[\"$\",\"p\",\"p-78\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.10625\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h2\",\"h2-6\",{\"id\":\"related-ideas\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#related-ideas\",\"children\":\"Related ideas\"}]}],\"\\n\",[\"$\",\"h3\",\"h3-6\",{\"id\":\"maieutic-prompting\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#maieutic-prompting\",\"children\":\"Maieutic prompting\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-18\",{\"id\":\"method-6\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-6\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-79\",{\"children\":[\"In contrast to the previous techniques, which try to maximize the likelihood of correct answers, another approach is to use GPT-3 to generate a tree of possible explanations (both correct \",[\"$\",\"em\",\"em-0\",{\"children\":\"and incorrect\"}],\"), and then analyze their relationships to guess at which set is correct. This technique was coined maieutic prompting by \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11822\",\"children\":\"Jaehun Jung et al. in May 2022\"}],\" (maieutic means relating to the Socratic method of asking questions to elicit ideas).\"]}],\"\\n\",[\"$\",\"p\",\"p-80\",{\"children\":\"The method is complicated, and works as follows:\"}],\"\\n\",[\"$\",\"ul\",\"ul-6\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"First, build a maieutic tree, where each node is a statement that could be true or false:\",\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"Start with a multiple-choice question or true/false statement (e.g. \",[\"$\",\"code\",\"code-0\",{\"children\":\"War cannot have a tie\"}],\")\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"For each possible answer to the question, use the model to generate a corresponding explanation (with a prompt like \",[\"$\",\"code\",\"code-0\",{\"children\":\"War cannot have a tie? True, because\"}],\")\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"Then, prompt the model with the question and the generated explanation, and ask it to produce the answer. If reversing the explanation (with a prefix like \",[\"$\",\"code\",\"code-0\",{\"children\":\"It is wrong to say that {explanation}\"}],\") reverses the answer, then the explanation is considered 'logically integral.'\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"If an explanation is not logically integral, then repeat the above process recursively, with each explanation turned into a True or False question, and generate more explanations for each new question.\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"After all of the recursive explaining is done, you end up with a tree of explanations, where each leaf on the tree has the property that reversing the explanation reverses the model's answer.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"Second, convert the tree into a graph of relations:\",\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"For each node in the tree, calculate the model's relative belief in each node (inferred from the probability of getting an answer of \",[\"$\",\"code\",\"code-0\",{\"children\":\"True\"}],\" to given an explanation)\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"For each pair of nodes in the tree, use the model to identify whether they are entailed (implied) or contradicted\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"Third, find the most consistent set of beliefs and take those to be true:\",\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Specifically, using the strength of belief in each node and the logical relationships between them, formulate the problem as a weighted maximum satisfiability problem (MAX-SAT)\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Use a solver to the find the most self-consistent set of beliefs, and take those as true\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-81\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11822\",\"children\":[\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"/images/maieutic_fig2.png\",\"alt\":\"Maieutic prompting\"}],\"\\n\",[\"$\",\"img\",\"img-1\",{\"src\":\"/images/maieutic_fig6.png\",\"alt\":\"Maieutic prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\"}],\" by Jaehun Jung et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-19\",{\"id\":\"results-6\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-6\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-82\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11822\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/maieutic_tab1.png\",\"alt\":\"Maieutic prompting results\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\"}],\" by Jaehun Jung et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-20\",{\"id\":\"implications-6\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-6\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-83\",{\"children\":\"Beyond the complexity, one limitation of this method is that it appears to only apply to questions that can be posed as multiple-choice.\"}],\"\\n\",[\"$\",\"p\",\"p-84\",{\"children\":[\"To learn more, read the \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11822\",\"children\":\"full paper\"}],\".\"]}],\"\\n\",[\"$\",\"h2\",\"h2-7\",{\"id\":\"extensions\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#extensions\",\"children\":\"Extensions\"}]}],\"\\n\",[\"$\",\"h3\",\"h3-7\",{\"id\":\"self-consistency\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#self-consistency\",\"children\":\"Self-consistency\"}]}],\"\\n\",[\"$\",\"h4\",\"h4-21\",{\"id\":\"method-7\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-7\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-85\",{\"children\":\"For tasks with a discrete set of answers, one simple way to improve reliability is to sample multiple explanations \u0026 answers from the model (using a positive temperature) and then pick the final answer that appears most often.\"}],\"\\n\",[\"$\",\"p\",\"p-86\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.11171\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/self-consistency_fig1.png\",\"alt\":\"Self-consistency method\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\"}],\" by Xuezhi Wang et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-22\",{\"id\":\"results-7\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-7\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-87\",{\"children\":\"This technique lifted accuracies by anywhere from 1 to 24 percentage points on a suite of math and reasoning benchmarks. (Plotted below are results from Google's LaMDA model; using Google's larger PaLM model, the baselines were higher but the gains were a bit smaller.)\"}],\"\\n\",[\"$\",\"p\",\"p-88\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.11171\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/self-consistency_fig3.png\",\"alt\":\"Self-consistency results\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\"}],\" by Xuezhi Wang et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-23\",{\"id\":\"implications-7\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-7\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-89\",{\"children\":\"Although this technique is simple to implement, it can be costly. Generating a set of 10 answers will increase your costs by 10x.\"}],\"\\n\",[\"$\",\"p\",\"p-90\",{\"children\":\"Also, as with many of these techniques, it applies only to tasks with a limited set of answers. For open-ended tasks where each answer is unique (such as writing a poem), it's not obvious what it would mean to pick the most common answer.\"}],\"\\n\",[\"$\",\"p\",\"p-91\",{\"children\":\"Lastly, this technique ought to be most beneficial when there are multiple paths or phrasings to reach an answer; if there's only one path, then the technique may not help at all. An extreme example: If the task was to generate a single token answer, then taking the most common token from 100 generations would be no different than taking the token with the highest logprobs (which you can get with a single generation at temperature=0).\"}],\"\\n\",[\"$\",\"h3\",\"h3-8\",{\"id\":\"verifiers\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#verifiers\",\"children\":\"Verifiers\"}]}],\"\\n\",[\"$\",\"p\",\"p-92\",{\"children\":\"Another key technique for improving task performance is to train a verifier or discriminator model to evaluate the outputs of the main generative model. If the discriminator rejects the output, then you can resample the generative model until you get an acceptable output. In many cases, it's easier to judge an answer than it is to create an answer, which helps explain the power of this method.\"}],\"\\n\",[\"$\",\"h4\",\"h4-24\",{\"id\":\"method-8\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#method-8\",\"children\":\"Method\"}]}],\"\\n\",[\"$\",\"p\",\"p-93\",{\"children\":\"In 2021, OpenAI researchers applied this technique to grade school math problems, using the following procedure:\"}],\"\\n\",[\"$\",\"ul\",\"ul-7\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"First, they fine-tuned a model on questions and solutions\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"For each problem in the training set, they generated 100 solutions\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Each of those 100 solutions was automatically labeled as either correct or incorrect, based on whether the final answer was correct\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"Using those solutions, with some labeled correct and some labeled incorrect, they fine-tuned a verifier model to classify whether a question and candidate solution was correct or incorrect\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"Finally, at test time, the generative model creates 100 solutions to each problem, and the one with the highest score according to the verifier model is picked as the final answer\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"p-94\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2110.14168\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/verifiers_fig3.png\",\"alt\":\"Verifier method\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Training Verifiers to Solve Math Word Problems\"}],\" by Karl Cobbe et al. (2021)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-25\",{\"id\":\"results-8\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#results-8\",\"children\":\"Results\"}]}],\"\\n\",[\"$\",\"p\",\"p-95\",{\"children\":\"With a 175B GPT-3 model and 8,000 training examples, this technique substantially lifted grade school math accuracy from ~33% to ~55%.\"}],\"\\n\",[\"$\",\"p\",\"p-96\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2110.14168\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/verifiers_fig5.png\",\"alt\":\"Verifier results\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Training Verifiers to Solve Math Word Problems\"}],\" by Karl Cobbe et al. (2021)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-26\",{\"id\":\"implications-8\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-8\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-97\",{\"children\":\"Similar to the self-consistency technique, this method can get expensive, as generating, say, 100 solutions per task will increase your costs by roughly ~100x.\"}],\"\\n\",[\"$\",\"h2\",\"h2-8\",{\"id\":\"theories-of-reliability\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#theories-of-reliability\",\"children\":\"Theories of reliability\"}]}],\"\\n\",[\"$\",\"p\",\"p-98\",{\"children\":\"Although the techniques above vary in their approach, they all share the goal of improving reliability on complex tasks. Mainly they do this by:\"}],\"\\n\",[\"$\",\"ul\",\"ul-8\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting)\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",\"h3-9\",{\"id\":\"probabilistic-graphical-models\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#probabilistic-graphical-models\",\"children\":\"Probabilistic graphical models\"}]}],\"\\n\",[\"$\",\"p\",\"p-99\",{\"children\":\"This paradigm of trying to build a reliable system out of less reliable components is reminiscent of probabilistic programming, and many of the analysis techniques of that field can be applied to this one.\"}],\"\\n\",[\"$\",\"p\",\"p-100\",{\"children\":[\"In the paper \",[\"$\",\"em\",\"em-0\",{\"children\":\"Language Model Cascades\"}],\", David Dohan et al. interpret the above techniques in the paradigm of probabilistic graphical models:\"]}],\"\\n\",[\"$\",\"h4\",\"h4-27\",{\"id\":\"chain-of-thought-prompting\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#chain-of-thought-prompting\",\"children\":\"Chain of thought prompting\"}]}],\"\\n\",[\"$\",\"p\",\"p-101\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2207.10342\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/lm_cascades_fig1.png\",\"alt\":\"graphical model of chain of thought prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Language Model Cascades\"}],\" by David Dohan et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-28\",{\"id\":\"fine-tuned-chain-of-thought-prompting--self-taught-reasoner\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#fine-tuned-chain-of-thought-prompting--self-taught-reasoner\",\"children\":\"Fine-tuned chain of thought prompting / Self-taught reasoner\"}]}],\"\\n\",[\"$\",\"p\",\"p-102\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2207.10342\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/lm_cascades_fig3.png\",\"alt\":\"graphical model of fine-tuned chain of thought prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Language Model Cascades\"}],\" by David Dohan et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-29\",{\"id\":\"selection-inference-prompting-1\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#selection-inference-prompting-1\",\"children\":\"Selection-inference prompting\"}]}],\"\\n\",[\"$\",\"p\",\"p-103\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2207.10342\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/lm_cascades_fig4.png\",\"alt\":\"graphical model of selection-inference prompting\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Language Model Cascades\"}],\" by David Dohan et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-30\",{\"id\":\"verifiers-1\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#verifiers-1\",\"children\":\"Verifiers\"}]}],\"\\n\",[\"$\",\"p\",\"p-104\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2207.10342\",\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"/images/lm_cascades_fig5.png\",\"alt\":\"graphical model of verifiers\"}],\"\\n\",\"Source: \",[\"$\",\"em\",\"em-0\",{\"children\":\"Language Model Cascades\"}],\" by David Dohan et al. (2022)\"]}]}],\"\\n\",[\"$\",\"h4\",\"h4-31\",{\"id\":\"implications-9\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#implications-9\",\"children\":\"Implications\"}]}],\"\\n\",[\"$\",\"p\",\"p-105\",{\"children\":\"Although formulating these techniques as probabilistic graphical models may not be immediately useful for solving any particular problem, the framework may be helpful in selecting, combining, and discovering new techniques.\"}],\"\\n\",[\"$\",\"h2\",\"h2-9\",{\"id\":\"closing-thoughts\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#closing-thoughts\",\"children\":\"Closing thoughts\"}]}],\"\\n\",[\"$\",\"p\",\"p-106\",{\"children\":\"Research into large language models is very active and evolving rapidly. Not only do researchers continue to improve the models, they also continue to improve our understanding of how to best employ the models. To underscore the pace of these developments, note that all of the papers shared above were published within the past 12 months (as I write in Sep 2022).\"}],\"\\n\",[\"$\",\"p\",\"p-107\",{\"children\":\"In the future, expect better models and better techniques to be published. Even if the specific techniques here are eclipsed by future best practices, the general principles behind them will likely remain a key part of any expert user's toolkit.\"}],\"\\n\",[\"$\",\"h2\",\"h2-10\",{\"id\":\"bibliography\",\"children\":[\"$\",\"a\",\"a-0\",{\"className\":\"heading-link\",\"href\":\"#bibliography\",\"children\":\"Bibliography\"}]}],\"\\n\",[\"$\",\"table\",\"table-0\",{\"children\":[[\"$\",\"thead\",\"thead-0\",{\"children\":[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"th\",\"th-0\",{\"children\":\"Lesson\"}],[\"$\",\"th\",\"th-1\",{\"children\":\"Paper\"}],[\"$\",\"th\",\"th-2\",{\"children\":\"Date\"}]]}]}],[\"$\",\"tbody\",\"tbody-0\",{\"children\":[[\"$\",\"tr\",\"tr-0\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"Break complex tasks into simpler subtasks (and consider exposing the intermediate outputs to users)\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2110.01691\",\"children\":\"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2021 Oct\"}]]}],[\"$\",\"tr\",\"tr-1\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can improve output by generating many candidates, and then picking the one that looks best\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2110.14168\",\"children\":\"Training Verifiers to Solve Math Word Problems\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2021 Oct\"}]]}],[\"$\",\"tr\",\"tr-2\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"On reasoning tasks, models do better when they reason step-by-step before answering\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2201.11903\",\"children\":\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 Jan\"}]]}],[\"$\",\"tr\",\"tr-3\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can improve step-by-step reasoning by generating many explanation-answer outputs, and picking the most popular answer\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.11171\",\"children\":\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 Mar\"}]]}],[\"$\",\"tr\",\"tr-4\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"If you want to fine-tune a step-by-step reasoner, you can do it with multiple-choice question \u0026 answer data alone\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2203.14465\",\"children\":\"STaR: Bootstrapping Reasoning With Reasoning\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 Mar\"}]]}],[\"$\",\"tr\",\"tr-5\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"The step-by-step reasoning method works great even with zero examples\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11916\",\"children\":\"Large Language Models are Zero-Shot Reasoners\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 May\"}]]}],[\"$\",\"tr\",\"tr-6\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can do better than step-by-step reasoning by alternating a ‘selection’ prompt and an ‘inference’ prompt\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.09712\",\"children\":\"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 May\"}]]}],[\"$\",\"tr\",\"tr-7\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"On long reasoning problems, you can improve step-by-step reasoning by splitting the problem into pieces to solve incrementally\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.10625\",\"children\":\"Least-to-most Prompting Enables Complex Reasoning in Large Language Models\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 May\"}]]}],[\"$\",\"tr\",\"tr-8\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can have the model analyze both good and bogus explanations to figure out which set of explanations are most consistent\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2205.11822\",\"children\":\"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 May\"}]]}],[\"$\",\"tr\",\"tr-9\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can think about these techniques in terms of probabilistic programming, where systems comprise unreliable components\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2207.10342\",\"children\":\"Language Model Cascades\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 Jul\"}]]}],[\"$\",\"tr\",\"tr-10\",{\"children\":[[\"$\",\"td\",\"td-0\",{\"children\":\"You can eliminate hallucination with sentence label manipulation, and you can reduce wrong answers with a 'halter' prompt\"}],[\"$\",\"td\",\"td-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/2208.14271\",\"children\":\"Faithful Reasoning Using Large Language Models\"}]}],[\"$\",\"td\",\"td-2\",{\"children\":\"2022 Aug\"}]]}]]}]]}]]}]\n"])</script><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><nav class="flex items-center justify-between py-3 px-4 sm:px-8 border-b sticky top-0 bg-background h-16 z-50"><a href="https://cookbook.openai.com/" class="flex items-center space-x-1.5"><svg viewBox="0 0 320 320" xmlns="http://www.w3.org/2000/svg" class="h-7 fill-black dark:fill-white min-[850px]:hidden"><path d="m297.06 130.97c7.26-21.79 4.76-45.66-6.85-65.48-17.46-30.4-52.56-46.04-86.84-38.68-15.25-17.18-37.16-26.95-60.13-26.81-35.04-.08-66.13 22.48-76.91 55.82-22.51 4.61-41.94 18.7-53.31 38.67-17.59 30.32-13.58 68.54 9.92 94.54-7.26 21.79-4.76 45.66 6.85 65.48 17.46 30.4 52.56 46.04 86.84 38.68 15.24 17.18 37.16 26.95 60.13 26.8 35.06.09 66.16-22.49 76.94-55.86 22.51-4.61 41.94-18.7 53.31-38.67 17.57-30.32 13.55-68.51-9.94-94.51zm-120.28 168.11c-14.03.02-27.62-4.89-38.39-13.88.49-.26 1.34-.73 1.89-1.07l63.72-36.8c3.26-1.85 5.26-5.32 5.24-9.07v-89.83l26.93 15.55c.29.14.48.42.52.74v74.39c-.04 33.08-26.83 59.9-59.91 59.97zm-128.84-55.03c-7.03-12.14-9.56-26.37-7.15-40.18.47.28 1.3.79 1.89 1.13l63.72 36.8c3.23 1.89 7.23 1.89 10.47 0l77.79-44.92v31.1c.02.32-.13.63-.38.83l-64.41 37.19c-28.69 16.52-65.33 6.7-81.92-21.95zm-16.77-139.09c7-12.16 18.05-21.46 31.21-26.29 0 .55-.03 1.52-.03 2.2v73.61c-.02 3.74 1.98 7.21 5.23 9.06l77.79 44.91-26.93 15.55c-.27.18-.61.21-.91.08l-64.42-37.22c-28.63-16.58-38.45-53.21-21.95-81.89zm221.26 51.49-77.79-44.92 26.93-15.54c.27-.18.61-.21.91-.08l64.42 37.19c28.68 16.57 38.51 53.26 21.94 81.94-7.01 12.14-18.05 21.44-31.2 26.28v-75.81c.03-3.74-1.96-7.2-5.2-9.06zm26.8-40.34c-.47-.29-1.3-.79-1.89-1.13l-63.72-36.8c-3.23-1.89-7.23-1.89-10.47 0l-77.79 44.92v-31.1c-.02-.32.13-.63.38-.83l64.41-37.16c28.69-16.55 65.37-6.7 81.91 22 6.99 12.12 9.52 26.31 7.15 40.1zm-168.51 55.43-26.94-15.55c-.29-.14-.48-.42-.52-.74v-74.39c.02-33.12 26.89-59.96 60.01-59.94 14.01 0 27.57 4.92 38.34 13.88-.49.26-1.33.73-1.89 1.07l-63.72 36.8c-3.26 1.85-5.26 5.31-5.24 9.06l-.04 89.79zm14.63-31.54 34.65-20.01 34.65 20v40.01l-34.65 20-34.65-20z"></path></svg><svg viewBox="0 0 1180 320" xmlns="http://www.w3.org/2000/svg" class="h-7 fill-black dark:fill-white hidden min-[850px]:block"><path d="m367.44 153.84c0 52.32 33.6 88.8 80.16 88.8s80.16-36.48 80.16-88.8-33.6-88.8-80.16-88.8-80.16 36.48-80.16 88.8zm129.6 0c0 37.44-20.4 61.68-49.44 61.68s-49.44-24.24-49.44-61.68 20.4-61.68 49.44-61.68 49.44 24.24 49.44 61.68z"></path><path d="m614.27 242.64c35.28 0 55.44-29.76 55.44-65.52s-20.16-65.52-55.44-65.52c-16.32 0-28.32 6.48-36.24 15.84v-13.44h-28.8v169.2h28.8v-56.4c7.92 9.36 19.92 15.84 36.24 15.84zm-36.96-69.12c0-23.76 13.44-36.72 31.2-36.72 20.88 0 32.16 16.32 32.16 40.32s-11.28 40.32-32.16 40.32c-17.76 0-31.2-13.2-31.2-36.48z"></path><path d="m747.65 242.64c25.2 0 45.12-13.2 54-35.28l-24.72-9.36c-3.84 12.96-15.12 20.16-29.28 20.16-18.48 0-31.44-13.2-33.6-34.8h88.32v-9.6c0-34.56-19.44-62.16-55.92-62.16s-60 28.56-60 65.52c0 38.88 25.2 65.52 61.2 65.52zm-1.44-106.8c18.24 0 26.88 12 27.12 25.92h-57.84c4.32-17.04 15.84-25.92 30.72-25.92z"></path><path d="m823.98 240h28.8v-73.92c0-18 13.2-27.6 26.16-27.6 15.84 0 22.08 11.28 22.08 26.88v74.64h28.8v-83.04c0-27.12-15.84-45.36-42.24-45.36-16.32 0-27.6 7.44-34.8 15.84v-13.44h-28.8z"></path><path d="m1014.17 67.68-65.28 172.32h30.48l14.64-39.36h74.4l14.88 39.36h30.96l-65.28-172.32zm16.8 34.08 27.36 72h-54.24z"></path><path d="m1163.69 68.18h-30.72v172.32h30.72z"></path><path d="m297.06 130.97c7.26-21.79 4.76-45.66-6.85-65.48-17.46-30.4-52.56-46.04-86.84-38.68-15.25-17.18-37.16-26.95-60.13-26.81-35.04-.08-66.13 22.48-76.91 55.82-22.51 4.61-41.94 18.7-53.31 38.67-17.59 30.32-13.58 68.54 9.92 94.54-7.26 21.79-4.76 45.66 6.85 65.48 17.46 30.4 52.56 46.04 86.84 38.68 15.24 17.18 37.16 26.95 60.13 26.8 35.06.09 66.16-22.49 76.94-55.86 22.51-4.61 41.94-18.7 53.31-38.67 17.57-30.32 13.55-68.51-9.94-94.51zm-120.28 168.11c-14.03.02-27.62-4.89-38.39-13.88.49-.26 1.34-.73 1.89-1.07l63.72-36.8c3.26-1.85 5.26-5.32 5.24-9.07v-89.83l26.93 15.55c.29.14.48.42.52.74v74.39c-.04 33.08-26.83 59.9-59.91 59.97zm-128.84-55.03c-7.03-12.14-9.56-26.37-7.15-40.18.47.28 1.3.79 1.89 1.13l63.72 36.8c3.23 1.89 7.23 1.89 10.47 0l77.79-44.92v31.1c.02.32-.13.63-.38.83l-64.41 37.19c-28.69 16.52-65.33 6.7-81.92-21.95zm-16.77-139.09c7-12.16 18.05-21.46 31.21-26.29 0 .55-.03 1.52-.03 2.2v73.61c-.02 3.74 1.98 7.21 5.23 9.06l77.79 44.91-26.93 15.55c-.27.18-.61.21-.91.08l-64.42-37.22c-28.63-16.58-38.45-53.21-21.95-81.89zm221.26 51.49-77.79-44.92 26.93-15.54c.27-.18.61-.21.91-.08l64.42 37.19c28.68 16.57 38.51 53.26 21.94 81.94-7.01 12.14-18.05 21.44-31.2 26.28v-75.81c.03-3.74-1.96-7.2-5.2-9.06zm26.8-40.34c-.47-.29-1.3-.79-1.89-1.13l-63.72-36.8c-3.23-1.89-7.23-1.89-10.47 0l-77.79 44.92v-31.1c-.02-.32.13-.63.38-.83l64.41-37.16c28.69-16.55 65.37-6.7 81.91 22 6.99 12.12 9.52 26.31 7.15 40.1zm-168.51 55.43-26.94-15.55c-.29-.14-.48-.42-.52-.74v-74.39c.02-33.12 26.89-59.96 60.01-59.94 14.01 0 27.57 4.92 38.34 13.88-.49.26-1.33.73-1.89 1.07l-63.72 36.8c-3.26 1.85-5.26 5.31-5.24 9.06l-.04 89.79zm14.63-31.54 34.65-20.01 34.65 20v40.01l-34.65 20-34.65-20z"></path></svg><span class="text-primary text-xl font-light mb-0.5">Cookbook</span></a><div class="mobileNav"><button class="IconButton p-2" aria-label="Customise options" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r0:" data-state="closed"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.5 3C1.22386 3 1 3.22386 1 3.5C1 3.77614 1.22386 4 1.5 4H13.5C13.7761 4 14 3.77614 14 3.5C14 3.22386 13.7761 3 13.5 3H1.5ZM1 7.5C1 7.22386 1.22386 7 1.5 7H13.5C13.7761 7 14 7.22386 14 7.5C14 7.77614 13.7761 8 13.5 8H1.5C1.22386 8 1 7.77614 1 7.5ZM1 11.5C1 11.2239 1.22386 11 1.5 11H13.5C13.7761 11 14 11.2239 14 11.5C14 11.7761 13.7761 12 13.5 12H1.5C1.22386 12 1 11.7761 1 11.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></button><button class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors focus-visible:outline-none disabled:pointer-events-none disabled:opacity-50 h-10 hover:bg-transparent text-muted-foreground hover:text-primary w-8" type="button" id="radix-:r1:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-[1rem] w-[1rem] rotate-0 scale-100 dark:-rotate-90 dark:scale-0"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="absolute h-[1rem] w-[1rem] rotate-90 scale-0 dark:rotate-0 dark:scale-100"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg><span class="sr-only">Toggle theme</span></button></div><div class="flex deskNav"><div class="flex items-center space-x-1 sm:space-x-2 mx-3 sm:mx-4"><button type="button" class="transition-colors text-sm flex flex-row items-center category-button text-muted-foreground py-2 rounded hover:text-primary" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r3:" data-state="closed"><span class="flex items-center justify-between">Topics<svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4 ml-1"><path d="M3.13523 6.15803C3.3241 5.95657 3.64052 5.94637 3.84197 6.13523L7.5 9.56464L11.158 6.13523C11.3595 5.94637 11.6759 5.95657 11.8648 6.15803C12.0536 6.35949 12.0434 6.67591 11.842 6.86477L7.84197 10.6148C7.64964 10.7951 7.35036 10.7951 7.15803 10.6148L3.15803 6.86477C2.95657 6.67591 2.94637 6.35949 3.13523 6.15803Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></span></button><a href="https://cookbook.openai.com/about" class="text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 "><span class="hidden sm:inline">About</span><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="inline sm:hidden w-3.5 h-3.5"><path d="M0.877075 7.49972C0.877075 3.84204 3.84222 0.876892 7.49991 0.876892C11.1576 0.876892 14.1227 3.84204 14.1227 7.49972C14.1227 11.1574 11.1576 14.1226 7.49991 14.1226C3.84222 14.1226 0.877075 11.1574 0.877075 7.49972ZM7.49991 1.82689C4.36689 1.82689 1.82708 4.36671 1.82708 7.49972C1.82708 10.6327 4.36689 13.1726 7.49991 13.1726C10.6329 13.1726 13.1727 10.6327 13.1727 7.49972C13.1727 4.36671 10.6329 1.82689 7.49991 1.82689ZM8.24993 10.5C8.24993 10.9142 7.91414 11.25 7.49993 11.25C7.08571 11.25 6.74993 10.9142 6.74993 10.5C6.74993 10.0858 7.08571 9.75 7.49993 9.75C7.91414 9.75 8.24993 10.0858 8.24993 10.5ZM6.05003 6.25C6.05003 5.57211 6.63511 4.925 7.50003 4.925C8.36496 4.925 8.95003 5.57211 8.95003 6.25C8.95003 6.74118 8.68002 6.99212 8.21447 7.27494C8.16251 7.30651 8.10258 7.34131 8.03847 7.37854L8.03841 7.37858C7.85521 7.48497 7.63788 7.61119 7.47449 7.73849C7.23214 7.92732 6.95003 8.23198 6.95003 8.7C6.95004 9.00376 7.19628 9.25 7.50004 9.25C7.8024 9.25 8.04778 9.00601 8.05002 8.70417L8.05056 8.7033C8.05924 8.6896 8.08493 8.65735 8.15058 8.6062C8.25207 8.52712 8.36508 8.46163 8.51567 8.37436L8.51571 8.37433C8.59422 8.32883 8.68296 8.27741 8.78559 8.21506C9.32004 7.89038 10.05 7.35382 10.05 6.25C10.05 4.92789 8.93511 3.825 7.50003 3.825C6.06496 3.825 4.95003 4.92789 4.95003 6.25C4.95003 6.55376 5.19628 6.8 5.50003 6.8C5.80379 6.8 6.05003 6.55376 6.05003 6.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></a><a href="https://platform.openai.com/docs/introduction" target="_blank" class="text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 "><span class="hidden sm:inline">API Docs</span><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="inline sm:hidden w-3.5 h-3.5"><path d="M3 2.5C3 2.22386 3.22386 2 3.5 2H9.08579C9.21839 2 9.34557 2.05268 9.43934 2.14645L11.8536 4.56066C11.9473 4.65443 12 4.78161 12 4.91421V12.5C12 12.7761 11.7761 13 11.5 13H3.5C3.22386 13 3 12.7761 3 12.5V2.5ZM3.5 1C2.67157 1 2 1.67157 2 2.5V12.5C2 13.3284 2.67157 14 3.5 14H11.5C12.3284 14 13 13.3284 13 12.5V4.91421C13 4.51639 12.842 4.13486 12.5607 3.85355L10.1464 1.43934C9.86514 1.15804 9.48361 1 9.08579 1H3.5ZM4.5 4C4.22386 4 4 4.22386 4 4.5C4 4.77614 4.22386 5 4.5 5H7.5C7.77614 5 8 4.77614 8 4.5C8 4.22386 7.77614 4 7.5 4H4.5ZM4.5 7C4.22386 7 4 7.22386 4 7.5C4 7.77614 4.22386 8 4.5 8H10.5C10.7761 8 11 7.77614 11 7.5C11 7.22386 10.7761 7 10.5 7H4.5ZM4.5 10C4.22386 10 4 10.2239 4 10.5C4 10.7761 4.22386 11 4.5 11H10.5C10.7761 11 11 10.7761 11 10.5C11 10.2239 10.7761 10 10.5 10H4.5Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="hidden sm:inline w-3.5 h-3.5 ml-0.5 mt-0.5 -mr-0.5"><path d="M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></a><a href="https://github.com/openai/openai-cookbook" target="_blank" class="text-muted-foreground hover:text-primary transition-colors text-sm flex flex-row items-center p-2 "><span class="hidden sm:inline">Contribute</span><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-3.5 h-3.5 sm:ml-1.5"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg></a><button class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors focus-visible:outline-none disabled:pointer-events-none disabled:opacity-50 h-10 hover:bg-transparent text-muted-foreground hover:text-primary w-8" type="button" id="radix-:r4:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-[1rem] w-[1rem] rotate-0 scale-100 dark:-rotate-90 dark:scale-0"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="absolute h-[1rem] w-[1rem] rotate-90 scale-0 dark:rotate-0 dark:scale-100"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg><span class="sr-only">Toggle theme</span></button></div><button><div class="hidden text-muted-foreground md:inline-flex justify-between p-1.5 border rounded w-60 cursor-pointer hover:bg-muted transition-colors"><div class="flex items-center"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 mr-2 stroke-muted-foreground"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg><span class="text-sm">Search...</span></div><div class="inline-flex select-none items-center rounded border bg-muted gap-1 font-mono text-[11px] font-medium px-2 text-primary"><span class="text-[15px]">⌘</span>K</div></div><div class="md:hidden h-6 my-1.5"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-6 mr-2 stroke-muted-foreground"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></div></button></div></nav><div class="flex flex-col items-center pt-8 pb-32 px-4 sm:px-8"><div class="max-w-3xl w-full"><div><div><h1 class="sm:text-3xl text-2xl font-bold my-2"><span>Techniques to improve reliability</span></h1><div class="flex justify-between items-end"><div class="flex space-x-2 items-center"><div class="flex"><button class="transform transition-all cursor-pointer ml-0" rel="noopener noreferrer" aria-label="View profile of ted-at-openai" style="z-index: 1;"><div class="rounded-full h-8 w-8 bg-secondary"><img alt="OpenAI Logo" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" class="rounded-full h-8 w-8" srcset="/_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F95656834%3Fv%3D4&amp;w=32&amp;q=75 1x, /_next/image?url=https%3A%2F%2Favatars.githubusercontent.com%2Fu%2F95656834%3Fv%3D4&amp;w=64&amp;q=75 2x" src="./Techniques to improve reliability _ OpenAI Cookbook_files/95656834.jpg" style="color: transparent;"></div></button></div><div class="flex flex-col justify-center"><div><span class="text-sm text-primary"><button aria-label="View profile of ted-at-openai" rel="noopener noreferrer" class="h-8">Ted Sanders</button></span></div><span class="text-xs text-muted-foreground">Sep 11, 2022</span></div></div><a href="https://github.com/openai/openai-cookbook/blob/main/articles/techniques_to_improve_reliability.md" target="_blank" rel="noopener noreferrer" class="text-sm rounded py-2 px-2 sm:px-3 hover:bg-muted transition-colors border-border border flex gap-2 items-center"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="max-sm:hidden">Open in Github</span></a></div><hr class="mt-4 mb-8 border-border"></div><article class="prose prose-sm sm:prose-base max-w-none dark:prose-invert"><p>When GPT-3 fails on a task, what should you do?</p>
<ul>
<li>Search for a better prompt that elicits more reliable answers?</li>
<li>Invest in thousands of examples to fine-tune a custom model?</li>
<li>Assume the model is incapable of the task, and move on?</li>
</ul>
<p>There is no simple answer - it depends. However, if your task involves logical reasoning or complexity, consider trying the techniques in this article to build more reliable, high-performing prompts.</p>
<h2 id="why-gpt-3-fails-on-complex-tasks"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#why-gpt-3-fails-on-complex-tasks">Why GPT-3 fails on complex tasks</a></h2>
<p>If you were asked to multiply 13 by 17, would the answer pop immediately into your mind? For most of us, probably not. Yet, that doesn't mean humans are incapable of two-digit multiplication. With a few seconds, and some pen and paper, it's not too taxing to work out that 13 x 17 = 130 + 70 + 21 = 221.</p>
<p>Similarly, if you give GPT-3 a task that's too complex to do in the time it takes to calculate its next token, it may confabulate an incorrect guess. Yet, akin to humans, that doesn't necessarily mean the model is incapable of the task. With some time and space to reason things out, the model still may be able to answer reliably.</p>
<p>As an example, if you ask <code>gpt-3.5-turbo-instruct</code> the following math problem about juggling balls, it answers incorrectly:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">A:</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">There are 8 blue golf balls.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Does this mean that GPT-3 cannot do simple math problems? No; in fact, it turns out that by prompting the model with <code>Let's think step by step</code>, the model solves the problem reliably:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">A: Let's think step by step.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">There are 16 balls in total.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Half of the balls are golf balls.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">That means that there are 8 golf balls.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Half of the golf balls are blue.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">That means that there are 4 blue golf balls.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Of course, it's hard to tell from only a single example whether this <code>Let's think step by step</code> trick actually works in general or just got lucky on this particular problem. But it really does work. On a benchmark of word math problems, the <code>Let's think step by step</code> trick raised GPT-3's solve rate massively, from a worthless 18% to a decent 79%!</p>
<h2 id="model-capabilities-depend-on-context"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#model-capabilities-depend-on-context">Model capabilities depend on context</a></h2>
<p>When learning to work with GPT-3, one common conceptual mistake is to believe that its capabilities are fixed across all contexts. E.g., if GPT-3 gets a simple logic question wrong, then it must be incapable of simple logic.</p>
<p>But as the <code>Let's think step by step</code> example illustrates, apparent failures of GPT-3 can sometimes be remedied with a better prompt that helps the model steer itself toward the correct output.</p>
<h2 id="how-to-improve-reliability-on-complex-tasks"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#how-to-improve-reliability-on-complex-tasks">How to improve reliability on complex tasks</a></h2>
<p>The rest of this article shares techniques for improving reliability of large language models on complex tasks. Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks, e.g.:</p>
<ul>
<li>Give clearer instructions</li>
<li>Split complex tasks into simpler subtasks</li>
<li>Structure the instruction to keep the model on task</li>
<li>Prompt the model to explain before answering</li>
<li>Ask for justifications of many possible answers, and then synthesize</li>
<li>Generate many outputs, and then use the model to pick the best one</li>
<li>Fine-tune custom models to maximize performance</li>
</ul>
<h2 id="split-complex-tasks-into-simpler-tasks"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#split-complex-tasks-into-simpler-tasks">Split complex tasks into simpler tasks</a></h2>
<p>One way to give a model more time and space to think is to break tasks into simpler pieces.</p>
<p>As an example, consider a task where we ask the model a multiple-choice question about some text - in this case, a game of Clue. When asked directly, <code>gpt-3.5-turbo-instruct</code> isn't able to put clues 3 &amp; 5 together, and answers incorrectly:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Use the following clues to answer the following multiple-choice question.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clues:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">1. Miss Scarlett was the only person in the lounge.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">2. The person with the pipe was in the kitchen.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">3. Colonel Mustard was the only person in the observatory.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">4. Professor Plum was not in the library nor the billiard room.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">5. The person with the candlestick was in the observatory.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Question: Was Colonel Mustard in the observatory with the candlestick?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(a) Yes; Colonel Mustard was in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(b) No; Colonel Mustard was not in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Solution:</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>Although clues 3 and 5 establish that Colonel Mustard was the only person in the observatory and that the person in the observatory had the candlestick, the model fails to combine them into a correct answer of (a) Yes.</p>
<p>However, instead of asking for the answer directly, we can split the task into three pieces:</p>
<ul>
<li>First, go through the clues one by one and consider whether the clue is potentially relevant</li>
<li>Second, combine the relevant clues to reason out the answer to the question</li>
<li>Third, write the final answer: either (a), (b), or (c)</li>
</ul>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Use the following clues to answer the following multiple-choice question, using the following procedure:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(1) First, go through the clues one by one and consider whether the clue is potentially relevant</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(2) Second, combine the relevant clues to reason out the answer to the question</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(3) Third, map the answer to one of the multiple choice answers: either (a), (b), or (c)</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clues:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">1. Miss Scarlett was the only person in the lounge.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">2. The person with the pipe was in the kitchen.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">3. Colonel Mustard was the only person in the observatory.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">4. Professor Plum was not in the library nor the billiard room.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">5. The person with the candlestick was in the observatory.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Question: Was Colonel Mustard in the observatory with the candlestick?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(a) Yes; Colonel Mustard was in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(b) No; Colonel Mustard was not in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(c) Unknown; there is not enough information to determine whether Colonel Mustard was in the observatory with the candlestick</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Solution:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(1) First, go through the clues one by one and consider whether the clue is potentially relevant:</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Clue 1: Miss Scarlett was the only person in the lounge. This clue is not relevant.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clue 2: The person with the pipe was in the kitchen. This clue is not relevant.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clue 3: Colonel Mustard was the only person in the observatory. This clue is relevant.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clue 4: Professor Plum was not in the library nor the billiard room. This clue is not relevant.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clue 5: The person with the candlestick was in the observatory. This clue is relevant.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(2) Second, combine the relevant clues to reason out the answer to the question:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Clue 3 and Clue 5 both say that the person with the candlestick was in the observatory. Colonel Mustard was the only person in the observatory, so Colonel Mustard was the person with the candlestick.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(3) Third, write the final answer:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(a) Yes; Colonel Mustard was in the observatory with the candlestick</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>By giving the model more time and space to think, and guiding it along a reasoning plan, it's able to figure out the correct answer of (a) Yes.</p>
<p>Another benefit of splitting complex instructions into smaller subtasks is that it can help keep the model focused on each subtask.</p>
<p>For example, if we ask <code>gpt-3.5-turbo-instruct</code> to summarize a text in its original language, the model can lapse back into English:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Summarize the text using the original language of the text. The summary should be one sentence long.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Text:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">La estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, "hombre de Estado")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Summary:</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">The text explains that statistics is a science that studies the variability, collection, organization, analysis, interpretation, and presentation of data, as well as the random process that generates them following the laws of probability.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<p>However, if we first ask the model to identify the language of the text, and then summarize the text, it becomes more reliable:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">First, identify the language of the text. Second, summarize the text using the original language of the text. The summary should be one sentence long.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Text:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">La estadística (la forma femenina del término alemán Statistik, derivado a su vez del italiano statista, "hombre de Estado")​ es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.​ La estadística es una ciencia formal deductiva, con un conocimiento propio, dinámico y en continuo desarrollo obtenido a través del método científico formal. En ocasiones, las ciencias fácticas necesitan utilizar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación. En estos casos, la aplicación de la estadística permite el análisis de datos provenientes de una muestra representativa, que busca explicar las correlaciones y dependencias de un fenómeno físico o natural, de ocurrencia en forma aleatoria o condicional.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Language:</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Spanish</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">La estadística es una ciencia que estudia la variabilidad, colección, organización, análisis, interpretación, y presentación de los datos, así como el proceso aleatorio que los genera siguiendo las leyes de la probabilidad.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h2 id="prompt-the-model-to-explain-before-answering"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#prompt-the-model-to-explain-before-answering">Prompt the model to explain before answering</a></h2>
<p>Another powerful technique for improving the reliability of answers is to prompt the model to gradually reason out the answer rather than jumping immediately to the final answer. By 'thinking aloud' the model can be far more likely to arrive at the correct answer.</p>
<h3 id="zero-shot"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#zero-shot">Zero-shot</a></h3>
<h4 id="method"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method">Method</a></h4>
<p>Published by <a href="https://arxiv.org/abs/2205.11916">Takeshi Kojima et al. in 2022</a>, the easiest way to prompt a model to reason out the answer is to simply prepend answers with <code>Let's think step by step.</code> Figure 2 illustrates an example:</p>
<p><a href="https://arxiv.org/abs/2205.11916"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/zero-shot_reasoners_fig2.png" alt="zero-shot reasoning example">
Source: <em>Large Language Models are Zero-Shot Reasoners</em> by Takeshi Kojima et al. (2022).</a></p>
<h4 id="results"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results">Results</a></h4>
<p>Applying this simple trick to the MultiArith math dataset, the authors found <code>Let's think step by step</code> quadrupled the accuracy, from 18% to 79%!</p>
<p><a href="https://arxiv.org/abs/2205.11916"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/zero-shot_reasoners_tab5.png" alt="zero-shot reasoning example">
Source: <em>Large Language Models are Zero-Shot Reasoners</em> by Takeshi Kojima et al. (2022).</a></p>
<h4 id="implications"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications">Implications</a></h4>
<p>Although the <code>Let's think step by step</code> trick works well on math problems, it's not effective on all tasks. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn't help with simple math problems or common sense questions, and presumably wouldn't help with many other non-reasoning tasks either.</p>
<p><a href="https://arxiv.org/abs/2205.11916"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/zero-shot_reasoners_tab1.png" alt="zero-shot reasoning example">
Source: <em>Large Language Models are Zero-Shot Reasoners</em> by Takeshi Kojima et al. (2022).</a></p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2205.11916">full paper</a>.</p>
<p>If you apply this technique to your own tasks, don't be afraid to experiment with customizing the instruction. <code>Let's think step by step</code> is rather generic, so you may find better performance with instructions that hew to a stricter format customized to your use case. For example, you can try more structured variants like <code>First, think step by step about why X might be true. Second, think step by step about why Y might be true. Third, think step by step about whether X or Y makes more sense.</code>. And you can even give the model an example format to help keep it on track, e.g.:</p>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);">Using the IRS guidance below, answer the following questions using this format:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(1) For each criterion, determine whether it is met by the vehicle purchase</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- {Criterion} Let's think step by step. {explanation} {yes or no, or if the question does not apply then N/A}.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(2) After considering each criterion in turn, phrase the final answer as "Because of {reasons}, the answer is likely {yes or no}."</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">IRS guidance:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">You may be eligible for a federal tax credit under Section 30D if you purchased a car or truck that meets the following criteria:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle have at least four wheels?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle weigh less than 14,000 pounds?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Was the vehicle purchased in a year before 2022?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">  - If so, has the manufacturer sold less than 200,000 qualifying vehicles? (Tesla and GM have sold more than 200,000 qualifying vehicles.)</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Was the vehicle purchased in a year after 2022?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">  - If so, is the vehicle present in the following list of North American-assembled vehicles? (The only electric vehicles assembled in North America are the Audi Q5, BMW 330e, BMW X5, Chevrolet Bolt EUV, Chevrolet Bolt EV, Chrysler Pacifica PHEV, Ford Escape PHEV, Ford F Series, Ford Mustang MACH E, Ford Transit Van, GMC Hummer Pickup, GMC Hummer SUV, Jeep Grand Cherokee PHEV, Jeep Wrangler PHEV, Lincoln Aviator PHEV, Lincoln Corsair Plug-in, Lucid Air, Nissan Leaf, Rivian EDV, Rivian R1S, Rivian R1T, Tesla Model 3, Tesla Model S, Tesla Model X, Tesla Model Y, Volvo S60, BMW 330e, Bolt EV, Cadillac Lyriq, Mercedes EQS SUV, and Nissan Leaf.)</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">"""</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Question: Can I claim a federal tax credit for my Toyota Prius Prime bought in 2021?</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Solution:</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(1) For each criterion, determine whether it is met by the vehicle purchase</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle have at least four wheels? Let's think step by step.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<div data-rehype-pretty-code-fragment=""><div class="relative"><pre><code data-language="gpt-3.5-turbo-instruct" data-theme="default" style="display: grid;"><span data-line=""><span style="color: rgb(225, 228, 232);"> The Toyota Prius Prime has four wheels, so the answer is yes.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle weigh less than 14,000 pounds? Let's think step by step. The Toyota Prius Prime weighs less than 14,000 pounds, so the answer is yes.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Does the vehicle draw energy from a battery with at least 4 kilowatt hours that may be recharged from an external source? Let's think step by step. The Toyota Prius Prime has a battery with at least 4 kilowatt hours that may be recharged from an external source, so the answer is yes.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Was the vehicle purchased in a year before 2022? Let's think step by step. The Toyota Prius Prime was purchased in 2021, which is before 2022, so the answer is yes.</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- Was the vehicle purchased in a year after 2022? N/A</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">- If so, is the vehicle present in the following list of North American-assembled vehicles? N/A</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">(2) After considering each criterion in turn, phrase the final answer as "Because of {reasons}, the answer is likely {yes or no}."</span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);"></span></span>
<span data-line=""><span style="color: rgb(225, 228, 232);">Because the Toyota Prius Prime meets all of the criteria for a federal tax credit, the answer is likely yes.</span></span></code></pre><button class="rounded p-1.5 hover:bg-muted/10 transition-all absolute top-2 right-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-5 w-5 stroke-1 stroke-gray-200"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div></div>
<h3 id="few-shot-examples"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#few-shot-examples">Few-shot examples</a></h3>
<h4 id="method-1"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-1">Method</a></h4>
<p>Prompting the model to reason out its answers can be done in many ways. One way is to demonstrate with a few examples ('few-shot'), as studied by <a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html">Jason Wei and Denny Zhou et al. from Google</a>. Here's an example few-shot chain-of-thought prompt:</p>
<p><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/chain_of_thought_fig1.png" alt="chain of thought example">
Source: <em>Chain of Thought Prompting Elicits Reasoning in Large Language Models</em> Jason Wei and Denny Zhou et al. (2022)</a></p>
<p>More demonstrations of reasoning chains written by human labelers:</p>
<p><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/chain_of_thought_fig3.png" alt="chain of thought example">
Source: <em>Chain of Thought Prompting Elicits Reasoning in Large Language Models</em> Jason Wei and Denny Zhou et al. (2022)</a></p>
<p><a href="https://twitter.com/Meaningness/status/1561062170074370048?s=20&amp;t=mpHt8f3RRboztXxdhLFnWQ">(Note that it has been called into question whether pears actually float)</a></p>
<h4 id="results-1"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-1">Results</a></h4>
<p>Testing on grade school math problems, the authors found that chain of thought prompting tripled the solve rate, from 18% to 57%.</p>
<p><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/chain_of_thought_fig5.png" alt="chain of thought example">
Source: <em>Chain of Thought Prompting Elicits Reasoning in Large Language Models</em> Jason Wei and Denny Zhou et al. (2022)</a></p>
<p>In addition to math problems, chain of thought prompting also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation. In most cases, not many examples were need to saturate the performance gains (less than 8 or so).</p>
<p><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/chain_of_thought_fig11.png" alt="chain of thought example">
Source: <em>Chain of Thought Prompting Elicits Reasoning in Large Language Models</em> Jason Wei and Denny Zhou et al. (2022)</a></p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2201.11903">full paper</a>.</p>
<h4 id="implications-1"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-1">Implications</a></h4>
<p>One advantage of the few-shot example-based approach relative to the <code>Let's think step by step</code> technique is that you can more easily specify the format, length, and style of reasoning that you want the model to perform before landing on its final answer. This can be particularly helpful in cases where the model isn't initially reasoning in the right way or depth.</p>
<h3 id="fine-tuned"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#fine-tuned">Fine-tuned</a></h3>
<h4 id="method-2"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-2">Method</a></h4>
<p>In general, to eke out maximum performance on a task, you'll need to fine-tune a custom model. However, fine-tuning a model using explanations may take thousands of example explanations, which are costly to write.</p>
<p>In 2022, Eric Zelikman and Yuhuai Wu et al. published a clever procedure for using a few-shot prompt to generate a dataset of explanations that could be used to fine-tune a model. The idea is to use a few-shot prompt to generate candidate explanations, and only keep the explanations that produce the correct answer. Then, to get additional explanations for some of the incorrect answers, retry the few-shot prompt but with correct answers given as part of the question. The authors called their procedure STaR (Self-taught Reasoner):</p>
<p><a href="https://arxiv.org/abs/2203.14465"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/star_fig1.png" alt="STaR procedure">
Source: <em>STaR: Bootstrapping Reasoning With Reasoning</em> by Eric Zelikman and Yujuai Wu et al. (2022)</a></p>
<p>With this technique, you can combine the benefits of fine-tuning with the benefits of chain-of-thought prompting without needing to write thousands of example explanations.</p>
<h4 id="results-2"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-2">Results</a></h4>
<p>When the authors applied this technique to a Common Sense Q&amp;A dataset, they found that STaR outperformed both chain-of-thought prompting alone (73% &gt; 37%) and fine-tuning alone (73% &gt; 60%):</p>
<p><a href="https://arxiv.org/abs/2203.14465"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/star_tab1.png" alt="STaR results">
Source: <em>STaR: Bootstrapping Reasoning With Reasoning</em> by Eric Zelikman and Yujuai Wu et al. (2022)</a></p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2203.14465">full paper</a>.</p>
<h4 id="implications-2"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-2">Implications</a></h4>
<p>Using a few-shot prompt to extend or modify a fine-tuning dataset is an idea that can be generalized beyond explanation writing. For example, if you have large quantities of unstructured text that you want to train on, you may find opportunities to use a prompt to extract a structured dataset from your unstructured text, and then fine-tune a custom model on that structured dataset.</p>
<h2 id="extensions-to-chain-of-thought-prompting"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#extensions-to-chain-of-thought-prompting">Extensions to chain-of-thought prompting</a></h2>
<p>A number of extensions of chain-of-thought prompting have been published as well.</p>
<h3 id="selection-inference-prompting"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#selection-inference-prompting">Selection-inference prompting</a></h3>
<h4 id="method-3"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-3">Method</a></h4>
<p>Published by Antonia Creswell et al., one extension of the chain-of-thought technique is to split the single prompt for generating explanations and answers into smaller parts. First, a prompt selects a relevant subset of facts from the text ('selection prompt'). Then, a second prompt infers a conclusion from the selected facts ('inference prompt'). These prompts are then alternated in a loop to generate multiple steps of reasoning and eventually land on a final answer. The authors illustrate the idea in the following figure:</p>
<p><a href="https://arxiv.org/abs/2205.09712"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/selection-inference_fig1.png" alt="Selection-inference prompting">
Source: <em>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</em> by Antonia Creswell et al. (2022)</a></p>
<h4 id="results-3"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-3">Results</a></h4>
<p>When applied to a 7B-parameter model, the authors found that selection-inference prompting substantially improved performance relative to chain-of-thought prompting on the bAbi and Proof Writer benchmark tasks (both of which require longer sequences of reasoning steps). The best performance they achieved combined both selection-inference prompting with fine-tuning.</p>
<p><a href="https://arxiv.org/abs/2205.09712"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/selection-inference_fig4.png" alt="Selection-inference prompting">
Source: <em>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</em> by Antonia Creswell et al. (2022)</a></p>
<h4 id="implications-3"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-3">Implications</a></h4>
<p>Although the gains on these benchmarks were large, these benchmarks were specifically chosen because they required longer sequences of reasoning. On problems that don't require reasoning with many steps, the gains are likely smaller.</p>
<p>The results highlight a couple of general lessons for working with large language models. One, splitting up complex tasks into smaller tasks is a great way to improve reliability and performance; the more atomic the task, the less room there is for the model to err. Two, getting maximum performance often means combining fine-tuning with whatever approach you've chosen.</p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2205.09712">full paper</a>.</p>
<h3 id="faithful-reasoning-architecture"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#faithful-reasoning-architecture">Faithful reasoning architecture</a></h3>
<p>A few months after publishing the selection-inference prompting technique, the authors extended the technique in a follow-up paper, with ideas for:</p>
<ul>
<li>figuring out when the selection-inference cycle should stop or continue</li>
<li>adding a value function to help search over multiple reasoning paths</li>
<li>reducing hallucination of fake facts by fine-tuning a model to reason about sentence labels (e.g., sen1) rather than writing out the sentences themselves</li>
</ul>
<h4 id="method-4"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-4">Method</a></h4>
<p>In the original selection-inference technique, specialized 'selection' and 'inference' prompts are alternated to select facts and make inferences from those facts, combining to generate a sequence of reasoning steps.</p>
<p>The authors extend this technique with two additional components.</p>
<p>First, the authors add a 'halter' model that, after each inference step, is asked whether the inferences thus far are sufficient to answer the question. If yes, then the model generates a final answer.</p>
<p>The halter models brings a couple of advantages:</p>
<ul>
<li>it can tell the selection-inference process to stop or keep going, as necessary.</li>
<li>if the process never halts, you'll get no answer, which is often preferable to a hallucinated guess</li>
</ul>
<p><a href="https://arxiv.org/abs/2208.14271"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_fig3.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)</a></p>
<p><a href="https://arxiv.org/abs/2208.14271"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_fig5.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)</a></p>
<p>Second, the authors add a value function, which is used to assess the quality of reasoning steps and search over multiple reasoning trajectories. This echoes a common theme for increasing reliability; instead of generating a single answer from the model, generate a set of answers and then use some type of value function / discriminator / verifier model to pick the best one.</p>
<p><a href="https://arxiv.org/abs/2208.14271"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_fig7.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)</a></p>
<p>In addition to these two extensions, the authors also use a trick to reduce hallucination of fake facts. Rather than asking the model to write out factual sentences, they fine-tune a model to work with sentence labels (e.g., sen1) instead. This helps prevent the model from hallucinating fake facts not mentioned in the prompt context.</p>
<p><a href="https://arxiv.org/abs/2208.14271"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_fig4.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)</a></p>
<h4 id="results-4"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-4">Results</a></h4>
<p>The authors evaluated their technique on two benchmarks: the ProofWriter task (not shown) and <a href="https://allenai.org/data/entailmentbank">EntailmentBankQA</a> (shown). The technique increased accuracy substantially, especially on harder reasoning problems.</p>
<p><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_tab2.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)](<a href="https://arxiv.org/abs/2208.14271">https://arxiv.org/abs/2208.14271</a>)</p>
<p>In addition, their sentence label manipulation trick essentially eliminated hallucination!</p>
<p><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/faithful-reasoning_tab5.png" alt="Faithful reasoning">
Source: <em>Faithful Reasoning Using Large Language Models</em> by Antonia Creswell et al. (2022)](<a href="https://arxiv.org/abs/2208.14271">https://arxiv.org/abs/2208.14271</a>)</p>
<h4 id="implications-4"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-4">Implications</a></h4>
<p>This paper illustrates a number of helpful lessons for improving the reliability of large language models:</p>
<ul>
<li>Split complex tasks into smaller, more reliable subtasks</li>
<li>Generate your answer in a step-by-step fashion, evaluating it along the way</li>
<li>Generate many possible answers and use another model or function to pick the ones that look best</li>
<li>Reduce hallucination by constraining what the model can say (e.g., by using sentence labels instead of sentences)</li>
<li>Maximize performance of models by fine-tuning them on specialized tasks</li>
</ul>
<p>To learn more, read the <a href="https://arxiv.org/abs/2205.09712">full paper</a>.</p>
<h3 id="least-to-most-prompting"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#least-to-most-prompting">Least-to-most prompting</a></h3>
<p>In addition to doing poorly on long reasoning chains (where selection-inference shines), chain-of-thought prompting can especially struggle when the examples are short but the task is long.</p>
<h4 id="method-5"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-5">Method</a></h4>
<p>Least-to-most prompting is another technique that splits up reasoning tasks into smaller, more reliable subtasks. The idea is to elicit a subtask from the model by prompting it with something like <code>To solve {question}, we need to first solve: "</code>. Then, with that subtask in hand, the model can generate a solution. The solution is appended to the original question and the process is repeated until a final answer is produced.</p>
<p><a href="https://arxiv.org/abs/2205.10625"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/least-to-most_fig1.png" alt="Least-to-most prompting">
Source: <em>Least-to-most Prompting Enables Complex Reasoning in Large Language Models</em> by Denny Zhou et al. (2022)</a></p>
<h4 id="results-5"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-5">Results</a></h4>
<p>When applied to benchmarks involving long reasoning chains using <code>code-davinci-002</code> (which is optimized for code but can still understand text), the authors measured gains as large as 16% -&gt; 99.7%!</p>
<p><a href="https://arxiv.org/abs/2205.10625">
<img src="./Techniques to improve reliability _ OpenAI Cookbook_files/least-to-most_tab4.png" alt="Least-to-most prompting results on last-letter-concatenation task">
<img src="./Techniques to improve reliability _ OpenAI Cookbook_files/least-to-most_tab9.png" alt="Least-to-most prompting results on SCAN">
<img src="./Techniques to improve reliability _ OpenAI Cookbook_files/least-to-most_tab11.png" alt="Least-to-most prompting results on DROP numerical reasoning">
Source: <em>Least-to-most Prompting Enables Complex Reasoning in Large Language Models</em> by Denny Zhou et al. (2022)</a></p>
<h4 id="implications-5"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-5">Implications</a></h4>
<p>Although the above gains from least-to-most prompting are impressive, they are measured on a very narrow set of tasks that require long reasoning chains.</p>
<p>Still, they illustrate a common theme: increase reliability by (a) breaking complex tasks into smaller subtasks and (b) giving the model more time and space to work out the answer.</p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2205.10625">full paper</a>.</p>
<h2 id="related-ideas"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#related-ideas">Related ideas</a></h2>
<h3 id="maieutic-prompting"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#maieutic-prompting">Maieutic prompting</a></h3>
<h4 id="method-6"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-6">Method</a></h4>
<p>In contrast to the previous techniques, which try to maximize the likelihood of correct answers, another approach is to use GPT-3 to generate a tree of possible explanations (both correct <em>and incorrect</em>), and then analyze their relationships to guess at which set is correct. This technique was coined maieutic prompting by <a href="https://arxiv.org/abs/2205.11822">Jaehun Jung et al. in May 2022</a> (maieutic means relating to the Socratic method of asking questions to elicit ideas).</p>
<p>The method is complicated, and works as follows:</p>
<ul>
<li>First, build a maieutic tree, where each node is a statement that could be true or false:
<ul>
<li>Start with a multiple-choice question or true/false statement (e.g. <code>War cannot have a tie</code>)</li>
<li>For each possible answer to the question, use the model to generate a corresponding explanation (with a prompt like <code>War cannot have a tie? True, because</code>)</li>
<li>Then, prompt the model with the question and the generated explanation, and ask it to produce the answer. If reversing the explanation (with a prefix like <code>It is wrong to say that {explanation}</code>) reverses the answer, then the explanation is considered 'logically integral.'</li>
<li>If an explanation is not logically integral, then repeat the above process recursively, with each explanation turned into a True or False question, and generate more explanations for each new question.</li>
<li>After all of the recursive explaining is done, you end up with a tree of explanations, where each leaf on the tree has the property that reversing the explanation reverses the model's answer.</li>
</ul>
</li>
<li>Second, convert the tree into a graph of relations:
<ul>
<li>For each node in the tree, calculate the model's relative belief in each node (inferred from the probability of getting an answer of <code>True</code> to given an explanation)</li>
<li>For each pair of nodes in the tree, use the model to identify whether they are entailed (implied) or contradicted</li>
</ul>
</li>
<li>Third, find the most consistent set of beliefs and take those to be true:
<ul>
<li>Specifically, using the strength of belief in each node and the logical relationships between them, formulate the problem as a weighted maximum satisfiability problem (MAX-SAT)</li>
<li>Use a solver to the find the most self-consistent set of beliefs, and take those as true</li>
</ul>
</li>
</ul>
<p><a href="https://arxiv.org/abs/2205.11822">
<img src="./Techniques to improve reliability _ OpenAI Cookbook_files/maieutic_fig2.png" alt="Maieutic prompting">
<img src="./Techniques to improve reliability _ OpenAI Cookbook_files/maieutic_fig6.png" alt="Maieutic prompting">
Source: <em>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</em> by Jaehun Jung et al. (2022)</a></p>
<h4 id="results-6"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-6">Results</a></h4>
<p><a href="https://arxiv.org/abs/2205.11822"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/maieutic_tab1.png" alt="Maieutic prompting results">
Source: <em>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</em> by Jaehun Jung et al. (2022)</a></p>
<h4 id="implications-6"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-6">Implications</a></h4>
<p>Beyond the complexity, one limitation of this method is that it appears to only apply to questions that can be posed as multiple-choice.</p>
<p>To learn more, read the <a href="https://arxiv.org/abs/2205.11822">full paper</a>.</p>
<h2 id="extensions"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#extensions">Extensions</a></h2>
<h3 id="self-consistency"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#self-consistency">Self-consistency</a></h3>
<h4 id="method-7"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-7">Method</a></h4>
<p>For tasks with a discrete set of answers, one simple way to improve reliability is to sample multiple explanations &amp; answers from the model (using a positive temperature) and then pick the final answer that appears most often.</p>
<p><a href="https://arxiv.org/abs/2203.11171"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/self-consistency_fig1.png" alt="Self-consistency method">
Source: <em>Self-Consistency Improves Chain of Thought Reasoning in Language Models</em> by Xuezhi Wang et al. (2022)</a></p>
<h4 id="results-7"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-7">Results</a></h4>
<p>This technique lifted accuracies by anywhere from 1 to 24 percentage points on a suite of math and reasoning benchmarks. (Plotted below are results from Google's LaMDA model; using Google's larger PaLM model, the baselines were higher but the gains were a bit smaller.)</p>
<p><a href="https://arxiv.org/abs/2203.11171"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/self-consistency_fig3.png" alt="Self-consistency results">
Source: <em>Self-Consistency Improves Chain of Thought Reasoning in Language Models</em> by Xuezhi Wang et al. (2022)</a></p>
<h4 id="implications-7"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-7">Implications</a></h4>
<p>Although this technique is simple to implement, it can be costly. Generating a set of 10 answers will increase your costs by 10x.</p>
<p>Also, as with many of these techniques, it applies only to tasks with a limited set of answers. For open-ended tasks where each answer is unique (such as writing a poem), it's not obvious what it would mean to pick the most common answer.</p>
<p>Lastly, this technique ought to be most beneficial when there are multiple paths or phrasings to reach an answer; if there's only one path, then the technique may not help at all. An extreme example: If the task was to generate a single token answer, then taking the most common token from 100 generations would be no different than taking the token with the highest logprobs (which you can get with a single generation at temperature=0).</p>
<h3 id="verifiers"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#verifiers">Verifiers</a></h3>
<p>Another key technique for improving task performance is to train a verifier or discriminator model to evaluate the outputs of the main generative model. If the discriminator rejects the output, then you can resample the generative model until you get an acceptable output. In many cases, it's easier to judge an answer than it is to create an answer, which helps explain the power of this method.</p>
<h4 id="method-8"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#method-8">Method</a></h4>
<p>In 2021, OpenAI researchers applied this technique to grade school math problems, using the following procedure:</p>
<ul>
<li>First, they fine-tuned a model on questions and solutions</li>
<li>For each problem in the training set, they generated 100 solutions</li>
<li>Each of those 100 solutions was automatically labeled as either correct or incorrect, based on whether the final answer was correct</li>
<li>Using those solutions, with some labeled correct and some labeled incorrect, they fine-tuned a verifier model to classify whether a question and candidate solution was correct or incorrect</li>
<li>Finally, at test time, the generative model creates 100 solutions to each problem, and the one with the highest score according to the verifier model is picked as the final answer</li>
</ul>
<p><a href="https://arxiv.org/abs/2110.14168"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/verifiers_fig3.png" alt="Verifier method">
Source: <em>Training Verifiers to Solve Math Word Problems</em> by Karl Cobbe et al. (2021)</a></p>
<h4 id="results-8"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#results-8">Results</a></h4>
<p>With a 175B GPT-3 model and 8,000 training examples, this technique substantially lifted grade school math accuracy from ~33% to ~55%.</p>
<p><a href="https://arxiv.org/abs/2110.14168"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/verifiers_fig5.png" alt="Verifier results">
Source: <em>Training Verifiers to Solve Math Word Problems</em> by Karl Cobbe et al. (2021)</a></p>
<h4 id="implications-8"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-8">Implications</a></h4>
<p>Similar to the self-consistency technique, this method can get expensive, as generating, say, 100 solutions per task will increase your costs by roughly ~100x.</p>
<h2 id="theories-of-reliability"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#theories-of-reliability">Theories of reliability</a></h2>
<p>Although the techniques above vary in their approach, they all share the goal of improving reliability on complex tasks. Mainly they do this by:</p>
<ul>
<li>decomposing unreliable operations into smaller, more reliable operations (e.g., selection-inference prompting)</li>
<li>using multiple steps or multiple relationships to make the system's reliability greater than any individual component (e.g., maieutic prompting)</li>
</ul>
<h3 id="probabilistic-graphical-models"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#probabilistic-graphical-models">Probabilistic graphical models</a></h3>
<p>This paradigm of trying to build a reliable system out of less reliable components is reminiscent of probabilistic programming, and many of the analysis techniques of that field can be applied to this one.</p>
<p>In the paper <em>Language Model Cascades</em>, David Dohan et al. interpret the above techniques in the paradigm of probabilistic graphical models:</p>
<h4 id="chain-of-thought-prompting"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#chain-of-thought-prompting">Chain of thought prompting</a></h4>
<p><a href="https://arxiv.org/abs/2207.10342"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/lm_cascades_fig1.png" alt="graphical model of chain of thought prompting">
Source: <em>Language Model Cascades</em> by David Dohan et al. (2022)</a></p>
<h4 id="fine-tuned-chain-of-thought-prompting--self-taught-reasoner"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#fine-tuned-chain-of-thought-prompting--self-taught-reasoner">Fine-tuned chain of thought prompting / Self-taught reasoner</a></h4>
<p><a href="https://arxiv.org/abs/2207.10342"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/lm_cascades_fig3.png" alt="graphical model of fine-tuned chain of thought prompting">
Source: <em>Language Model Cascades</em> by David Dohan et al. (2022)</a></p>
<h4 id="selection-inference-prompting-1"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#selection-inference-prompting-1">Selection-inference prompting</a></h4>
<p><a href="https://arxiv.org/abs/2207.10342"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/lm_cascades_fig4.png" alt="graphical model of selection-inference prompting">
Source: <em>Language Model Cascades</em> by David Dohan et al. (2022)</a></p>
<h4 id="verifiers-1"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#verifiers-1">Verifiers</a></h4>
<p><a href="https://arxiv.org/abs/2207.10342"><img src="./Techniques to improve reliability _ OpenAI Cookbook_files/lm_cascades_fig5.png" alt="graphical model of verifiers">
Source: <em>Language Model Cascades</em> by David Dohan et al. (2022)</a></p>
<h4 id="implications-9"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#implications-9">Implications</a></h4>
<p>Although formulating these techniques as probabilistic graphical models may not be immediately useful for solving any particular problem, the framework may be helpful in selecting, combining, and discovering new techniques.</p>
<h2 id="closing-thoughts"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#closing-thoughts">Closing thoughts</a></h2>
<p>Research into large language models is very active and evolving rapidly. Not only do researchers continue to improve the models, they also continue to improve our understanding of how to best employ the models. To underscore the pace of these developments, note that all of the papers shared above were published within the past 12 months (as I write in Sep 2022).</p>
<p>In the future, expect better models and better techniques to be published. Even if the specific techniques here are eclipsed by future best practices, the general principles behind them will likely remain a key part of any expert user's toolkit.</p>
<h2 id="bibliography"><a class="heading-link" href="https://cookbook.openai.com/articles/techniques_to_improve_reliability#bibliography">Bibliography</a></h2>
<table><thead><tr><th>Lesson</th><th>Paper</th><th>Date</th></tr></thead><tbody><tr><td>Break complex tasks into simpler subtasks (and consider exposing the intermediate outputs to users)</td><td><a href="https://arxiv.org/abs/2110.01691">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</a></td><td>2021 Oct</td></tr><tr><td>You can improve output by generating many candidates, and then picking the one that looks best</td><td><a href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a></td><td>2021 Oct</td></tr><tr><td>On reasoning tasks, models do better when they reason step-by-step before answering</td><td><a href="https://arxiv.org/abs/2201.11903">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></td><td>2022 Jan</td></tr><tr><td>You can improve step-by-step reasoning by generating many explanation-answer outputs, and picking the most popular answer</td><td><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></td><td>2022 Mar</td></tr><tr><td>If you want to fine-tune a step-by-step reasoner, you can do it with multiple-choice question &amp; answer data alone</td><td><a href="https://arxiv.org/abs/2203.14465">STaR: Bootstrapping Reasoning With Reasoning</a></td><td>2022 Mar</td></tr><tr><td>The step-by-step reasoning method works great even with zero examples</td><td><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></td><td>2022 May</td></tr><tr><td>You can do better than step-by-step reasoning by alternating a ‘selection’ prompt and an ‘inference’ prompt</td><td><a href="https://arxiv.org/abs/2205.09712">Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</a></td><td>2022 May</td></tr><tr><td>On long reasoning problems, you can improve step-by-step reasoning by splitting the problem into pieces to solve incrementally</td><td><a href="https://arxiv.org/abs/2205.10625">Least-to-most Prompting Enables Complex Reasoning in Large Language Models</a></td><td>2022 May</td></tr><tr><td>You can have the model analyze both good and bogus explanations to figure out which set of explanations are most consistent</td><td><a href="https://arxiv.org/abs/2205.11822">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</a></td><td>2022 May</td></tr><tr><td>You can think about these techniques in terms of probabilistic programming, where systems comprise unreliable components</td><td><a href="https://arxiv.org/abs/2207.10342">Language Model Cascades</a></td><td>2022 Jul</td></tr><tr><td>You can eliminate hallucination with sentence label manipulation, and you can reduce wrong answers with a 'halter' prompt</td><td><a href="https://arxiv.org/abs/2208.14271">Faithful Reasoning Using Large Language Models</a></td><td>2022 Aug</td></tr></tbody></table></article></div></div></div><next-route-announcer style="position: absolute;"><template shadowrootmode="open"><div aria-live="assertive" id="__next-route-announcer__" role="alert" style="position: absolute; border: 0px; height: 1px; margin: -1px; padding: 0px; width: 1px; clip: rect(0px, 0px, 0px, 0px); overflow: hidden; white-space: nowrap; overflow-wrap: normal;"></div></template></next-route-announcer></body></html>